[
  {
    "path": "posts/2021-05-05-web-scraping-with-r/",
    "title": "Web scraping with R",
    "description": "Web Scraping with rvest, httr and jsonlite packages.",
    "author": [
      {
        "name": "Basil Okola",
        "url": "https://github.com/Bokola"
      }
    ],
    "date": "2021-05-05",
    "categories": [
      "web scraping",
      "rvest",
      "httr",
      "jsonlite"
    ],
    "contents": "\r\n\r\nWeb scraping in R\r\nHTML Data structure\r\nWeb pages are styles with CSS files: cascade style sheets that determine layout of the webpage. CSS selectors can be used to look for HTML elements of interest. One such is the SelectorGadget google chrome extension. You need to install it to your browser before proceeding.\r\nTo use it open the page\r\nClick on the element you want to select. SelectorGadget will make a first guess at what css selector you want. It’s likely to be bad since it only has one example to learn from, but it’s a start. Elements that match the selector will be highlighted in yellow.\r\nClick on elements that shouldn’t be selected. They will turn red. Click on elements that should be selected. They will turn green.\r\nIterate until only the elements you want are selected. SelectorGadget isn’t perfect and sometimes won’t be able to find a useful css selector. Sometimes starting from a different element helps. More at tidyverse/rvest\r\nFor example, if we want the actors listed on the IMDB movie page, e.g. The Shawshank Redemption\r\nHTML tags can be passed to functions to retrieve the web page elements of interest.\r\nrvest\r\nFor scrapping (harvesting) data fro the web in a structured format that can be used in further analysis.\r\nrvest functions\r\nread_html(): collects data from the webpage\r\nhtml_nodes(): extract the relevant pieces\r\nhtml_text(): extract tags of the relevant piece\r\nhtml_attributes(): extract attributes of the relevant piece\r\n\r\n\r\n# specify url \r\nurl = 'https://www.imdb.com/title/tt0111161/'\r\n# reading the html code from the \r\nwebpage = read_html(url)\r\nwebpage\r\n\r\n\r\n{html_document}\r\n<html xmlns:og=\"http://ogp.me/ns#\" xmlns:fb=\"http://www.facebook.com/2008/fbml\">\r\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; cha ...\r\n[2] <body id=\"styleguide-v2\" class=\"fixed\">\\n            <img heigh ...\r\n\r\nOnce we have determined the CSS selector, we use it to extract the information we want\r\n\r\n\r\ncast = html_nodes(webpage, \".primary_photo+ td a\")\r\nlength(cast)\r\n\r\n\r\n[1] 15\r\n\r\ncast[1:2]\r\n\r\n\r\n{xml_nodeset (2)}\r\n[1] <a href=\"/name/nm0000209/?ref_=tt_cl_t1\"> Tim Robbins\\n<\/a>\r\n[2] <a href=\"/name/nm0000151/?ref_=tt_cl_t2\"> Morgan Freeman\\n<\/a>\r\n\r\nFinally, we extract the text from the selected HTML nodes.\r\n\r\n\r\nhtml_text(cast, trim = T)\r\n\r\n\r\n [1] \"Tim Robbins\"       \"Morgan Freeman\"    \"Bob Gunton\"       \r\n [4] \"William Sadler\"    \"Clancy Brown\"      \"Gil Bellows\"      \r\n [7] \"Mark Rolston\"      \"James Whitmore\"    \"Jeffrey DeMunn\"   \r\n[10] \"Larry Brandenburg\" \"Neil Giuntoli\"     \"Brian Libby\"      \r\n[13] \"David Proval\"      \"Joseph Ragno\"      \"Jude Ciccolella\"  \r\n\r\nExtracting a table\r\n\r\n\r\nall_tables = html_table(webpage, \"table\", header = FALSE)\r\ncasttable = html_table(webpage, \".cast_list\", header = F)[[1]]\r\nhead(casttable)\r\n\r\n\r\n                                 X1                                X2\r\n1 Cast overview, first billed only: Cast overview, first billed only:\r\n2                      \\n                  \\n Tim Robbins\\n          \r\n3                      \\n               \\n Morgan Freeman\\n          \r\n4                      \\n                   \\n Bob Gunton\\n          \r\n5                      \\n               \\n William Sadler\\n          \r\n6                      \\n                 \\n Clancy Brown\\n          \r\n                                 X3\r\n1 Cast overview, first billed only:\r\n2   \\n              ...\\n          \r\n3   \\n              ...\\n          \r\n4   \\n              ...\\n          \r\n5   \\n              ...\\n          \r\n6   \\n              ...\\n          \r\n                                                                       X4\r\n1                                       Cast overview, first billed only:\r\n2            \\n            Andy Dufresne \\n                  \\n          \r\n3 \\n            Ellis Boyd 'Red' Redding \\n                  \\n          \r\n4            \\n            Warden Norton \\n                  \\n          \r\n5                  \\n            Heywood \\n                  \\n          \r\n6           \\n            Captain Hadley \\n                  \\n          \r\n\r\nAttributes of an element\r\nIf say we are also interested in extracting the links to the actor’s pages, we can acces html attributes of the selected nodes using html_attrs( ).\r\n\r\n\r\ncast_attrs = html_attrs(cast)\r\ncast_attrs[1:2]\r\n\r\n\r\n[[1]]\r\n                            href \r\n\"/name/nm0000209/?ref_=tt_cl_t1\" \r\n\r\n[[2]]\r\n                            href \r\n\"/name/nm0000151/?ref_=tt_cl_t2\" \r\n\r\nAs we can see there’s only one attribute called href which contains relative url to the actor’s page. We can extract it using html_attr(), indicating the name of the attribute of interest. Relative urls can be turned into absolute urls using url_absolute().\r\n\r\n\r\ncast_rel_urls = html_attr(cast, \"href\")\r\nlength(cast_rel_urls)\r\n\r\n\r\n[1] 15\r\n\r\ncast_rel_urls[1:2]\r\n\r\n\r\n[1] \"/name/nm0000209/?ref_=tt_cl_t1\" \"/name/nm0000151/?ref_=tt_cl_t2\"\r\n\r\ncast_abs_urls = html_attr(cast, \"href\") %>%\r\n  url_absolute(url)\r\ncast_abs_urls[1:2]\r\n\r\n\r\n[1] \"https://www.imdb.com/name/nm0000209/?ref_=tt_cl_t1\"\r\n[2] \"https://www.imdb.com/name/nm0000151/?ref_=tt_cl_t2\"\r\n\r\nMaking API Requests in R\r\nApplication Program Interface: Places where a computer interacts with another, or with itself:\r\nthe client requests data, the server provide data.\r\n\r\nApplicable R packages: httr and jsonlite\r\nhttr\r\nCreate request with GET() function. Input is a url which specifies the address of the server.\r\nExample: current number of people in space\r\n\r\n\r\nres = GET('http://api.open-notify.org/astros.json')\r\nres\r\n\r\n\r\nResponse [http://api.open-notify.org/astros.json]\r\n  Date: 2021-05-04 22:51\r\n  Status: 200\r\n  Content-Type: application/json\r\n  Size: 355 B\r\n\r\nJSON Format\r\nDifferent formats to share data on the internet\r\nCurrently, JavaScript Object notation is being widely adopted\r\n[ { “name”: “Miguel”, “student_id”: 1, “exam_1”: 85, “exam_2”: 86 }, { “name”: “Sofia”, “student_id”: 2, “exam_1”: 94, “exam_2”: 93\r\njsonlite\r\nSeveral organizations provide a JSON API or a web service\r\nThe jsonlite package provides parser and generator functions:\r\nfromJSON()\r\ntoJSON()\r\n\r\nfromJSON():\r\nInput is a JSON string, URL, JSON file\r\nReturns a list of data.frames\r\n\r\ntoJSON():\r\nInput is any object\r\nReturns a JSON string\r\n\r\nfromJSON()\r\nconvert the raw Unicode of the GET request into a JSON string\r\n\r\n\r\nrawToChar(res$content)\r\n\r\n\r\n[1] \"{\\\"number\\\": 7, \\\"message\\\": \\\"success\\\", \\\"people\\\": [{\\\"name\\\": \\\"Mark Vande Hei\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Oleg Novitskiy\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Pyotr Dubrov\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Thomas Pesquet\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Megan McArthur\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Shane Kimbrough\\\", \\\"craft\\\": \\\"ISS\\\"}, {\\\"name\\\": \\\"Akihiko Hoshide\\\", \\\"craft\\\": \\\"ISS\\\"}]}\"\r\n\r\nconvert the JSON format to a list of data.frames\r\n\r\n\r\ndata = fromJSON(rawToChar(res$content))\r\ndata\r\n\r\n\r\n$number\r\n[1] 7\r\n\r\n$message\r\n[1] \"success\"\r\n\r\n$people\r\n             name craft\r\n1  Mark Vande Hei   ISS\r\n2  Oleg Novitskiy   ISS\r\n3    Pyotr Dubrov   ISS\r\n4  Thomas Pesquet   ISS\r\n5  Megan McArthur   ISS\r\n6 Shane Kimbrough   ISS\r\n7 Akihiko Hoshide   ISS\r\n\r\nRead directly from a url\r\nfor example: read from a github page\r\n\r\n\r\ndata <- fromJSON(\"https://api.github.com/users/hadley/orgs\")\r\nclass(data)\r\n\r\n\r\n[1] \"data.frame\"\r\n\r\nnames(data)\r\n\r\n\r\n [1] \"login\"              \"id\"                 \"node_id\"           \r\n [4] \"url\"                \"repos_url\"          \"events_url\"        \r\n [7] \"hooks_url\"          \"issues_url\"         \"members_url\"       \r\n[10] \"public_members_url\" \"avatar_url\"         \"description\"       \r\n\r\ntoJSON()\r\nWrite a dataframe to JSON format\r\n\r\n\r\njsondata = toJSON(data)\r\nhead(jsondata)\r\n\r\n\r\n[1] \"[{\\\"login\\\":\\\"ggobi\\\",\\\"id\\\":423638,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjQyMzYzOA==\\\",\\\"url\\\":\\\"https://api.github.com/orgs/ggobi\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/ggobi/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/ggobi/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/ggobi/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/ggobi/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/ggobi/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/ggobi/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/423638?v=4\\\",\\\"description\\\":\\\"\\\"},{\\\"login\\\":\\\"rstudio\\\",\\\"id\\\":513560,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjUxMzU2MA==\\\",\\\"url\\\":\\\"https://api.github.com/orgs/rstudio\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/rstudio/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/rstudio/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/rstudio/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/rstudio/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/rstudio/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/rstudio/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/513560?v=4\\\",\\\"description\\\":\\\"\\\"},{\\\"login\\\":\\\"rstats\\\",\\\"id\\\":722735,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjcyMjczNQ==\\\",\\\"url\\\":\\\"https://api.github.com/orgs/rstats\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/rstats/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/rstats/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/rstats/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/rstats/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/rstats/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/rstats/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/722735?v=4\\\"},{\\\"login\\\":\\\"ropensci\\\",\\\"id\\\":1200269,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjEyMDAyNjk=\\\",\\\"url\\\":\\\"https://api.github.com/orgs/ropensci\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/ropensci/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/ropensci/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/ropensci/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/ropensci/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/ropensci/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/ropensci/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/1200269?v=4\\\",\\\"description\\\":\\\"\\\"},{\\\"login\\\":\\\"rjournal\\\",\\\"id\\\":3330561,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjMzMzA1NjE=\\\",\\\"url\\\":\\\"https://api.github.com/orgs/rjournal\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/rjournal/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/rjournal/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/rjournal/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/rjournal/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/rjournal/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/rjournal/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/3330561?v=4\\\"},{\\\"login\\\":\\\"r-dbi\\\",\\\"id\\\":5695665,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjU2OTU2NjU=\\\",\\\"url\\\":\\\"https://api.github.com/orgs/r-dbi\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/r-dbi/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/r-dbi/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/r-dbi/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/r-dbi/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/r-dbi/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/r-dbi/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/5695665?v=4\\\",\\\"description\\\":\\\"R + databases\\\"},{\\\"login\\\":\\\"RConsortium\\\",\\\"id\\\":15366137,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjE1MzY2MTM3\\\",\\\"url\\\":\\\"https://api.github.com/orgs/RConsortium\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/RConsortium/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/RConsortium/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/RConsortium/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/RConsortium/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/RConsortium/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/RConsortium/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/15366137?v=4\\\",\\\"description\\\":\\\"The R Consortium, Inc was established to provide support to the R Foundation and R Community, using maintaining and distributing R software.\\\"},{\\\"login\\\":\\\"tidyverse\\\",\\\"id\\\":22032646,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjIyMDMyNjQ2\\\",\\\"url\\\":\\\"https://api.github.com/orgs/tidyverse\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/tidyverse/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/tidyverse/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/tidyverse/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/tidyverse/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/tidyverse/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/tidyverse/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/22032646?v=4\\\",\\\"description\\\":\\\"The tidyverse is a collection of R packages that share common principles and are designed to work together seamlessly\\\"},{\\\"login\\\":\\\"r-lib\\\",\\\"id\\\":22618716,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjIyNjE4NzE2\\\",\\\"url\\\":\\\"https://api.github.com/orgs/r-lib\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/r-lib/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/r-lib/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/r-lib/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/r-lib/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/r-lib/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/r-lib/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/22618716?v=4\\\",\\\"description\\\":\\\"\\\"},{\\\"login\\\":\\\"rstudio-education\\\",\\\"id\\\":34165516,\\\"node_id\\\":\\\"MDEyOk9yZ2FuaXphdGlvbjM0MTY1NTE2\\\",\\\"url\\\":\\\"https://api.github.com/orgs/rstudio-education\\\",\\\"repos_url\\\":\\\"https://api.github.com/orgs/rstudio-education/repos\\\",\\\"events_url\\\":\\\"https://api.github.com/orgs/rstudio-education/events\\\",\\\"hooks_url\\\":\\\"https://api.github.com/orgs/rstudio-education/hooks\\\",\\\"issues_url\\\":\\\"https://api.github.com/orgs/rstudio-education/issues\\\",\\\"members_url\\\":\\\"https://api.github.com/orgs/rstudio-education/members{/member}\\\",\\\"public_members_url\\\":\\\"https://api.github.com/orgs/rstudio-education/public_members{/member}\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/34165516?v=4\\\",\\\"description\\\":\\\"\\\"}]\"\r\n\r\nAnd back from JSON format to dataframe\r\n\r\n\r\nbackagain = fromJSON(jsondata)\r\nidentical(data, backagain)\r\n\r\n\r\n[1] TRUE\r\n\r\nAPI with Query Parameters\r\nWhen is the ISS was going to pass over a given location on earth?\r\nThe ISS and the Brooklynn Bridge\r\nQuery parameters:\r\nlongitude\r\nlatitude\r\n\r\nCombine with the original URL\r\n\r\n\r\nres = GET(\"http://api.open-notify.org/iss-pass.json\",\r\nquery = list(lat = 40.7, lon = -74))\r\nres\r\n\r\n\r\nResponse [http://api.open-notify.org/iss-pass.json?lat=40.7&lon=-74]\r\n  Date: 2021-05-04 22:51\r\n  Status: 200\r\n  Content-Type: application/json\r\n  Size: 518 B\r\n{\r\n  \"message\": \"success\", \r\n  \"request\": {\r\n    \"altitude\": 100, \r\n    \"datetime\": 1620167911, \r\n    \"latitude\": 40.7, \r\n    \"longitude\": -74.0, \r\n    \"passes\": 5\r\n  }, \r\n  \"response\": [\r\n...\r\n\r\n\r\n\r\ndata = fromJSON(rawToChar(res$content))\r\ndata$response\r\n\r\n\r\n  duration   risetime\r\n1      564 1620188939\r\n2      654 1620194681\r\n3      592 1620200551\r\n4      566 1620206436\r\n5      632 1620212262\r\n\r\nCheck the documentation of the API you are using\r\nrequired and optional parameters\r\nauthentication\r\n\r\nNot every API works with query parameters\r\nNote the “?{var}=” construction in the URL of the ISS\r\nStudy the URL before building your function Distill is a publication format for scientific and technical writing, native to the web.\r\n\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-05T00:54:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-28-glmm-faq/",
    "title": "GLMM FAQ",
    "description": "an informal GLMM FAQ list for the r-sig-mixed-models mailing list",
    "author": [
      {
        "name": "Ben Bolker",
        "url": "https://github.com/Bokola"
      }
    ],
    "date": "2021-04-28",
    "categories": [
      "GLMM",
      "poisson",
      "log link",
      "logit",
      "probit"
    ],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\nIntroduction\r\nThis is an informal FAQ list for the r-sig-mixed-models mailing list.\r\nThe most commonly used functions for mixed modeling in R are\r\nlinear mixed models: aov(), nlme::lme1, lme4::lmer; brms::brm\r\ngeneralized linear mixed models (GLMMs)\r\nfrequentist: MASS::glmmPQL, lme4::glmer; glmmTMB\r\nBayesian: MCMCglmm::MCMCglmm; brms::brm\r\n\r\nnonlinear mixed models: nlme::nlme, lme4::nlmer; brms::brm\r\nGNLMMs: brms::brm\r\nAnother quick-and-dirty way to search for mixed-model related packages on CRAN:\r\n\r\n\r\ngrep(\"l.?m[me][^t]\",rownames(available.packages(repos = 'https://cran.us.r-project.org')),value=TRUE)\r\n\r\n\r\n [1] \"blmeco\"         \"buildmer\"       \"cellVolumeDist\"\r\n [4] \"climextRemes\"   \"elementR\"       \"glmertree\"     \r\n [7] \"glmmboot\"       \"glmmEP\"         \"glmmfields\"    \r\n[10] \"glmmLasso\"      \"glmmML\"         \"glmmSeq\"       \r\n[13] \"glmmsr\"         \"glmmTMB\"        \"lamme\"         \r\n[16] \"lme4\"           \"lmec\"           \"lmeInfo\"       \r\n[19] \"lmem.qtler\"     \"lmeNB\"          \"lmeNBBayes\"    \r\n[22] \"lmeresampler\"   \"lmerTest\"       \"lmeSplines\"    \r\n[25] \"lmeVarComp\"     \"lmmot\"          \"lmmpar\"        \r\n[28] \"lrmest\"         \"lsmeans\"        \"mailmerge\"     \r\n[31] \"mlmm.gwas\"      \"mlmmm\"          \"mvglmmRank\"    \r\n[34] \"nlmeODE\"        \"nlmeU\"          \"palmerpenguins\"\r\n[37] \"tlmec\"          \"vagalumeR\"     \r\n\r\nThere are some false positives here (e.g. palmerpenguins); see here if you’re interested in “regex golf”.\r\nOther sources of help\r\nthe mailing list is r-sig-mixed-models@r-project.org\r\nsign up here\r\narchives here\r\nor Google search with the tag site:https://stat.ethz.ch/pipermail/r-sig-mixed-models/\r\n\r\nThe source code of this document is available on GitHub; the rendered (HTML) version lives on GitHub pages.\r\nSearching on StackOverflow with the [r] [mixed-models] tags, or on CrossValidated with the [mixed-model] tag may be helpful (these sites also have an [lme4] tag).\r\nDISCLAIMERS:\r\n(G)LMMs are hard - harder than you may think based on what you may have learned in your second statistics class, which probably focused on picking the appropriate sums of squares terms and degrees of freedom for the numerator and denominator of an \\(F\\) test. ‘Modern’ mixed model approaches, although more powerful (they can handle more complex designs, lack of balance, crossed random factors, some kinds of non-Normally distributed responses, etc.), also require a new set of conceptual tools. In order to use these tools you should have at least a general acquaintance with classical mixed-model experimental designs but you should also, probably, read something about modern mixed model approaches. @littell_sas_2006 and @pinheiro_mixed-effects_2000 are two places to start, although Pinheiro and Bates is probably more useful if you want to use R. Other useful references include @gelman_data_2006 (focused on Bayesian methods) and @zuur_mixed_2009. If you are going to use generalized linear mixed models, you should understand generalized linear models (@dobson_introduction_2008, @faraway_extending_2006, and @McCullaghNelder1989 are standard references; the last is the canonical reference, but also the most challenging).\r\nAll of the issues that arise with regular linear or generalized-linear modeling (e.g.: inadequacy of p-values alone for thorough statistical analysis; need to understand how models are parameterized; need to understand the principle of marginality and how interactions can be treated; dangers of overfitting, which are not mitigated by stepwise procedures; the non-existence of free lunches) also apply, and can apply more severely, to mixed models.\r\nWhen SAS (or Stata, or Genstat/AS-REML or …) and R differ in their answers, R may not be wrong. Both SAS and R may be `right’ but proceeding in a different way/answering different questions/using a different philosophical approach (or both may be wrong …)\r\nThe advice in this FAQ comes with absolutely no warranty of any sort.\r\nReferences\r\nlinear mixed models\r\nweb/open\r\nUCLA IDRE statistical consulting\r\n@barr_learning_2020 Chapters 5-8\r\nbooks (dead-tree/closed)\r\npinheiro_mixed-effects_2000: LMM only.\r\n@zuur_mixed_2009: Focused on ecology.\r\n@gelman_data_2006: LMM and GLMM; Bayesian; examples from social science. Intermediate mathematics.\r\n(Rethinking)\r\nModel definition\r\nModel specification\r\nThe following formula extensions for specifying random-effects structures in R are used by\r\nlme4\r\nnlme (nested effects only, although crossed effects can be specified with more work)\r\nglmmADMB and glmmTMB\r\nMCMCglmm uses a different specification, inherited from AS-REML.\r\n(Modified from Robin Jeffries, UCLA:)\r\n\r\nformula\r\nmeaning\r\n(1|group)\r\nrandom group intercept\r\n(x|group) = (1+x|group)\r\nrandom slope of x within group with correlated intercept\r\n(0+x|group) = (-1+x|group)\r\nrandom slope of x within group: no variation in intercept\r\n(1|group) + (0+x|group)\r\nuncorrelated random intercept and random slope within group\r\n(1|site/block) = (1|site)+(1|site:block)\r\nintercept varying among sites and among blocks within sites (nested random effects)\r\nsite+(1|site:block)\r\nfixed effect of sites plus random variation in intercept among blocks within sites\r\n(x|site/block) = (x|site)+(x|site:block) = (1 + x|site)+(1+x|site:block)\r\nslope and intercept varying among sites and among blocks within sites\r\n(x1|site)+(x2|block)\r\ntwo different effects, varying at different levels\r\nx*site+(x|site:block)\r\nfixed effect variation of slope and intercept varying among sites and random variation of slope and intercept among blocks within sites\r\n(1|group1)+(1|group2)\r\nintercept varying among crossed random effects (e.g. site, year)\r\n\r\nOr in a little more detail:\r\n\r\nequation\r\nformula\r\n\\(ÃŸ_0 + ÃŸ_{1}X_{i} + e_{si}\\)\r\nn/a (Not a mixed-effects model)\r\n\\((ÃŸ_0 + b_{S,0s}) + ÃŸ_{1}X_i + e_{si}\\)\r\n~ X + (1|Subject)\r\n\\((ÃŸ_0 + b_{S,0s}) + (ÃŸ_{1} + b_{S,1s}) X_i + e_{si}\\)\r\n~ X + (1 + X|Subject)\r\n\\((ÃŸ_0 + b_{S,0s} + b_{I,0i}) + (ÃŸ_{1} + b_{S,1s}) X_i + e_{si}\\)\r\n~ X + (1 + X|Subject) + (1|Item)\r\nAs above, but \\(S_{0s}\\), \\(S_{1s}\\) independent\r\n~ X + (1|Subject) + (0 + X| Subject) + (1|Item)\r\n\\((ÃŸ_0 + b_{S,0s} + b_{I,0i}) + ÃŸ_{1}X_i + e_{si}\\)\r\n~ X + (1|Subject) + (1|Item)\r\n\\((ÃŸ_0 + b_{I,0i}) + (ÃŸ_{1} + b_{S,1s})X_i + e_{si}\\)\r\n~ X + (0 + X|Subject) + (1|Item)\r\n\r\nModified from: http://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet?lq=1 (Livius)\r\nThe magic development version of the equatiomatic package can handle mixed models (remotes::install_github(\"datalorax/equatiomatic\")), e.g.\r\n\r\n\r\nlibrary(lme4)\r\nlibrary(equatiomatic)\r\nfm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)\r\nequatiomatic::extract_eq(fm1)\r\n\r\n\r\n\\[\r\n\\begin{aligned}\r\n  \\operatorname{Reaction}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1j[i]}(\\operatorname{Days}), \\sigma^2 \\right) \\\\    \r\n\\left(\r\n  \\begin{array}{c} \r\n    \\begin{aligned}\r\n      &\\alpha_{j} \\\\\r\n      &\\beta_{1j}\r\n    \\end{aligned}\r\n  \\end{array}\r\n\\right)\r\n  &\\sim N \\left(\r\n\\left(\r\n  \\begin{array}{c} \r\n    \\begin{aligned}\r\n      &\\mu_{\\alpha_{j}} \\\\\r\n      &\\mu_{\\beta_{1j}}\r\n    \\end{aligned}\r\n  \\end{array}\r\n\\right)\r\n, \r\n\\left(\r\n  \\begin{array}{cc}\r\n     \\sigma^2_{\\alpha_{j}} & \\rho_{\\alpha_{j}\\beta_{1j}} \\\\ \r\n     \\rho_{\\beta_{1j}\\alpha_{j}} & \\sigma^2_{\\beta_{1j}}\r\n  \\end{array}\r\n\\right)\r\n \\right)\r\n    \\text{, for Subject j = 1,} \\dots \\text{,J}\r\n\\end{aligned}\r\n\\]\r\n\r\nIt doesn’t handle GLMMs (yet), but you could fit two fake models — one LMM like your GLMM but with a Gaussian response, and one GLM with the same family/link function as your GLMM but without the random effects — and put the pieces together.\r\nMore possibly useful links:\r\nRense Nieuwenhuis’s blogpost/lesson on lme4 model specification\r\nCrossValidated’s lmer cheat sheet\r\nKristoffer Magnusson’s Using R and lme/lmer to fit different two- and three-level longitudinal models\r\nShould I treat factor xxx as fixed or random?\r\nThis is in general a far more difficult question than it seems on the surface. There are many competing philosophies and definitions. For example, from @gelman_analysis_2005:\r\n\r\nBefore discussing the technical issues, we briefly review what is meant by fixed and random effects. It turns out that different—in fact, incompatible—definitions are used in different contexts. [See also Kreft and de Leeuw (1998), Section 1.3.3, for a discussion of the multiplicity of definitions of fixed and random effects and coefficients, and Robinson (1998) for a historical overview.] Here we outline five definitions that we have seen: 1. Fixed effects are constant across individuals, and random effects vary. For example, in a growth study, a model with random intercepts αi and fixed slope β corresponds to parallel lines for different individuals i, or the model yit = αi + βt. Kreft and de Leeuw [(1998), page 12] thus distinguish between fixed and random coefficients. 2. Effects are fixed if they are interesting in themselves or random if there is interest in the underlying population. Searle, Casella and McCulloch [(1992), Section 1.4] explore this distinction in depth. 3. “When a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random” [Green and Tukey (1960)]. 4. “If an effect is assumed to be a realized value of a random variable, it is called a random effect” [LaMotte (1983)]. 5. Fixed effects are estimated using least squares (or, more generally, maximum likelihood) and random effects are estimated with shrinkage [“linear unbiased prediction” in the terminology of Robinson (1991)]. This definition is standard in the multilevel modeling literature [see, e.g., Snijders and Bosker (1999), Section 4.2] and in econometrics.\r\n\r\nAnother useful comment (via Kevin Wright) reinforcing the idea that “random vs. fixed” is not a simple, cut-and-dried decision: from @schabenberger_contemporary_2001, p. 627:\r\n\r\nBefore proceeding further with random field linear models we need to remind the reader of the adage that one modeler’s random effect is another modeler’s fixed effect.\r\n\r\n@clark2015should address this question from a mostly econometric perspective, focusing mostly on practical variance/bias/RMSE criteria.\r\nOne point of particular relevance to ‘modern’ mixed model estimation (rather than ‘classical’ method-of-moments estimation) is that, for practical purposes, there must be a reasonable number of random-effects levels (e.g. blocks) – more than 5 or 6 at a minimum. This is not surprising if you consider that random effects estimation is trying to estimate an among-block variance. For example, from @Crawley2002 p. 670:\r\n\r\nAre there enough levels of the factor in the data on which to base an estimate of the variance of the population of effects? No, means [you should probably treat the variable as] fixed effects.\r\n\r\nSome researchers (who treat fixed vs random as a philosophical rather than a pragmatic decision) object to this approach.\r\nAlso see a very thoughtful chapter in @hodges_richly_2016.\r\nTreating factors with small numbers of levels as random will in the best case lead to very small and/or imprecise estimates of random effects; in the worst case it will lead to various numerical difficulties such as lack of convergence, zero variance estimates, etc.. (A small simulation exercise shows that at least the estimates of the standard deviation are downwardly biased in this case; it’s not clear whether/how this bias would affect the point estimates of fixed effects or their estimated confidence intervals.) In the classical method-of-moments approach these problems may not arise (because the sums of squares are always well defined as long as there are at least two units), but the underlying problems of lack of power are there nevertheless.\r\nAlso see this thread on the r-sig-mixed-models mailing list.\r\nNested or crossed?\r\n\r\nRelatively few mixed effect modeling packages can handle crossed random effects, i.e. those where one level of a random effect can appear in conjunction with more than one level of another effect. (This definition is confusing, and I would happily accept a better one.) A classic example is crossed temporal and spatial effects. If there is random variation among temporal blocks (e.g. years) ‘’and’’ random variation among spatial blocks (e.g. sites), ‘’and’’ if there is a consistent year effect across sites and ‘’vice versa’’, then the random effects should be treated as crossed.\r\nlme4 does handled crossed effects, efficiently; if you need to deal with crossed REs in conjunction with some of the features that nlme offers (e.g. heteroscedasticity of residuals via weights/varStruct, correlation of residuals via correlation/corStruct, see p. 163ff of @pinheiro_mixed-effects_2000 (section 4.2.2: Google books link)\r\nI rarely find it useful to think of fixed effects as “nested” (although others disagree); if for example treatments A and B are only measured in block 1, and treatments C and D are only measured in block 2, one still assumes (because they are fixed effects) that each treatment would have the same effect if applied in the other block. (One might like to estimate treatment-by-block interactions, but in this case the experimental design doesn’t allow it; one would have to have multiple treatments measured within each block, although not necessarily all treatments in every block.) One would code this analysis as response~treatment+(1|block) in lme4. Also, in the case of fixed effects, crossed and nested specifications change the parameterization of the model, but not anything else (e.g. the number of parameters estimated, log-likelihood, model predictions are all identical). That is, in R’s model.matrix function (which implements a version of Wilkinson-Rogers notation) a*b and a/b (which expand to 1+a+b+a:b and 1+a+a:b respectively) give model matrices with the same number of columns.\r\nWhether you explicitly specify a random effect as nested or not depends (in part) on the way the levels of the random effects are coded. If the ‘lower-level’ random effect is coded with unique levels, then the two syntaxes (1|a/b) (or (1|a)+(1|a:b)) and (1|a)+(1|b) are equivalent. If the lower-level random effect has the same labels within each larger group (e.g. blocks 1, 2, 3, 4 within sites A, B, and C) then the explicit nesting (1|a/b) is required. It seems to be considered best practice to code the nested level uniquely (e.g. A1, A2, …, B1, B2, …) so that confusion between nested and crossed effects is less likely.\r\n(When) can I include a predictor as both fixed and random?\r\nSee blog post by Thierry Onkelinx\r\nModel extensions\r\nOverdispersion\r\n\r\nTesting for overdispersion/computing overdispersion factor\r\nwith the usual caveats, plus a few extras – counting degrees of freedom, etc. – the usual procedure of calculating the sum of squared Pearson residuals and comparing it to the residual degrees of freedom should give at least a crude idea of overdispersion. The following attempt counts each variance or covariance parameter as one model degree of freedom and presents the sum of squared Pearson residuals, the ratio of (SSQ residuals/rdf), the residual df, and the \\(p\\)-value based on the (approximately!!) appropriate \\(\\chi^2\\) distribution. Do PLEASE note the usual, and extra, caveats noted here: this is an APPROXIMATE estimate of an overdispersion parameter. Even in the GLM case, the expected deviance per point equaling 1 is only true as the distribution of individual deviates approaches normality, i.e. the usual \\(\\lambda>5\\) rules of thumb for Poisson values and \\(\\textrm{min}(Np, N(1-p)) > 5\\) for binomial values (e.g. see @venables_modern_2002, p. 208-209). (And that’s without the extra complexities due to GLMM, i.e. the “effective” residual df should be large enough to make the sums of squares converge on a \\(\\chi^2\\) distribution …)\r\nRemember that (1) overdispersion is irrelevant for models that estimate a scale parameter (i.e. almost anything but Poisson or binomial: Gaussian, Gamma, negative binomial …) and (2) overdispersion is not estimable (and hence practically irrelevant) for Bernoulli models (= binary data = binomial with \\(N=1\\)).\r\nThe recipes below may need adjustment for some of the more complex model types allowed by glmmTMB (e.g. zero-inflation/variable dispersion), where it’s less clear what to measure to estimate overdispersion.\r\nThe following function should work for a variety of model types (at least glmmADMB, glmmTMB, lme4, …).\r\n\r\n\r\noverdisp_fun <- function(model) {\r\n    rdf <- df.residual(model)\r\n    rp <- residuals(model,type=\"pearson\")\r\n    Pearson.chisq <- sum(rp^2)\r\n    prat <- Pearson.chisq/rdf\r\n    pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)\r\n    c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)\r\n}\r\n\r\n\r\n\r\nExample:\r\n\r\n\r\nlibrary(lme4)\r\nlibrary(glmmTMB)\r\n\r\n\r\n\r\n\r\n\r\nset.seed(101)  \r\nd <- data.frame(x=runif(1000),\r\n                f=factor(sample(1:10,size=1000,replace=TRUE)))\r\nsuppressMessages(d$y <- simulate(~x+(1|f), family=poisson,\r\n                          newdata=d,\r\n                          newparams=list(theta=1,beta=c(0,2)))[[1]])\r\nm1 <- glmer(y~x+(1|f),data=d,family=poisson)\r\noverdisp_fun(m1)\r\n\r\n\r\n       chisq        ratio          rdf            p \r\n1035.9966325    1.0391140  997.0000000    0.1902294 \r\n\r\nm2 <- glmmTMB(y~x+(1|f),data=d,family=\"poisson\")\r\noverdisp_fun(m2)\r\n\r\n\r\n       chisq        ratio          rdf            p \r\n1035.9961394    1.0391135  997.0000000    0.1902323 \r\n\r\nThe gof function in the aods3 provides similar functionality (it reports both deviance- and \\(\\chi^2\\)-based estimates of overdispersion and tests).\r\nFitting models with overdispersion?\r\nquasilikelihood estimation: MASS::glmmPQL. Quasi- was deemed unreliable in lme4, and is no longer available. (Part of the problem was questionable numerical results in some cases; the other problem was that DB felt that he did not have a sufficiently good understanding of the theoretical framework that would explain what the algorithm was actually estimating in this case.) geepack::geelgm may be workable (haven’t tried it)\r\nIf you really want quasi-likelihood analysis for glmer fits, you can do it yourself by adjusting the coefficient table - i.e., by multiplying the standard error by the square root of the dispersion factor 2 and recomputing the \\(Z\\)- and \\(p\\)-values accordingly, as follows:\r\n\r\n\r\n## extract summary table; you may also be able to do this via\r\n##  broom::tidy or broom.mixed::tidy\r\nquasi_table <- function(model,ctab=coef(summary(model)),\r\n                           phi=overdisp_fun(model)[\"ratio\"]) {\r\n    qctab <- within(as.data.frame(ctab),\r\n    {   `Std. Error` <- `Std. Error`*sqrt(phi)\r\n        `z value` <- Estimate/`Std. Error`\r\n        `Pr(>|z|)` <- 2*pnorm(abs(`z value`), lower.tail=FALSE)\r\n    })\r\n    return(qctab)\r\n}\r\nprintCoefmat(quasi_table(m1),digits=3)\r\n\r\n\r\n            Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)   0.2277     0.2700    0.84      0.4    \r\nx             2.0640     0.0528   39.11   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n## to use this with glmmTMB, we need to separate out the\r\n##  conditional component of the summary\r\nprintCoefmat(quasi_table(m2,\r\n                         ctab=coef(summary(m2))[[\"cond\"]]),\r\n             digits=3)\r\n\r\n\r\n            Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)   0.2277     0.2700    0.84      0.4    \r\nx             2.0640     0.0528   39.09   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nAnother version, this one tidyverse-centric:\r\n\r\n\r\nlibrary(broom.mixed)\r\nlibrary(dplyr)\r\ntidy_quasi <- function(model, phi=overdisp_fun(model)[\"ratio\"],\r\n                       conf.level=0.95) {\r\n    tt <- (tidy(model, effects=\"fixed\")\r\n        %>% mutate(std.error=std.error*sqrt(phi),\r\n                   statistic=estimate/std.error,\r\n                   p.value=2*pnorm(abs(statistic), lower.tail=FALSE))\r\n    )\r\n    return(tt)\r\n}\r\ntidy_quasi(m1)\r\n\r\n\r\n# A tibble: 2 x 6\r\n  effect term        estimate std.error statistic p.value\r\n  <chr>  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\r\n1 fixed  (Intercept)    0.228    0.270      0.843   0.399\r\n2 fixed  x              2.06     0.0528    39.1     0    \r\n\r\ntidy_quasi(m2)\r\n\r\n\r\n# A tibble: 2 x 7\r\n  effect component term        estimate std.error statistic p.value\r\n  <chr>  <chr>     <chr>          <dbl>     <dbl>     <dbl>   <dbl>\r\n1 fixed  cond      (Intercept)    0.228    0.270      0.843   0.399\r\n2 fixed  cond      x              2.06     0.0528    39.1     0    \r\n\r\nThese functions make some simplifying assumptions: (1) this overdispersion computation is approximate\r\nIn this case using quasi-likelihood doesn’t make much difference, since the data we simulated in the first place were Poisson.) Keep in mind that once you switch to quasi-likelihood you will either have to eschew inferential methods such as the likelihood ratio test, profile confidence intervals, AIC, etc., or make more heroic assumptions to compute “quasi-” analogs of all of the above (such as QAIC).\r\nobservation-level random effects (OLRE: this approach should work in most packages). If you want to a citation for this approach, try @elston_analysis_2001, who cite @lawson_disease_1999; apparently there is also an example in section 10.5 of @maindonald_data_2010, and (according to an R-sig-mixed-models post) this is also discussed by @rabehesketh_multilevel_2008. Also see @browne_variance_2005 for an example in the binomial context (i.e. logit-normal-binomial rather than lognormal-Poisson). Agresti’s excellent (2002) book @agresti_categorical_2002 also discusses this (section 13.5), referring back to @breslow_extrapoisson_1984 and @hinde_compound_1982. [Notes: (a) I haven’t checked all these references myself, (b) I can’t find the reference any more, but I have seen it stated that observation-level random effect estimation is probably dodgy for PQL approaches as used in Elston et al 2001]\r\nalternative distributions\r\nPoisson-lognormal model for counts or binomial-logit-Normal model for proportions (see above, “observation-level random effects”)\r\nnegative binomial for counts or beta-binomial for proportions\r\nlme4::glmer.nb() should fit a negative binomial, although it is somewhat slow and fragile compared to some of the other methods suggested here. lme4 cannot fit beta-binomial models (these cannot be formulated as a part of the exponential family of distributions)\r\nglmmTMB will fit two parameterizations of the negative binomial: family=\"nbinom2\" gives the classic parameterization with \\(\\sigma^2=\\mu(1+\\mu/k)\\) (“NB2” in Hardin and Hilbe’s terminology) while family=\"nbinom1\" gives a parameterization with \\(\\sigma^2=\\phi \\mu\\), \\(\\phi>1\\) (“NB1” to Hardin and Hilbe). The latter might also be called a “quasi-Poisson” parameterization because it matches the mean-variance relationship assumed by quasi-Poisson models, i.e. the variance is strictly proportional to the mean (although the proportionality constant must be >1, a limitation that does not apply to quasi-likelihood approaches). (glmmADMB will also fit these models, with family=\"nbinom\" for NB2, but is deprecated in favour of glmmTMB.)\r\nglmmTMB allows beta-binomial models ([@harrison_comparison_2015] suggests comparing beta-binomial with OLRE models to assess reliability)\r\nthe brms package has a negbinomial family (no beta-binomial, but it does have a wide range of other families)\r\n\r\n\r\nother packages/approaches (less widely used, or requiring a bit more effort)\r\ngamlss.mx:gamlssNP\r\nWinBUGS/JAGS (via R2WinBUGS/Rjags)\r\nAD Model Builder (possibly via R2admb package) or TMB\r\ngnlmm in the repeated package (off-CRAN)\r\nASREML\r\n\r\nNegative binomial models in glmmTMB and lognormal-Poisson models in glmer (or MCMCglmm) are probably the best quick alternatives for overdispersed count data. If you need to explore alternatives (different variance-mean relationships, different distributions), then ADMB, TMB, WinBUGS, Stan, NIMBLE are the most flexible alternatives.\r\nUnderdispersion\r\nUnderdispersion (much less variability than expected) is a less common problem than overdispersion.\r\nmild overdispersion is sometimes ignored, since it tends in general to lead to conservative rather than anti-conservative results\r\nquasi-likelihood (and the quasi-hack listed above) can handle under- as well as underdispersion\r\nsome other solutions exist, but are less widely implemented\r\nfor distributions with a small range (e.g. litter sizes of large mammals), one can treat responses as ordinal (e.g. using the ordinal package, or MCMCglmm or brms for Bayesian solutions)\r\nthe COM-Poisson distribution and generalized Poisson distributions, implemented in glmmTMB, can handle underdispersion (J. Hilbe recommends the latter in this CrossValidated answer). (VGAM has a generalized Poisson distribution, but doesn’t handle random effects.)\r\n\r\nGamma GLMMs\r\nWhile one (well, OK I) would naively think that GLMMs with Gamma distributions would be just as easy (or hard) as any other sort of GLMMs, it seems that they are in fact harder to implement. Basic simulated examples of Gamma GLMMs can fail in lme4 despite analogous problems with Poisson, binomial, etc. distributions. Solutions: - the default inverse link seems particularly problematic; try other links (especially family=Gamma(link=\"log\")) if that is possible/makes sense - consider whether a lognormal model (i.e. a regular LMM on logged data) would work/makes sense. - @lo_transform_2015 argue that the Gamma family with an identity link is superior to lognormal models for reaction-time data. I (BMB) don’t find their argument particularly convincing, but lots of people want to do this. Unfortunately this is technically challenging (see here), because it is likely that some “illegal” values (predicted responses \\(\\le 0\\)) will occur while fitting the model, even if the final fitted model makes no impossible predictions. Thus something has to be done to make the model-fitting machinery tolerant of such values (i.e. returning NA for these model evaluations, or clamping illegal values to the constrained space with an appropriate smooth penalty function).\r\nGamma models can be fitted by a wide variety of platforms (lme4::glmer, MASS::glmmPQL, glmmADMB, glmmTMB, MixedModels.jl, MCMCglmm, brms … not sure about others.\r\nBeta GLMMs\r\nProportion data where the denominator (e.g. maximum possible number of successes for a given observation) is not known can be modeled using a Beta distribution. @smithson_better_2006 is a good introduction for non-statisticians (not in the mixed-model case), and the betareg package [@cribari-neto_beta_2009] handles non-mixed Beta regressions. The glmmTMB and brms packages handle Beta mixed models (brms also handles zero-inflated and zero-one inflated models).\r\nZero-inflation\r\nSee e.g. @martin_zero_2005 or @warton_many_2005 (“many zeros does not mean zero inflation”) or @zuur_zero-truncated_2009 for general information on zero-inflation.\r\nCount data\r\nMCMCglmm handles zero-truncated, zero-inflated, and zero-altered models, although specifying the models is a little bit tricky: see Sections 5.3 to 5.5 of the CourseNotes vignette\r\nglmmADMB handles\r\nzero-inflated models (with a single zero-inflation parameter – i.e., the level of zero-inflation is assumed constant across the whole data set)\r\ntruncated Poisson and negative binomial distributions (which allows two-stage fitting of hurdle models)\r\n\r\nglmmTMB handles a variety of Z-I and Z-T models (allows covariates, and random effects, in the zero-alteration model)\r\nbrms does too\r\nso does GLMMadaptive\r\nGavin Simpson has a detailed writeup showing that mgcv::gam() can do simple mixed models (Poisson, not NB) with zero-inflation, and comparing mgcv with glmmTMB results\r\ngamlssNP in the gamlss.mx package should handle zero-inflation, and the gamlss.tr package should handle truncated (i.e. hurdle) models – but I haven’t tried them\r\nroll-your-own: ADMB/R2admb, WinBUGS/R2WinBUGS, TMB, Stan, …\r\nContinuous data\r\nContinuous data are a special case where the mixture model for zero-inflated data is less relevant, because observations that are exactly zero occur with probability (but not probability density) zero. There are two cases of interest:\r\nProbability density of \\(x\\) zero or infinite\r\nIn this case zero is a problematic observation for the distribution; it’s either impossible or infinitely (locally) likely. Some examples:\r\nGamma distribution: probability density at zero is infinite (if shape<1) or zero (if shape>1); it’s finite only for an exponential distribution (shape==1)\r\nLognormal distribution: the probability density at zero is zero.\r\nBeta distribution: the probability densities at 0 and 1 are zero (if the corresponding shape parameter is >1) or infinite (if shape<1)\r\nThe best solution depends very much on the data-generating mechanism.\r\nIf the bad (0/1) values are generated by rounding (e.g. proportions that are too close to the boundaries are reported as being on the boundaries), the simplest solution is to “squeeze” these in slightly, e.g. \\(y \\to (y +a)/2a\\) for some sensible value of \\(a\\) [@smithson_better_2006]\r\nIf you think that zero values are generated by a separate process, the simplest solution is to fit a Bernoulli model to the zero/non-zero data, then a conditional continuous model for the non-zero values; this is effectively a hurdle model.\r\nyou might have censored data where all values below a certain limit (e.g. a detection limit) are recorded as zero; in this case you might be able to use survreg() and frailty() in the survival package for random-intercept models (as suggested on r-help by Thomas Lumley in 2003 or on StackOverflow by user 42- in 2014. The lmec package handles linear mixed models.\r\nThe cplm package handles ‘Tweedie compound Poisson linear models’, which in a particular range of parameters allows for skewed continuous responses with a spike at zero\r\nProbability density of \\(x\\) positive and finite\r\nIn this case (e.g. a spike of zeros in the center of an otherwise continuous distribution), the hurdle model probably makes the most sense.\r\nTests for zero-inflation\r\nyou can use a likelihood ratio test between the regular and zero-inflated version of the model, but be aware of boundary issues (search “boundary” elsewhere on this page …) – the null value (no zero inflation) is on the boundary of the feasible space\r\nyou can use AIC or variations, with the same caveats\r\nyou can use Vuong’s test, which is often recommended for testing zero-inflation in GLMs, because under some circumstances the various model flavors under consideration (hurdle vs zero-inflated vs “vanilla”) are not nested. Vuong’s test is implemented (and referenced) in the pscl package, and should be feasible to implement for GLMMs, but I don’t know of an implementation. Someone should let me (BMB) know if they find one.\r\ntwo untested but reasonable approaches:\r\nuse a simulate() method if it exists to construct a simulated distribution of the proportion of zeros expected overall from your model, and compare it to the observed proportion of zeros in the data set\r\ncompute the probability of a zero for each observation. On the basis of (conditionally) independent Bernoulli trials, compute the expected number of zeros and the confidence intervals – compare it with the observed number.\r\n\r\nSpatial and temporal correlation models, heteroscedasticity (“R-side” models)\r\nIn nlme these so-called R-side (R for “residual”) structures are accessible via the weights/VarStruct (heteroscedasticity) and correlation/corStruct (spatial or temporal correlation) arguments and data structures. This extension is a bit harder than it might seem. In LMMs it is a natural extension to allow the residual error terms to be components of a single multivariate normal draw; if that MVN distribution is uncorrelated and homoscedastic (i.e. proportional to an identity matrix) we get the classic model, but we can in principle allow it to be correlated and/or heteroscedastic.\r\nIt is not too hard to define marginal correlation structures that don’t make sense. One class of reasonably sensible models is to always assume an observation-level random effect (as MCMCglmm does for computational reasons) and to allow that random effect to be MVN on the link scale (so that the full model is lognormal-Poisson, logit-normal binomial, etc., depending on the link function and family).\r\nFor example, a relatively simple Poisson model with spatially correlated errors might look like this:\r\n\\[\r\n\\begin{split}\r\n\\eta & \\sim \\textrm{MVN}(a + b x, \\Sigma) \\\\\r\n\\Sigma_{ij} & = \\sigma^2 \\exp(-d_{ij}/s) \\\\\r\ny_i & \\sim \\textrm{Poisson}(\\lambda=\\exp(\\eta_i))\r\n\\end{split}\r\n\\]\r\nThat is, the marginal distributions of the response values are Poisson-lognormal, but on the link (log) scale the latent Normal variables underlying the response are multivariate normal, with a variance-covariance matrix described by an exponential spatial correlation function with scale parameter \\(s\\).\r\nHow can one achieve this?\r\nThese types of models are not implemented in lme4, for either LMMs or GLMMs; they are fairly low priority, and it is hard to see how they could be implemented for GLMMs (the equivalent for LMMs is tedious but should be straightforward to implement).\r\nFor LMMs, you can use the spatial/temporal correlation structures that are built into (n)lme\r\nYou can use the spatial/temporal correlation structures available for (n)lme, which include basic geostatistical (space) and ARMA-type (time) models.\r\n\r\n\r\nlibrary(sos)\r\nfindFn(\"corStruct\")\r\n\r\n\r\n\r\n\r\nfinds additional possibilities in the ramps (extended geostatistical) and ape (phylogenetic) packages.\r\nYou can use these structures in GLMMs via MASS::glmmPQL (see Dormann et al.)\r\ngeepack::geeglm\r\ngeoR, geoRglm (power tools); these are mostly designed for fitting spatial random field GLMMs via MCMC – not sure that they do random effects other than the spatial random effect\r\nR-INLA (super-power tool)\r\nit is possible to use AD Model Builder to fit spatial GLMMs, as shown in these AD Model Builder examples; this capability is not in the glmmADMB package (and may not be for a while!), but it would be possible to run AD Model Builder via the R2admb package (requires installing – and learning! ADMB)\r\ngeoBUGS, the geostatistical/spatial correlation module for WinBUGS, is another alternative (but again requires going outside of R)\r\nPenalization/handling complete separation\r\nComplete separation occurs in a binary-response model when there is some linear combination of the parameters that perfectly separates failures from successes - for example, when all of the observations are zero for some particular combination of categories. The symptoms of this problem are unrealistically large parameter estimates; ridiculously large Wald standard errors (the Hauck-Donner effect); and various warnings.\r\nIn particular, binomial glmer() models with complete separation can lead to “Downdated VtV is not positive definite” (e.g. see here) or “PIRLS step-halvings failed to reduce deviance in pwrssUpdate” errors (e.g. see here). Roughly speaking, the complete separation is likely to appear even if one considers only the fixed effects part of the model (counterarguments or counterexamples welcome!), suggesting two quick-and-dirty diagnostic methods. If fixed_form is the formula including only the fixed effects:\r\nsummary(g1 <- glm(fixed_form, family=binomial, data=...)) will show one or more of the following symptoms:\r\nwarnings that glm.fit: fitted probabilities numerically 0 or 1 occurred\r\nparameter estimates of large magnitude (e.g. any(abs(g1)>8), assuming that predictors are either categorical or scaled to have standard deviations of \\(\\approx 1\\))\r\nextremely large Wald standard errors, and large p-values (Hauck-Donner effect)\r\nthe brglm2 package has a method for detecting complete separation: library(\"brglm2\"); glm(fixed_form, data = ..., family = binomial, method=\"detect_separation\"). This should say whether complete separation occurs, and in which (combinations of) variables, e.g.\r\n\r\nSeparation: TRUE \r\nExistence of maximum likelihood estimates\r\n(Intercept)      height \r\n        Inf         Inf \r\n0: finite value, Inf: infinity, -Inf: -infinity\r\nIf complete separation is occurring between categories of a single categorical fixed-effect predictor with a large number of levels, one option would be to treat this fixed effect as a random effect, which will allow some degree of shrinkage to the mean. (It might be reasonable to specify the variance of this term a priori to a large value [minimal shrinkage], rather than trying to estimate it from the data.)\r\n(TODO: worked example)\r\nThe general approach to handling complete separation in logistic regression is called penalized regression; it’s available in the brglm, brglm2, logistf, and rms packages. However, these packages don’t handle mixed models, so the best available general approach is to use a Bayesian method that allows you to set a prior on the fixed effects, e.g. a Gaussian with standard deviation of 3; this can be done in any of the Bayesian GLMM packages (e.g. blme, MCMCglmm, brms, …) (See supplementary material for Fox et al. 2016 for a worked example.)\r\nNon-Gaussian random effects\r\nI’m not aware of easy ways to fit mixed models with non-Gaussian random effects distributions in R (i.e., convenient, flexible, well-tested implementations). @mcculloch_misspecifying_2011 discusses when this misspecification may be important. This presentation discusses various approaches to solving the problem (e.g. using a Gamma rather than a Normal distribution of REs in log-link models). The spaMM package implements H-likelihood models [@lee_generalized_2017], and claims to allow a range of random-effects distributions (perhaps not well tested though …)\r\nIn principle you can implement any random-effects distribution you want in a fully capable Bayesian modeling language (e.g. JAGS/Stan/PyMC/etc.); see e.g. this StackOverflow answer, which uses the rethinking package’s interface to Stan.\r\nEstimation\r\nWhat methods are available to fit (estimate) GLMMs?\r\n(adapted from Bolker et al TREE 2009)\r\n\r\nMethod\r\nAdvantages\r\nDisadvantages\r\nPackages\r\nPenalized quasi-likelihood\r\nFlexible, widely implemented\r\nLikelihood inference may be inappropriate; biased for large variance or small means\r\nPROC GLIMMIX (SAS), GLMM (GenStat), glmmPQL (R:MASS), ASREML-R\r\nLaplace approximation\r\nMore accurate than PQL\r\nSlower and less flexible than PQL\r\nglmer (R:lme4,lme4a), glmm.admb (R:glmmADMB), INLA, glmmTMB, AD Model Builder, HLM\r\nGauss-Hermite quadrature\r\nMore accurate than Laplace\r\nSlower than Laplace; limited to 2-3 random effects\r\nPROC NLMIXED (SAS), glmer (R:lme4, lme4a), glmmML (R:glmmML), xtlogit (Stata)\r\nMarkov chain Monte Carlo\r\nHighly flexible, arbitrary number of random effects; accurate\r\nSlow, technically challenging, Bayesian framework\r\nMCMCglmm (R:MCMCglmm), rstanarm (R), brms (R), MCMCpack (R), WinBUGS/OpenBUGS (R interface: BRugs/R2WinBUGS), JAGS (R interface: rjags/R2jags), AD Model Builder (R interface: R2admb), glmm.admb (post hoc MCMC after Laplace fit) (R:glmmADMB)\r\n\r\nTroubleshooting\r\ndouble-check the model specification and the data for mistakes\r\ncenter and scale continuous predictor variables (e.g. with scale())\r\ntry all available optimizers (e.g. several different implementations of BOBYQA and Nelder-Mead, L-BFGS-B from optim, nlminb(), …). While this will of course be slow for large fits, we consider it the gold standard; if all optimizers converge to values that are practically equivalent (it’s up to the user to decide what “practically equivalent means for their case”), then we would consider the model fit to be good enough. For example:\r\n\r\n\r\nmodelfit.all <- lme4::allFit(model)\r\nss <- summary(modelfit.all)\r\n\r\n\r\n\r\n\r\nConvergence warnings\r\nMost of the current advice about troubleshooting lme4 convergence problems can be found in the help page ?convergence. That page explains that the convergence tests in the current version of lme4 (1.1-11, February 2016) generate lots of false positives. We are considering raising the gradient warning threshold to 0.01 in future releases of lme4. In addition to the general troubleshooting tips above:\r\ndouble-check the Hessian calculation with the more expensive Richardson extrapolation method (see examples)\r\nrestart the fit from the apparent optimum, or from a point perturbed slightly away from the optimum (getME(model,c(\"theta\",\"beta\")) should retrieve the parameters in a form suitable to be used as the start parameter)\r\na common error is to specify an offset to a log-link model as a raw searching-effort value, i.e. offset(effort) rather than offset(log(effort)). While the intention is to fit a model where \\(\\textrm{counts} \\propto \\textrm{effort}\\), specifying offset(effort) leads to a model where \\(\\textrm{counts} \\propto \\exp(\\textrm{effort})\\) instead; exp(effort) is often a huge (and model-destabilizing) number.\r\n \r\nSingular models: random effect variances estimated as zero, or correlations estimated as +/- 1\r\nIt is very common for overfitted mixed models to result in singular fits. Technically, singularity means that some of the \\(\\boldsymbol \\theta\\) (variance-covariance Cholesky decomposition) parameters corresponding to diagonal elements of the Cholesky factor are exactly zero, which is the edge of the feasible space, or equivalently that the variance-covariance matrix has some zero eigenvalues (i.e. is positive semidefinite rather than positive definite), or (almost equivalently) that some of the variances are estimated as zero or some of the correlations are estimated as +/-1. This commonly occurs in two scenarios:\r\nsmall numbers of random-effect levels (e.g. <5), as illustrated in these simulations and discussed (in a somewhat different, Bayesian context) by @gelman_prior_2006.\r\ncomplex random-effects models, e.g. models of the form (f|g) where f is a categorical variable with a relatively large number of levels, or models with several different random-slopes terms.\r\nWhen using lme4, singularity is most obviously detectable in the output of summary.merMod() or VarCorr.merMod() when a variance is estimated as 0 (or very small, i.e. orders of magnitude smaller than other variance components) or when a correlation is estimated as exactly \\(\\pm 1\\). However, as pointed out by @bates_parsimonious_2015, singularities in larger variance-covariance matrices can be hard to detect: checking for small values among the diagonal elements of the Cholesky factor is a good start.\r\n\r\n\r\ntheta <- getME(model,\"theta\")\r\n## diagonal elements are identifiable because they are fitted\r\n##  with a lower bound of zero ...\r\ndiag.element <- getME(model,\"lower\")==0\r\nany(theta[diag.element]<1e-5)\r\n\r\n\r\n\r\n\r\nAs of lme4 version 1.1-19, this functionality is available as isSingular(model). - In MCMCglmm, singular or near-singular models will provoke an error and a requirement to specify a stronger prior.\r\nAt present there are a variety of strong opinions about how to resolve such problems. Briefly:\r\n@barr_random_2013 suggest always starting with the maximal model (i.e. the most random-effects component of the model that is theoretically identifiable given the experimental design) and then dropping terms when singularity or non-convergence occurs (please see the paper for detailed recommendations …)\r\n@matuschek_balancing_2017 and @bates_parsimonious_2015 strongly disagree, suggesting that models should be simplified a priori whenever possible; they also provide tools for diagnosing and mitigating singularity.\r\nOne alternative (suggested by Robert LaBudde) for the small-numbers-of-levels scenario is to “fit the model with the random factor as a fixed effect, get the level coefficients in the sum to zero form, and then compute the standard deviation of the coefficients.” This is appropriate for users who are (a) primarily interested in measuring variation (i.e. the random effects are not just nuisance parameters, and the variability [rather than the estimated values for each level] is of scientific interest), (b) unable or unwilling to use other approaches (e.g. MCMC with half-Cauchy priors in WinBUGS), (c) unable or unwilling to collect more data. For the simplest case (balanced, orthogonal, nested designs with normal errors) these estimates of standard deviations should equal the classical method-of-moments estimates.\r\nBayesian approaches allow the user to specify a informative prior that avoids singularity.\r\nThe blme package [@chung_nondegenerate_2013] provides a wrapper for the lme4 machinery that adds a particular form of weak prior to get an approximate a Bayesian maximum a posteriori estimate that avoids singularity.\r\nThe MCMCglmm package allows for priors on the variance-covariance matrix\r\nThe rstanarm and brms packages provide wrappers for the Stan Hamiltonian MCMC engine that fit GLMMs via lme4 syntax, again allowing a variety of priors to be set.\r\n\r\nIf a variance component is zero, dropping it from the model will have no effect on any of the estimated quantities (although it will affect the AIC, as the variance parameter is counted even though it has no effect). @pasch_interspecific_2013 gives one example where random effects were dropped because the variance components were consistently estimated as zero. Conversely, if one chooses for philosophical grounds to retain these parameters, it won’t change any of the answers.\r\nSetting residual variances to a fixed value (zero or other)\r\nFor some problems it would be convenient to be able to set the residual variance term to zero, or a fixed value. This is difficult in lme4, because the model is parameterized internally in such a way that the residual variance is profiled out (i.e., calculated directly from a residual deviance term) and the random-effects variances are scaled by the residual variance.\r\nSearching the r-sig-mixed-models list for “fix residual variance”\r\nThis is done in the metafor package, for meta-analytic models\r\nYou can use the blme package to fix the residual variance: from Vincent Dorie,\r\nlibrary(blme)\r\nblmer(formula = y ~ 1 + (1 | group), weights = V,\r\n      resid.prior = point(1.0), cov.prior = NULL)\r\nThis sets the residual variance to 1.0. You cannot use this to make it exactly zero, but you can make it very small (and experiment with setting it to different small values, e.g. 0.001 vs 0.0001, to see how sensitive the results are). - Similarly, you can fix the residual variance to a small positive value in [n]lme via the control() argument [@heisterkamp_update_2017]:\r\n\r\n\r\nnlme::lme(Reaction~Days,random=~1|Subject,\r\n          data=lme4::sleepstudy,\r\n          control=list(sigma=1e-8))\r\n\r\n\r\n\r\nthe glmmTMB package can set the residual variance to zero, by specifying dispformula = ~0\r\nThere is an rrBlupMethod6 package on CRAN (“Re-parametrization of mixed model formulation to allow for a fixed residual variance when using RR-BLUP for genom[e]wide estimation of marker effects”), but it seems fairly special-purpose.\r\nit might be possible in principle to adapt lme4’s internal devfun2() function (used in the likelihood profiling computation for LMMs), which uses a specified value of the residual standard deviation in computing likelihood, but as @bates_fitting_2015 say:\r\n\r\nThe resulting function is not useful for general nonlinear optimization — one can easily wander into parameter regimes corresponding to infeasible (non-positive semidefinite) variance-covariance matrices — but it serves for likelihood profiling, where one focal parameter is varied at a time and the optimization over the other parameters is likely to start close to an optimum.\r\n\r\nOther problems/lme4 error messages\r\nMost of the following error messages are relatively unusual, and happen mostly with complex/large/unstable models. There is often no simple fix; the standard suggestions for troubleshooting are (1) try rescaling and/or centering predictors; (2) see if a simpler model can be made to work; (3) look for severe lack of balance and/or complete separation in the data set.\r\nPIRLS step-halvings failed to reduce deviance in pwrssUpdate\r\nthis can also occur due to complete or quasi-complete separation (see Penalization/handling complete separation\r\nWhen using lme4 to fit GLMMs with link functions that do not automatically constrain the response to the allowable range of the distributional family (e.g. binomial models with a log link, where the estimated probability can be >1, or inverse-Gamma models, where the estimated mean can be negative), it is not unusual to get this error. This occurs because lme4 doesn’t do anything to constrain the predicted values, so NaN values pop up, which aren’t handled gracefully. If possible, switch to a link function to one that constrains the response (e.g. logit link for binomial or log link for Gamma).\r\notherwise this message often occurs when there is something else wrong with the model or data, e.g.  - a model fitted to underdispersed data includes both a negative binomial response and observation-level random effects - negative response values for a link function that doesn’t allow them\r\n\r\nDowndated VtV is not positive definite: no specific advice, see general suggestions above\r\nconvergence code 3 from bobyqa: bobyqa -- a trust region step failed to reduce q: again no specific advice about fixing this, although there is a useful discussion of the meaning of the error message on CrossValidated\r\nREML for GLMMs\r\nWhile restricted maximum likelihood (REML) procedures (Wikipedia are well established for linear mixed models, it is less clear how one should define and compute the equivalent criteria (integrating out the effects of fixed parameters) for GLMMs. @millar_maximum_2011 and @berger_integrated_1999 are possible starting points in the peer-reviewed literature, and there are mailing-list discussions of these issues here and here.\r\nAttempting to use REML=TRUE with glmer will produce the warning extra argument(s) ‘REML’ disregarded\r\nglmmTMB allows REML=TRUE for GLMMs (it uses the Laplace approximation to integrate over the fixed effect parameters), since version 0.2.2\r\nModel diagnostics\r\nInference and confidence intervals\r\nTesting hypotheses\r\nWhat are the p-values listed by summary(glmerfit) etc.? Are they reliable?\r\nBy default, in keeping with the tradition in analysis of generalized linear models, lme4 and similar packages display the Wald Z-statistics for each parameter in the model summary. These have one big advantage: they’re convenient to compute. However, they are asymptotic approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal (or equivalently that the log-likelihood surface is quadratic) and that (2) the sampling distribution of the log-likelihood is (proportional to) \\(\\chi^2\\). The second approximation is discussed further under “Degrees of freedom”. The first assumption usually requires an even greater leap of faith, and is known to cause problems in some contexts (for binomial models failures of this assumption are called the Hauck-Donner effect), especially with extreme-valued parameters.\r\nMethods for testing single parameters\r\nFrom worst to best:\r\nWald \\(Z\\)-tests\r\nFor balanced, nested LMMs where degrees of freedom can be computed according to classical rules: Wald \\(t\\)-tests\r\nLikelihood ratio test, either by setting up the model so that the parameter can be isolated/dropped (via anova or drop1, or via computing likelihood profiles\r\nMarkov chain Monte Carlo (MCMC) or parametric bootstrap confidence intervals\r\nTests of effects (i.e. testing that several parameters are simultaneously zero)\r\nFrom worst to best:\r\nWald chi-square tests (e.g. car::Anova)\r\nLikelihood ratio test (via anova or drop1)\r\nFor balanced, nested LMMs where df can be computed: conditional F-tests\r\nFor LMMs: conditional F-tests with df correction (e.g. Kenward-Roger in pbkrtest package: see notes on K-R etc below.\r\nMCMC or parametric, or nonparametric, bootstrap comparisons (nonparametric bootstrapping must be implemented carefully to account for grouping factors)\r\nIs the likelihood ratio test reliable for mixed models?\r\nIt depends.\r\nNot for fixed effects in finite-size cases (see @pinheiro_mixed-effects_2000): may depend on ‘denominator degrees of freedom’ (number of groups) and/or total number of samples - total number of parameters\r\nConditional F-tests are preferred for LMMs, if denominator degrees of freedom are known\r\n\r\nWhy doesn’t lme4 display denominator degrees of freedom/p values? What other options do I have?\r\nThere is an R FAQ entry on this topic, which links to a mailing list post by Doug Bates (there is also a voluminous mailing list thread reproduced on the R wiki). The bottom line is\r\nFor special cases that correspond to classical experimental designs (i.e. balanced designs that are nested, split-plot, randomized block, etc.) … we can show that the null distributions of particular ratios of sums of squares follow an \\(F\\) distribution with known numerator and denominator degrees of freedom (and hence the sampling distributions of particular contrasts are t-distributed with known df). In more complicated situations (unbalanced, GLMMs, crossed random effects, models with temporal or spatial correlation, etc.) it is not in general clear that the null distribution of the computed ratio of sums of squares is really an F distribution, for any choice of denominator degrees of freedom.\r\nFor each simple degrees-of-freedom recipe that has been suggested (trace of the hat matrix, etc.) there seems to be at least one fairly simple counterexample where the recipe fails badly (e.g. see this r-help thread from September 2006).\r\nWhen the responses are normally distributed and the design is balanced, nested etc. (i.e. the classical LMM situation), the scaled deviances and differences in deviances are exactly \\(F\\)-distributed and looking at the experimental design (i.e., which treatments vary/are replicated at which levels) tells us what the relevant degrees of freedom are (see “df alternatives” below)\r\nTwo approaches to approximating df (Satterthwaite and Kenward-Roger) have been implemented in R, Satterthwaite in lmerTest and Kenward-Roger in pbkrtest (as KRmodcomp) (various packages such as lmerTest, emmeans, car, etc., import pbkrtest::get_Lb_ddf).\r\nK-R is probably the most reliable option [@schaalje_adequacy_2002], although it may be prohibitively computationally expensive for large data sets.\r\nK-R was derived for LMMs (and for REML?) in particular, it isn’t clear how it would apply to GLMMs. @stroup_rethinking_2014 states (referencing @stroup_non-normal_2013) that K-R actually works reasonably well for GLMMs (K-R is not implemented in R for GLMMs; Stroup suggests that a pseudo-likelihood [@wolfinger_generalized_1993] approach is necessary in order to implement K-R for GLMMs):\r\n\r\nNotice the non-integer values of the denominator df. They, and the \\(F\\) and \\(p\\) values, reflect the procedure developed by Kenward and Roger (2009) to account for the effect of the covariance structure on degrees of freedom and standard errors. Although the Kenward–Roger adjustment was derived for the LMM with normally distributed data and is an ad hoc procedure for GLMMs with non-normal data, informal simulation studies consistently have suggested that the adjustment is accurate. The Kenward-Roger adjustment requires that the SAS GLIMMIX default computing algorithm, pseudo-likelihood, be used rather than the Laplace algorithm used to obtain AICC statistics. Stroup (2013b) found that for binomial and Poisson GLMMs, pseudo-likelihood with the Kenward–Roger adjustment yields better Type I error control than Laplace while preserving the GLMM’s advantage with respect to power and accuracy in estimating treatment means.\r\n\r\n\r\nThere are several different issues at play in finite-size (small-sample) adjustments, which apply slightly differently to LMMs and GLMMs.\r\nWhen the data don’t fit into the classical framework (crossed, unbalanced, R-side effects), we might still guess that the deviances etc. are approximately F-distributed but that we don’t know the real degrees of freedom – this is what the Satterthwaite, Kenward-Roger, Fai-Cornelius, etc. approximations are supposed to do.\r\nWhen the responses are not normally distributed (as in GLMs and GLMMs), and when the scale parameter is not estimated (as in standard Poisson- and binomial-response models), then the deviance differences are only asymptotically F- or chi-square-distributed (i.e. not for our real, finite-size samples). In standard GLM practice, we usually ignore this problem; there is some literature on finite-size corrections for GLMs under the rubrics of “Bartlett corrections” and “higher order asymptotics” (see @McCullaghNelder1989, @cordeiro_improved_1994, @cordeiro_note_1998 and the cond package (on CRAN) [which works with GLMs, not GLMMs]), but it’s rarely used. (The bias correction/Firth approach implemented in the brglm package attempts to address the problem of finite-size bias, not finite-size non-chi-squaredness of the deviance differences.)\r\nWhen the scale parameter in a GLM is estimated rather than fixed (as in Gamma or quasi-likelihood models), it is sometimes recommended to use an \\(F\\) test to account for the uncertainty of the scale parameter (e.g. @venables_modern_2002 recommend anova(...,test=\"F\") for quasi-likelihood models)\r\nCombining these issues, one has to look pretty hard for information on small-sample or finite-size corrections for GLMMs: @feng_small_2004 and @bell_small_2010 look like good starting points, but it’s not at all trivial.\r\n\r\nDf alternatives:\r\nuse MASS::glmmPQL (uses old nlme rules approximately equivalent to SAS ‘inner-outer’/‘within-between’ rules) for GLMMs, or (n)lme for LMMs\r\nGuess the denominator df from standard rules (for standard designs, e.g. see @GotelliEllison2004) and apply them to \\(t\\) or \\(F\\) tests\r\nRun the model in lme (if possible) and use the denominator df reported there (which follow a simple ‘inner-outer’ rule which should correspond to the canonical answer for simple/orthogonal designs), applied to \\(t\\) or \\(F\\) tests. For the explicit specification of the rules that lme uses, see page 91 of Pinheiro and Bates (this page was previously available on Google Books, but the link is no longer useful, so here are the relevant paragraphs):\r\n\r\nThese conditional tests for fixed-effects terms require denominator degrees of freedom. In the case of the conditional \\(F\\)-tests, the numerator degrees of freedom are also required, being determined by the term itself. The denominator degrees of freedom are determined by the grouping level at which the term is estimated. A term is called inner relative to a factor if its value can change within a given level of the grouping factor. A term is outer to a grouping factor if its value does not changes within levels of the grouping factor. A term is said to be estimated at level \\(i\\), if it is inner to the \\(i-1\\)st grouping factor and outer to the \\(i\\)th grouping factor. For example, the term Machine in the fm2Machine model is outer to Machine %in% Worker and inner to Worker, so it is estimated at level 2 (Machine %in% Worker). If a term is inner to all \\(Q\\) grouping factors in a model, it is estimated at the level of the within-group errors, which we denote as the \\(Q+1\\)st level.\r\nThe intercept, which is the parameter corresponding to the column of all 1’s in the model matrices \\(X_i\\), is treated differently from all the other parameters, when it is present. As a parameter it is regarded as being estimated at level 0 because it is outer to all the grouping factors. However, its denominator degrees of freedom are calculated as if it were estimated at level \\(Q+1\\). This is because the intercept is the one parameter that pools information from all the observations at a level even when the corresponding column in \\(X_i\\) doesn’t change with the level.\r\nLetting \\(m_i\\) denote the total number of groups in level \\(i\\) (with the convention that \\(m_0=1\\) when the fixed effects model includes an intercept and 0 otherwise, and \\(m_{Q+1}=N\\)) and \\(p_i\\) denote the sum of the degrees of freedom corresponding to the terms estimated at level \\(i\\), the \\(i\\)th level denominator degrees of freedom is defined as\r\n\\[ \\mathrm{denDF}_i = m_i - (m_{i-1} + p_i), i = 1, \\dots, Q \\]\r\nThis definition coincides with the classical decomposition of degrees of freedom in balanced, multilevel ANOVA designs and gives a reasonable approximation for more general mixed-effects models.\r\n\r\nNote that the implementation used in lme gets the wrong answer for random-slopes models:\r\n\r\n\r\nlibrary(nlme)\r\nlmeDF <- function(formula=distance~age,random=~1|Subject) {\r\n     mod <- lme(formula,random,data=Orthodont)\r\n     aa <- anova(mod)\r\n    return(setNames(aa[,\"denDF\"],rownames(aa)))\r\n}\r\nlmeDF()\r\n\r\n\r\n(Intercept)         age \r\n         80          80 \r\n\r\nlmeDF(random=~age|Subject) ## wrong!\r\n\r\n\r\n(Intercept)         age \r\n         80          80 \r\n\r\nI (BB) have re-implemented this algorithm in a way that does slightly better for random-slopes models (but may still get confused!), see here.\r\n\r\n\r\nhome = ifelse(Sys.info()[\"sysname\"] == \"Windows\",\r\n              Sys.getenv(\"USERPROFILE\"),\r\n              Sys.getenv(\"HOME\"))\r\nhome = home %>% gsub(\"\\\\\\\\\", \"/\", .)\r\n\r\ndata_dir = file.path(\r\n  home,\r\n  \"Google Drive (basil.okola@student.uhasselt.be)\",\r\n  \"MSc. Stats Hasselt\",\r\n  \"y1 sem2\",\r\n  \"Multivariate and hierarchical data\",\r\n  \"sample size calculation\"\r\n)\r\nsite_dir = file.path(home, \"Distill websites\", \"_posts\")\r\nsite_dir2 = file.path(home, \"Distill websites\")\r\nsource(file.path(site_dir,\"2021-04-28-glmm-faq\", \"calcDenDF.R\"))\r\ncalcDenDF(~age,\"Subject\",nlme::Orthodont)\r\n\r\n\r\n(Intercept)         age \r\n         80          80 \r\n\r\ncalcDenDF(~age,data=nlme::Orthodont,random=~1|Subject)\r\n\r\n\r\n(Intercept)         age \r\n         80          80 \r\n\r\ncalcDenDF(~age,data=nlme::Orthodont,random=~age|Subject) ## off by 1\r\n\r\n\r\n(Intercept)         age \r\n         81          25 \r\n\r\nuse SAS, Genstat (AS-REML), Stata?\r\nAssume infinite denominator df (i.e. \\(Z\\)/\\(\\chi^2\\) test rather than \\(t\\)/\\(F\\)) if number of groups is large (>45? Various rules of thumb for how large is “approximately infinite” have been posed, including [in @angrist_mostly_2009], 42 (in homage to Douglas Adams)\r\nTesting significance of random effects\r\nthe most common way to do this is to use a likelihood ratio test, i.e. fit the full and reduced models (the reduced model is the model with the focal variance(s) set to zero). For example:\r\n\r\n\r\nlibrary(lme4)\r\nm2 <- lmer(Reaction~Days+(1|Subject)+(0+Days|Subject),sleepstudy,REML=FALSE)\r\nm1 <- update(m2,.~Days+(1|Subject))\r\nm0 <- lm(Reaction~Days,sleepstudy)\r\nanova(m2,m1,m0) ## two sequential tests\r\n\r\n\r\n\r\nData: sleepstudy\r\nModels:\r\nm0: Reaction ~ Days\r\nm1: Reaction ~ Days + (1 | Subject)\r\nm2: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)\r\n   npar    AIC    BIC  logLik deviance   Chisq Df Pr(>Chisq)    \r\nm0    3 1906.3 1915.9 -950.15   1900.3                          \r\nm1    4 1802.1 1814.8 -897.04   1794.1 106.214  1  < 2.2e-16 ***\r\nm2    5 1762.0 1778.0 -876.00   1752.0  42.075  1  8.782e-11 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nWith recent versions of lme4, goodness-of-fit (deviance) can be compared between (g)lmer and (g)lm models, although anova() must be called with the mixed ((g)lmer) model listed first. Keep in mind that LRT-based null hypothesis tests are conservative when the null value (such as \\(\\sigma^2=0\\)) is on the boundary of the feasible space [@self_asymptotic_1987;@stram_variance_1994;@GoldmanWhelan2000]; in the simplest case (single random effect variance), the p-value is approximately twice as large as it should be [@pinheiro_mixed-effects_2000].\r\nConsider not testing the significance of random effects. If the random effect is part of the experimental design, this procedure may be considered ‘sacrificial pseudoreplication’ [@Hurlbert1984]. Using stepwise approaches to eliminate non-significant terms in order to squeeze more significance out of the remaining terms is dangerous in any case.\r\nconsider using the RLRsim package, which has a fast implementation of simulation-based tests of null hypotheses about zero variances, for simple tests. (However, it only applies to lmer models, and is a bit tricky to use for more complex models.)\r\n\r\n\r\n\r\n\r\n\r\nlibrary(RLRsim)\r\n## compare m0 and m1\r\nexactLRT(m1,m0)\r\n\r\n\r\n\r\n    simulated finite sample distribution of LRT. (p-value based\r\n    on 10000 simulated values)\r\n\r\ndata:  \r\nLRT = 106.21, p-value < 2.2e-16\r\n\r\n## compare m1 and m2\r\nmA <- update(m2,REML=TRUE)\r\nm0B <- update(mA, . ~ . - (0 + Days|Subject))\r\nm.slope  <- update(mA, . ~ . - (1|Subject))\r\nexactRLRT(m0=m0B,m=m.slope,mA=mA)\r\n\r\n\r\n\r\n    simulated finite sample distribution of RLRT.\r\n    \r\n    (p-value based on 10000 simulated values)\r\n\r\ndata:  \r\nRLRT = 42.796, p-value < 2.2e-16\r\n\r\nParametric bootstrap: fit the reduced model, then repeatedly simulate from it and compute the differences between the deviance of the reduced and the full model for each simulated data set. Compare this null distribution to the observed deviance difference. This procedure is implemented in the pbkrtest package (messages and warnings suppressed).\r\n\r\n\r\n(pb <- pbkrtest::PBmodcomp(m2,m1,seed=101))\r\n\r\n\r\n\r\nBootstrap test; time: 53.98 sec; samples: 1000; extremes: 0;\r\nRequested samples: 1000 Used samples: 500 Extremes: 0\r\nlarge : Reaction ~ Days + (1 | Subject) + (0 + Days | Subject)\r\nReaction ~ Days + (1 | Subject)\r\n         stat df   p.value    \r\nLRT    42.075  1 8.782e-11 ***\r\nPBtest 42.075     0.001996 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nStandard errors of variance estimates\r\n\r\nParaphrasing Doug Bates: the sampling distribution of variance estimates is in general strongly asymmetric: the standard error may be a poor characterization of the uncertainty.\r\nlme4 allows for computing likelihood profiles of variances and computing confidence intervals on their basis; these likelihood profile confidence intervals are subject to the usual caveats about the LRT with finite sample sizes.\r\nUsing an MCMC-based approach (the simplest/most canned is probably to use the MCMCglmm package, although its mode specifications are not identical to those of lme4) will provide posterior distributions of the variance parameters: quantiles or credible intervals (HPDinterval() in the coda package) will characterize the uncertainty.\r\n(don’t say we didn’t warn you …) [n]lme fits contain an element called apVar which contains the approximate variance-covariance matrix (derived from the Hessian, the matrix of (numerically approximated) second derivatives of the likelihood (REML?) at the maximum (restricted?) likelihood values): you can derive the standard errors from this list element via sqrt(diag(lme.obj$apVar)). For whatever it’s worth, though, these estimates might not match the estimates that SAS gives which are supposedly derived in the same way.\r\nit’s not a full solution, but there is some more information here. I have some delta-method computations there that are off by a factor of 2 for the residual standard deviation, as well as some computations based on reparameterizing the deviance function.\r\nP-values: MCMC and parametric bootstrap\r\nAbandoning the approximate \\(F\\)/\\(t\\)-statistic route, one ends up with the more general problem of estimating \\(p\\)-values. There is a wider range of options here, although many of them are computationally intensive …\r\nMarkov chain Monte Carlo sampling:\r\npseudo-Bayesian: post-hoc sampling, typically (1) assuming flat priors and (2) starting from the MLE, possibly using the approximate variance-covariance estimate to choose a candidate distribution\r\nvia mcmcsamp (if available for your problem: i.e. LMMs with simple random effects – not GLMMs or complex random effects)\r\nvia pvals.fnc in the languageR package, a wrapper for mcmcsamp)\r\nin AD Model Builder, possibly via the glmmADMB package (use the mcmc=TRUE option) or the R2admb package (write your own model definition in AD Model Builder), or outside of R\r\nvia the sim function from the arm package (simulates the posterior only for the beta (fixed-effect) coefficients; not yet working with development lme4; would like a better formal description of the algorithm …?)\r\n\r\nfully Bayesian approaches\r\nvia the MCMCglmm package\r\nglmmBUGS (a WinBUGS wrapper/R interface)\r\nJAGS/WinBUGS/OpenBUGS etc., via the rjags/r2jags/R2WinBUGS/BRugs packages\r\n\r\nStatus of mcmcsamp\r\nmcmcsamp is a function for lme4 that is supposed to sample from the posterior distribution of the parameters, based on flat/improper priors for the parameters [ed: I believe, but am not sure, that these priors are flat on the scale of the theta (Cholesky-factor) parameters]. At present, in the CRAN version (lme4 0.999999-0) and the R-forge “stable” version (lme4.0 0.999999-1), this covers only linear mixed models with uncorrelated random effects.\r\nAs has been discussed in a variety of places (e.g. on r-sig-mixed models, and on the r-forge bug tracker, it is challenging to come up with a sampler that accounts properly for the possibility that the posterior distributions for some of the variance components may be mixtures of point masses at zero and continuous distributions. Naive samplers are likely to get stuck at or near zero. Doug Bates has always been a bit unsure that mcmcsamp is really performing as intended, even in the limited cases it now handles.\r\nGiven this uncertainty about how even the basic version works, the lme4 developers have been reluctant to make the effort to extend it to GLMMs or more complex LMMs, or to implement it for the development version of lme4 … so unless something miraculous happens, it will not be implemented for the new version of lme4. As always, users are encouraged to write and share their own code that implements these capabilities …\r\nParametric bootstrap\r\nThe idea here is that in order to do inference on the effect of (a) predictor(s), you (1) fit the reduced model (without the predictors) to the data; (2) many times, (2a) simulate data from the reduced model; (2b) fit both the reduced and the full model to the simulated (null) data; (2c) compute some statistic(s) [e.g. t-statistic of the focal parameter, or the log-likelihood or deviance difference between the models]; (3) compare the observed values of the statistic from fitting your full model to the data to the null distribution generated in step 2. - PBmodcomp in the pbkrtest package - see the example in help(\"simulate-mer\") in the lme4 package to roll your own, using a combination of simulate() and refit(). - bootMer in lme4 version >1.0.0 - a presentation at UseR! 2009 (abstract, slides) went into detail about a proposed bootMer package and suggested it could work for GLMMs too – but it does not seem to be active.\r\nPredictions and/or confidence (or prediction) intervals on predictions\r\nNote that none of the following approaches takes the uncertainty of the random effects parameters into account … if you want to take RE parameter uncertainty into account, a Bayesian approach is probably the easiest way to do it.\r\nThe general recipe for computing predictions from a linear or generalized linear model is to\r\nfigure out the model matrix \\(X\\) corresponding to the new data;\r\nmatrix-multiply \\(X\\) by the parameter vector \\(\\beta\\) to get the predictions (or linear predictor in the case of GLM(M)s);\r\nextract the variance-covariance matrix of the parameters \\(V\\)\r\ncompute \\(X V X^{\\prime}\\) to get the variance-covariance matrix of the predictions;\r\nextract the diagonal of this matrix to get variances of predictions;\r\nif computing prediction rather than confidence intervals, add the residual variance;\r\ntake the square-root of the variances to get the standard deviations (errors) of the predictions;\r\ncompute confidence intervals based on a Normal approximation;\r\nfor GL(M)Ms, run the confidence interval boundaries (not the standard errors) through the inverse-link function.\r\nlme\r\n\r\n\r\nlibrary(nlme) \r\nfm1 <- lme(distance ~ age*Sex, random = ~ 1 + age | Subject,\r\n           data = Orthodont) \r\nplot(Orthodont,asp=\"fill\") ## plot responses by individual\r\n\r\n\r\n\r\n## note that expand.grid() orders factor levels by *order of\r\n## appearance* -- must match levels(Orthodont$Sex)\r\nnewdat <- expand.grid(age=c(8,10,12,14), Sex=c(\"Female\",\"Male\")) \r\nnewdat$pred <- predict(fm1, newdat, level = 0)\r\n\r\n## [-2] drops response from formula\r\nDesignmat <- model.matrix(formula(fm1)[-2], newdat)\r\npredvar <- diag(Designmat %*% vcov(fm1) %*% t(Designmat)) \r\nnewdat$SE <- sqrt(predvar) \r\nnewdat$SE2 <- sqrt(predvar+fm1$sigma^2)\r\n\r\nlibrary(ggplot2) \r\npd <- position_dodge(width=0.4) \r\ng0 <- ggplot(newdat,aes(x=age,y=pred,colour=Sex))+ \r\n   geom_point(position=pd)\r\ncmult <- 2  ## could use 1.96 instead\r\ng0 + geom_linerange(aes(ymin=pred-cmult*SE,ymax=pred+cmult*SE), position=pd)\r\n\r\n\r\n\r\n## prediction intervals \r\ng0 + geom_linerange(aes(ymin=pred-cmult*SE2,ymax=pred+cmult*SE2), position=pd) \r\n\r\n\r\n\r\n\r\nA similar answer is laid out in the responses to this StackOverflow question.\r\nlme4\r\nCurrent versions of lme4 have a predict method.\r\n\r\n\r\nlibrary(lme4)\r\nlibrary(ggplot2)\r\ndata(\"Orthodont\",package=\"MEMSS\")\r\nfm1 <- lmer(\r\n  formula = distance ~ age*Sex + (age|Subject)\r\n  , data = Orthodont\r\n)\r\nnewdat <- expand.grid(\r\n  age=c(8,10,12,14)\r\n  , Sex=c(\"Female\",\"Male\")\r\n  , distance = 0\r\n)\r\nnewdat$distance <- predict(fm1,newdat,re.form=NA)\r\nmm <- model.matrix(terms(fm1),newdat)\r\n## or newdat$distance <- mm %*% fixef(fm1)\r\npvar1 <- diag(mm %*% tcrossprod(vcov(fm1),mm))\r\ntvar1 <- pvar1+VarCorr(fm1)$Subject[1]  ## must be adapted for more complex models\r\ncmult <- 2 ## could use 1.96\r\nnewdat <- data.frame(\r\n  newdat\r\n  , plo = newdat$distance-cmult*sqrt(pvar1)\r\n  , phi = newdat$distance+cmult*sqrt(pvar1)\r\n  , tlo = newdat$distance-cmult*sqrt(tvar1)\r\n  , thi = newdat$distance+cmult*sqrt(tvar1)\r\n)\r\n#plot confidence\r\ng0 <- ggplot(newdat, aes(x=age, y=distance, colour=Sex))+geom_point()\r\ng0 + geom_pointrange(aes(ymin = plo, ymax = phi))+\r\n    labs(title=\"CI based on fixed-effects uncertainty ONLY\")\r\n\r\n\r\n\r\n#plot prediction\r\ng0 + geom_pointrange(aes(ymin = tlo, ymax = thi))+\r\n    labs(title=\"CI based on FE uncertainty + RE variance\")\r\n\r\n\r\n\r\nrm(\"Orthodont\") ## clean up\r\n\r\n\r\n\r\nglmmTMB\r\n\r\n\r\nlibrary(glmmTMB)\r\ndata(Orthodont,package=\"nlme\")\r\nfm2 <- glmmTMB(distance ~ age*Sex + (age | Subject),\r\n                data = Orthodont,\r\n                family=\"gaussian\")\r\n\r\n## make prediction data frame\r\nnewdat <- expand.grid(age=c(8,10,12,14), Sex=c(\"Female\",\"Male\"))\r\n## design matrix (fixed effects)\r\nmm <- model.matrix(delete.response(terms(fm2)),newdat)\r\n## linear predictor (for GLMMs, back-transform this with the\r\n##  inverse link function (e.g. plogis() for binomial, beta;\r\n##  exp() for Poisson, negative binomial\r\nnewdat$distance <- drop(mm %*% fixef(fm2)[[\"cond\"]])\r\npredvar <- diag(mm %*% vcov(fm2)[[\"cond\"]] %*% t(mm))\r\nnewdat$SE <- sqrt(predvar) \r\nnewdat$SE2 <- sqrt(predvar+sigma(fm2)^2)\r\n\r\n\r\n\r\n(Probably overly complicated) ggplot code:\r\n\r\n\r\nlibrary(ggplot2);  theme_set(theme_bw())\r\npd <- position_dodge(width=0.4)\r\ng0 <- ggplot(Orthodont,aes(x=age,y=distance,colour=Sex))+\r\n    stat_sum(alpha=0.2,aes(size=..n..))+\r\n    scale_size_continuous(breaks=1:4,range=c(2,5))\r\ng1 <- g0+geom_line(data=newdat,position=pd)+\r\n    geom_point(data=newdat,shape=17,size=3,position=pd)\r\n## confidence intervals\r\ng2 <- g1 + geom_linerange(data=newdat,\r\n                          aes(ymin=distance-2*SE,ymax=distance+2*SE),\r\n                          lwd=2, position=pd)\r\n## prediction intervals \r\ng2 + geom_linerange(data=newdat,\r\n                    aes(ymin=distance-2*SE2,ymax=distance+2*SE2), position=pd)\r\n\r\n\r\n\r\n\r\nThe effects, emmeans, and sjPlot packages are also useful here.\r\nConfidence intervals on conditional means/BLUPs/random effects\r\nlme4\r\n(From Harold Doran, updated by Assaf Oron Nov. 2013:)\r\nIf you want the standard errors of the conditional means, you can extract them as follows:\r\n\r\n\r\nlibrary(lme4)\r\nfm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)\r\ncV <- ranef(fm1, condVar = TRUE)   \r\n\r\n\r\n\r\ncV is a list; each element is a data frame containing the conditional modes for a particular grouping factor. If you use scalar random effects (typically random intercepts), then specifying ranef(...,drop=TRUE) will return the conditional modes as a single named vector instead.\r\nThe conditional variances are returned as an attribute of the conditional modes. It may be easiest to use as.data.frame(cV), or broom.mixed::tidy(fm1, effects=\"ran_vals\"), to extract all of the conditional means and standard deviations.\r\nOr, digging in to the data structure by hand: if we set\r\n\r\n\r\nranvar <- attr(cV[[1]], \"postVar\")\r\n\r\n\r\n\r\nthen ranvar is a 3-D array (the attribute is still called postVar, rather than condVar, for historical reasons/backward compatibility). Individual-level covariance matrices of the conditional modes will sit on the [,,i] faces. For example, ranvar[,,1] is the variance-covariance matrix of the conditional distribution for the first group, so\r\n\r\n\r\nsqrt(diag(ranvar[,,1]))\r\n\r\n\r\n[1] 12.070857  2.304839\r\n\r\nwill provide the intercept and slope standard standard deviations for the first group’s conditional modes. If you have a scalar random effect and used drop=TRUE in ranef(), then you will (mercifully) receive only a vector of individual variances here (one for each level of the grouping factor). The following incantation will give a matrix of conditional variances with one row for each group and one column for each parameters:\r\n\r\n\r\nng <- dim(ranvar)[3]\r\nnp <- dim(ranvar)[2]\r\nmm <- matrix(ranvar[cbind(rep(seq(np),ng),\r\n             rep(seq(np),ng),\r\n             rep(ng,each=np))],\r\n       byrow=TRUE,\r\n       nrow=ng)\r\n\r\n\r\n\r\nGetting the uncertainty of the coefficients (i.e., what’s returned by coef(): the sum of the fixed-effect and random-effect predictions for a particular level) is not (alas) currently easy with lme4. If the fixed and random effects were independent then we could simply add the conditional variance and the variance of the fixed-effect predictions, but they aren’t in general. There is a long r-sig-mixed-models mailing list thread that discusses the issues, focusing on (1) how to extract the covariance between fixed-effect estimate and the random-effect prediction; (2) whether this value (the covariance between an estimated parameter and a predicted mode of a conditional distribution of a random variable) even makes sense in a frequentist framework. If you’re willing to assume independence of the conditional variance and the fixed-effect sampling variance, then (e.g.) the variance of the intercepts for each group would be the sum of the fixed-effect intercept variance and the conditional variance of the intercept for each group:\r\n\r\n\r\nvcov(fm1)[1,1]+mm[,1]\r\n\r\n\r\n [1] 192.2807 192.2807 192.2807 192.2807 192.2807 192.2807 192.2807\r\n [8] 192.2807 192.2807 192.2807 192.2807 192.2807 192.2807 192.2807\r\n[15] 192.2807 192.2807 192.2807 192.2807\r\n\r\nPower analysis\r\nPower analysis for (G)LMMs is mostly done by simulation, although there are some closed-form solutions and approximations, e.g. @snijders_standard_1993 (Snijders has links to programs and other resources on his web page). There are resources and bits of code examples spread all over the internet. e.g. here.\r\n@kain_practical_2015 and @johnson_power_2015 are peer-reviewed papers that discuss power analysis via simulation in more detail.\r\n\r\n\r\nlibrary(sos); findFn(\"{power analysis} mixed simulation\")\r\n\r\n\r\n\r\nlocates the fullfact, pamm, simr, and simglm packages. Depending on the goal, one of these packages may have sufficient flexibility to do what you want.\r\nModel selection and averaging\r\nCan I use AIC for mixed models? How do I count the number of degrees of freedom for a random effect?\r\nYes, with caution.\r\nIt depends on the “level of focus” (sensu @spiegelhalter_bayesian_2002) whether (e.g.) a single random-effect variance should be counted as 1 degree of freedom (i.e., the variance parameter or as a value between 1 and N-1 (where N is the number of groups): see @vaida_conditional_2005 and @greven_behaviour_2010. If you are interested in population-level prediction/inference, then the former (called marginal AIC [mAIC]); if individual-level prediction/inference (i.e., using the BLUPs/conditional modes), then the latter (called conditional AIC [cAIC]). Greven and Kneib present results for linear models, giving a version of cAIC that is both computationally efficient and takes uncertainty in the estimation of the variances into account. (Bob O’Hara has a very nice, illustrative blog post on this topic in the context of DIC …)\r\nin cases when testing a variance parameter, AIC may be subject to the same kinds of boundary effects as likelihood ratio test p-values (i.e., AICs may be conservative/overfit slightly when the nested parameter value is on the boundary of the feasible space). @greven_behaviour_2010 explore the problems with mAIC in this context, but do not suggest a solution (they point out that @hughes_model_2003 present a ‘one-sided’ AIC, but not one that deals with the non-independence of data points. I haven’t read Hughes and King, I should go do that).\r\nAIC also inherits the primary problem of likelihood ratio tests in the GLMM context – that is, that LRTs are asymptotic tests. A finite-size correction for AIC does exist (AICc) – but it was developed in the context of linear models. As far as I know its adequacy in the GLMM case has not been established. See e.g. @richards_testing_2005 for caution about AICc in the GLM (not GLMM) case.\r\nlme4 and nlme count parameters for AIC(c) as follows:\r\nthe number of fixed-effect parameters is straightforward (the length of the fixed-effect parameter vector beta, i.e. length(fixef(model)))\r\neach random term in the model with \\(q\\) components counts for \\(q(q+1)/2\\) parameters – for example, a term of the form (slope|group) has 3 parameters (intercept variance, slope variance, correlation between intercept and slope).\r\nmodels that use a scale parameter (e.g. the variance parameter of linear mixed models, or the scale parameter of a Gamma GLMM – standard GLMMs such as binomial and Poisson do not) get an extra parameter counted. Whether to add nuisance parameters or not, such as the residual variance parameter (estimated based on the residual variance, rather than an explicit parameter in the optimization) is as far as I know an open question. In the classic AIC context it doesn’t matter as long as one is consistent. In the AICc context, I don’t think anyone really knows the answer … adding +1 for the residual variance parameter (as lme4 does) would make the model selection process slightly more conservative.\r\n\r\nModel summaries (goodness-of-fit, decomposition of variance, etc.)\r\nHow do I compute a coefficient of determination (\\(R^2\\)), or an analogue, for (G)LMMs?\r\nProblem\r\n(This topic applies to both LMMs and GLMMs, perhaps more so to LMMs, because the issues are even harder for GLMMs.)\r\nResearchers often want to know if there is a simple (or at least implemented-in-R) way to get an analogue of \\(R^2\\) or another simple goodness-of-fit metric for LMMs or GLMMs. This is a challenging question in both the GLM and LMM worlds (and therefore doubly so for GLMMs), because it turns out that the wonderful simplicity of \\(R^2\\) breaks down in the extension to GLMs or LMMs. If you’re trying to quantify “fraction of variance explained” in the GLM context, should you include or exclude sampling variation (e.g., Poisson variation around the expected mean)? [According to an sos::findFn search for “Nagelkerke”, one of the common solutions to this problem, the LogRegR2 function in the descr package computes several different “pseudo-\\(R^2\\)” measures for logistic regression.] If you’re trying to quantify it in the LMM context, should you include or exclude variation of different random-effects terms?\r\nThe same questions apply more generally to decomposition of variance (i.e. trying to assess the contribution of various model components to the overall fit, not just trying to assess the overall goodness-of-fit of the model); there is unlikely to be a single recipe that does everything you want.\r\nThis has been discussed at various times on the mailing lists. This thread and this thread on the r-sig-mixed-models mailing list are good starting points, and this post is useful too.\r\nIn one of those threads, Doug Bates said:\r\n\r\nAssuming that one wants to define an R^2 measure, I think an argument could be made for treating the penalized residual sum of squares from a linear mixed model in the same way that we consider the residual sum of squares from a linear model. Or one could use just the residual sum of squares without the penalty or the minimum residual sum of squares obtainable from a given set of terms, which corresponds to an infinite precision matrix. I don’t know, really. It depends on what you are trying to characterize.\r\n\r\nSimple/crude solutions\r\nIn one of those threads, Jarrett Byrnes contributed the following code:\r\n\r\n\r\nr2.corr.mer <- function(m) {\r\n   lmfit <-  lm(model.response(model.frame(m)) ~ fitted(m))\r\n   summary(lmfit)$r.squared\r\n}\r\n\r\n\r\n\r\n\\(\\Omega^2_0\\) [@xu_measuring_2003], which is almost the same, is based on comparing the residual variance of the full model against the residual variance of a (fixed) intercept-only null model:\r\n\r\n\r\n1-var(residuals(m))/var(model.response(model.frame(m)))\r\n\r\n\r\n\r\nAnother possibility is the squared correlation between the response variable and the predicted values:\r\n\r\n\r\ncor(model.response(model.frame(m)),predict(m,type=\"response\"))^2\r\n\r\n\r\n\r\nSophisticated solutions\r\n@gelman_bayesian_2006 propose/discuss Bayesian measures of \\(R^2\\) (I don’t know if anyone has created a canned implementation of these measures in R). @nakagawa_general_2013 and @johnson_extension_2014 have also proposed a general methodology for computing \\(R^2\\); J. Lefcheck gives examples here and here, based on his implementation in the piecewiseSEM package (CRAN, Github). See also @jaeger_r2_2017, @rights_quantifying_2018 …\r\nA related question is how to quantify “repeatability” (i.e., ratios of variance at different levels) in GLMMs, especially how to compute the “residual error” term for GLMMs: see @nakagawa_repeatability_2010 and the rptR package.\r\nThe bottom line is that there are some simple recipes (and some more complex recipes that may or may not have been coded up by someone), but that ’‘’you have to think carefully about what information you want to get out of the coefficient of determination’’’, because no recipe will have all of the properties of \\(R^2\\) in the simple linear model case.\r\nPackages/functions: See performance::r2(), MuMIn::r.squaredGLMM(), the r2glmm package, the standalone r2MLM function, stuff in the piecewiseSEM package, psycho::R2_nakagawa, partR2 package … (try e.g. sos::findFn(\"Nakagawa Schielzeth\") for an up-to-date list …)\r\nVariable importance\r\nThe simplest way to get (within-study) measures of variable importance is to standardize the predictor variables (scaling by 1 SD or 2SD: @gelman_scaling_2008, @schielzeth_simple_2010)\r\nThe r2glmm package computes partial \\(R^2\\) values for fixed effects (only for lmer, lme, and glmmPQL models)\r\nHenrik Singmann has a detailed answer here on why standardized measures such as partial eta-squared are problematic:\r\n\r\nThe fact that calculating a global measure of model fit (such as R2) is already riddled with complications and that no simple single number can be found, should be a hint that doing so for a subset of the model parameters (i.e., main-effects or interactions) is even more difficult. Given this, I would not recommend to try finding a measure of standardized effect sizes for mixed models.\r\n\r\nHe even gives suggested wording for responding to reviewers who want standardized measures!\r\nDo I have to specify the levels of fixed effects in lmer?\r\nNo. See Doug Bates reply to this question here\r\nMiscellaneous/procedural\r\nPronunciation of lmer/glmer/etc.\r\nlmer: I have heard “ell emm ee arr” (i.e. pronouncing each letter); “elmer” (probably most common); and “lemur”\r\nglmer: “gee ell emm ee arr”, “gee elmer”, “glimmer”, or “gleamer”\r\nfor lme and nlme people just seem to spell out the names (rather than saying e.g. “lemmy” and “nelmy”)\r\nStoring information\r\nRecent versions of lme4 output contain an @optinfo slot that stores warnings.\r\nCopied from https://stat.ethz.ch/pipermail/r-help/2012-February/302767.html :\r\n\r\nThere’s a somewhat hack-ish solution, which is to use options(warn=2) to ‘upgrade’ warnings to errors, and then use try() or tryCatch() to catch them.\r\n\r\n\r\nMore fancily, I used code that looked something like this to save warnings as I went along (sorry about the <<- ) in a recent simulation study. You could also check w$message to do different things in the case of different warnings.\r\n\r\n\r\n\r\n## n.b. have to set up a 3D warn array first ...\r\nwithCallingHandlers(tryCatch(fun(n=nvec[j],tau=tauvec[i],...),\r\n                error = function(e) {\r\n                  warn[k,i,j] <<- paste(\"ERROR:\",e$message)\r\n              NA_ans}),\r\n               warning = function(w) {\r\n                  warn[k,i,j] <<- w$message\r\n                  invokeRestart(\"muffleWarning\")\r\n             })\r\n\r\n\r\n\r\nMixed modeling packages\r\nalso see the package comparison on glmm.wikidot.com\r\nWhich R packages (functions) fit GLMMs?\r\nMASS::glmmPQL (penalized quasi-likelihood)\r\nlme4::glmer (Laplace approximation and adaptive Gauss-Hermite quadrature [AGHQ])\r\nMCMCglmm (Markov chain Monte Carlo)\r\nglmmML (AGHQ)\r\nglmmAK (AGHQ?)\r\nglmmADMB (Laplace)\r\nglmm (from Jim Lindsey’s repeated package: AGHQ)\r\ngamlss.mx\r\nASREML-R\r\nsabreR\r\nShould I use aov(), nlme, or lme4, or some other package?\r\naov() (in the stats package in base R: balanced, orthogonal designs only (R analogue of SAS PROC GLM)\r\nnlme (analogue of SAS PROC MIXED/NLMIXED)\r\nallows more complex designs than aov (unbalanced, heteroscedasticity and/or correlation among residual errors)\r\nmore mature than lme4\r\nwell-documented [@pinheiro_mixed-effects_2000]\r\nimplements R-side effects (heteroscedasticity and correlation)\r\nestimates “denominator degrees of freedom” for \\(F\\) statistics, and hence \\(p\\) values, for LMMs (but see above)\r\n\r\nlme4 (also SAS PROC MIXED/NLMIXED):\r\nfastest\r\nbest for crossed designs (although they are possible in lme)\r\nGLMMs\r\ncutting-edge (for better or worse!)\r\nlikelihood profiles\r\nuse lme4 for GLMMs, or if you have big data (thousands to tens of thousands of records)\r\n\r\nThe following is modified from a contribution by Kingsford Jones, found 2010-03-16\r\nlinear and nonlinear mixed models\r\nlme – Linear mixed-effects models using S4 classes\r\nlmm – Linear mixed models\r\nnlme – Linear and Nonlinear Mixed Effects Models\r\nsabreR\r\nregress Linear mixed models\r\nGLMMs\r\nglmmAK – Generalized Linear Mixed Models\r\nMASS – Main Package of Venables and Ripley’s MASS (see function glmmPQL)\r\nMCMCglmm – MCMC Generalised Linear Mixed Models\r\nlme4 (glmer)\r\nglmmML\r\ngamlss.mx\r\nsabreR\r\nAdditive and generalized-additive mixed models\r\namer – Additive mixed models with lme4\r\ngamm4 – Generalized additive mixed models using mgcv and lme4\r\nmgcv (gamm function, via glmmPQL in MASS package)\r\ngamlss.mx\r\nHierarchical GLMs\r\nhglm – hglm is used to fit hierarchical generalized linear models\r\nHGLMMM – Hierarchical Generalized Linear Models\r\ndiagnostic and modeling frameworks\r\ninfluence.ME – Tools for detecting influential data in mixed effects models\r\narm – Data Analysis Using Regression and Multilevel/Hierarchical Models\r\npamm – Power analysis for random effects in mixed models\r\nRLRsim – Exact (Restricted) Likelihood Ratio tests for mixed and additive models\r\nnpde – Normalised prediction distribution errors for nonlinear mixed-effect models\r\nmultilevel – Multilevel Functions (psychology-oriented; within-group agreement, random group resampling, etc.)\r\nlanguageR\r\npbkrtest – parametric bootstrap and Kenward-Roger tests\r\ndata and examples\r\nMEMSS – Data sets from Mixed-effects Models in S\r\nmlmRev – Examples from Multilevel Modelling Software Review\r\nSASmixed – Data sets from “SAS System for Mixed Models”\r\nextensions\r\nlmeSplines – lmeSplines\r\nlmec – Linear Mixed-Effects Models with Censored Responses\r\nkinship – mixed-effects Cox models, sparse matrices, and modeling data from large pedigrees\r\ncoxme – Mixed Effects Cox Models\r\nordinal – Regression Models for Ordinal Data\r\nphmm – Proportional Hazards Mixed-effects Model (PHMM)\r\npedigreemm – Pedigree-based mixed-effects models\r\n(see also MCMCglmm for pedigree-based approaches)\r\nheavy – Estimation in the linear mixed model using heavy-tailed distributions\r\nGLMMarp – Generalized Linear Multilevel Model with AR(p) Errors Package\r\nglmmlasso – penalized GLMM fitting\r\nspatialCovariance – spatial covariance matrix calculations\r\nInterfaces to other systems\r\nglmmBUGS – Generalised Linear Mixed Models and Spatial Models with BUGS\r\nInterfaces to WinBUGS/OpenBUGS/JAGS (roll your own model file):\r\nR2WinBUGS\r\nr2jags\r\nrjags\r\nRBugs\r\nmodeling based on LMMs\r\nnlmeODE – Non-linear mixed-effects modelling in nlme using differential equations\r\nlongRPart – Recursive partitioning of longitudinal data using mixed-effects models\r\nPSM – Non-Linear Mixed-Effects modelling using Stochastic Differential Equations\r\nOff-CRAN mixed modeling packages:\r\nR-forge and Github:\r\nglmmADMB (R-forge, interface to AD Model Builder)\r\nspida, p3d (Georges Monette)\r\nOther open source:\r\nbernor package (logit-normal fitting), by Yun Ju Sung and Charles J. Geyer\r\nglmm (in Jim Lindsey’s repeated package: at Lindsey’s web site\r\nCommercial:\r\nOpenMx – Advanced Structural Equation Modeling\r\nASReml-R (commercial, but 30 days’ free use/free license for academic or developing-country use available). Very good at complex LMMs (fast, flexible covariance structures, etc.), but only offers PQL for GLMMs, and the manual says: > we cannot recommend the use of this technique for general use. It is included in the current version of asreml() for advanced users. It is highly recommended that its use be accompanied by some form of cross-validatory assessment for the specific dataset concerned.\" Resources:\r\nshort R wiki tutorial\r\nreference manual (PDF)\r\nLuis Apiolaza’s asreml-r cookbook\r\nPackage versions used\r\n\r\n\r\nsessionInfo()\r\n\r\n\r\nR version 4.0.2 (2020-06-22)\r\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19042)\r\n\r\nMatrix products: default\r\n\r\nlocale:\r\n[1] LC_COLLATE=English_United States.1252 \r\n[2] LC_CTYPE=English_United States.1252   \r\n[3] LC_MONETARY=English_United States.1252\r\n[4] LC_NUMERIC=C                          \r\n[5] LC_TIME=English_United States.1252    \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods  \r\n[7] base     \r\n\r\nother attached packages:\r\n [1] ggplot2_3.3.3      RLRsim_3.1-6       nlme_3.1-148      \r\n [4] dplyr_1.0.3        broom.mixed_0.2.6  glmmTMB_1.0.2.1   \r\n [7] equatiomatic_0.2.0 lme4_1.1-26        Matrix_1.2-18     \r\n[10] Cairo_1.5-12.2     pander_0.6.3       knitr_1.31        \r\n\r\nloaded via a namespace (and not attached):\r\n [1] Rcpp_1.0.6        mvtnorm_1.1-1     lattice_0.20-41  \r\n [4] tidyr_1.1.2       zoo_1.8-8         assertthat_0.2.1 \r\n [7] digest_0.6.27     utf8_1.1.4        R6_2.5.0         \r\n[10] plyr_1.8.6        backports_1.2.1   evaluate_0.14    \r\n[13] coda_0.19-4       highr_0.8         pillar_1.5.1     \r\n[16] rlang_0.4.10      multcomp_1.4-15   minqa_1.2.4      \r\n[19] rstudioapi_0.13   nloptr_1.2.2.2    jquerylib_0.1.3  \r\n[22] rmarkdown_2.7     labeling_0.4.2    splines_4.0.2    \r\n[25] statmod_1.4.35    stringr_1.4.0     TMB_1.7.18       \r\n[28] munsell_0.5.0     broom_0.7.5       compiler_4.0.2   \r\n[31] xfun_0.20         pkgconfig_2.0.3   mgcv_1.8-31      \r\n[34] htmltools_0.5.1.1 downlit_0.2.1     tidyselect_1.1.0 \r\n[37] tibble_3.1.0      codetools_0.2-16  fansi_0.4.2      \r\n[40] withr_2.4.0       crayon_1.3.4      MASS_7.3-53.1    \r\n[43] grid_4.0.2        gtable_0.3.0      jsonlite_1.7.2   \r\n[46] xtable_1.8-4      lifecycle_0.2.0   DBI_1.1.1        \r\n[49] magrittr_2.0.1    scales_1.1.1      estimability_1.3 \r\n[52] cli_2.2.0         stringi_1.5.3     farver_2.0.3     \r\n[55] reshape2_1.4.4    bslib_0.2.4       ellipsis_0.3.1   \r\n[58] generics_0.1.0    vctrs_0.3.6       boot_1.3-25      \r\n[61] sandwich_3.0-0    distill_1.2       TH.data_1.0-10   \r\n[64] tools_4.0.2       glue_1.4.2        purrr_0.3.4      \r\n[67] emmeans_1.5.3     survival_3.1-12   yaml_2.2.1       \r\n[70] colorspace_2.0-0  sass_0.3.1       \r\n\r\nTo do\r\nadd links to merDeriv for standard devs of variances, robust estimates. More on Rizopoulos package\r\nupdate package descriptions; cross-link with Task View ? rethinking, brms, …\r\nmore on post-analysis (broom(.mixed), emmeans, multcomp, …)\r\nmore on confidence intervals, simulating from conditional distributions, etc.)\r\nCredited to:\r\nThis is original work by Ben Bolker and colleagues, kindly credit them if you find this useful. Original post at https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\nin R, foo::bar (or foo::bar()) denotes “function bar in package foo”).↩︎\r\nthe dispersion factor is estimated on a variance, so we need to take the square root to apply it to the standard error↩︎\r\n",
    "preview": "posts/2021-04-28-glmm-faq/glmm-faq_files/figure-html5/lmepred-1.png",
    "last_modified": "2021-04-28T16:14:32+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-23-parallel-programming/",
    "title": "Parallel programming",
    "description": "Running large tasks in parallel in R: Power and Sample size Simulation with parallel package.",
    "author": [
      {
        "name": "Basil Okola",
        "url": "https://github.com/Bokola"
      }
    ],
    "date": "2021-04-23",
    "categories": [
      "parallel programming",
      "simulation",
      "sample size",
      "power"
    ],
    "contents": "\r\n\r\nSometimes you get to run repetitive tasks that consume a lot of time and computing memory. A good workaround is distributing these tasks across processors, what is universally referred to as core. parallel package comes in handy in such situations. The only thing you need to do is try bundle your procedures into a function so that you can easily apply them within the parallel library.\r\n\r\n\r\nlibrary(parallel)\r\n\r\n\r\n\r\nParallel programming is achieved by the parallel::mclapply() function. This function does not however work if you are running on windows like I am. For that, you’d have to use parallel::parLapply(). First you register available processors. If using windows:\r\n\r\n\r\ncl = makeCluster(detectCores())\r\n\r\n\r\n\r\nIf in Linux/Damian systems:\r\n\r\n\r\nnumcores = detectCores()\r\n\r\n\r\n\r\nI used parallel programming in a sample-size simulation that saved considerable amount of time, running 1000 simulations at each sample size.\r\n\r\n\r\nset.seed(0123)\r\nsample_power = function(n_sample = seq(50, 450, 25),\r\n                        n_simulations = 1:1000,\r\n                        alpha = 0.05 / 14,\r\n                        effect = 26.57964) {\r\n  set.seed(0123)\r\n  n = c()\r\n  pval = c()\r\n  z = c() # z statistic\r\n  powr = c()\r\n  for (j in seq_along(n_sample)) {\r\n    for (i in seq_along(n_simulations)) {\r\n      treatment = rpois(n_sample[j], lambda = effect + 1.947)\r\n      # Mean of the roses in water (mean from pilot study M = 12.53)\r\n      control = rpois(n_sample[j], lambda = effect)\r\n      sim_data = data.frame(\r\n        response = c(treatment, control),\r\n        treatment = rep(c(0, 1), each = n_sample[j])\r\n        # ,\r\n        # species = rep(c(0, 1), each = n_sample[j])\r\n      )\r\n      z[i] = summary(glm(\r\n        response ~ treatment,\r\n        #+ species,\r\n        data = sim_data,\r\n        family = poisson(link = 'log')\r\n      ))$coeff[\"treatment\", \"z value\"]\r\n      pval[i] =  summary(glm(\r\n        response ~ treatment,\r\n        #+ species,\r\n        data = sim_data,\r\n        family = poisson(link = 'log')\r\n      ))$coeff[\"treatment\", \"Pr(>|z|)\"] / 2\r\n      \r\n    }\r\n    n = c(n, n_sample[j])\r\n    powr = c(powr, sum(pval < alpha) / length(n_simulations))\r\n    out = data.frame(n, powr)\r\n    \r\n  }\r\n  return(out)\r\n}\r\n\r\n\r\n\r\nRunning the function in a normal way using lapply():\r\n\r\n\r\n# normal run\r\nsystem.time({\r\nhh = lapply(seq(50, 500, 50),sample_power)\r\n})\r\n\r\n\r\n   user  system elapsed \r\n 140.65    0.80  143.33 \r\n\r\nUsing parallel:parLapply(). Always remember to stop clusters after your task completes.\r\n\r\n\r\nsystem.time({\r\n  out = parLapply(cl, seq(50, 500, 50),sample_power)\r\n})\r\n\r\n\r\n   user  system elapsed \r\n   0.02    0.02   39.47 \r\n\r\n# remember to stop cluster\r\nstopCluster(cl)\r\n\r\n\r\n\r\nThanks for reading!\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-23T01:34:20+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-19-sample-size-calculation-for-glmm-model-with-a-log-link/",
    "title": "sample size calculation for GLMM model with a log link",
    "description": "A simple guide for sample size and power calculation, with a poisson distribution as a case study.",
    "author": [
      {
        "name": "Basil Okola",
        "url": "https://github.com/Bokola"
      }
    ],
    "date": "2021-04-19",
    "categories": [
      "sample size",
      "power",
      "GLMM",
      "poisson",
      "log link"
    ],
    "contents": "\r\n\r\nWe were recently tasked with determining sample size and power for a project in one of the courses in the Master of Statistics program. Sample size and power calculations have been well documented, and involves following 5 steps:\r\nSpecify a parameter, hypothesis and test\r\nSpecify significance level\r\nSpecify effect size\r\nObtain values or estimates of other parameters needed\r\nSpecify a target value for power.\r\nThe project involved determining from 14 compounds, those that guaranteed a longer vase life for a rose flower when compared to water. We chose to model count of days and adjusted for species of the flowers, including some random effects as well.\r\nWe set \\(\\alpha=0.05\\), effect size of 1, power set at \\(1 - \\beta = 0.85\\) with a right tailed alternative hypothesis. We had to correct for multiple hypothesis test using bonferroni correction. A pilot study of flowers preserved with water was used to estimate mean vase life used for simulating sample size.\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe sampled from a poisson distribution (\\(Y \\sim poisson(\\lambda)\\)) to simulate sample size and power but did not however include random intercepts in the sample size calculation, which overestimated our overall variability.\r\n\r\n\r\n# flower_1 = ests$mean[1]\r\n# flower_2 = ests$mean[2]\r\nn_sample = seq(50, 600, 50)\r\nn = c()\r\npval = c()\r\npowr = c()\r\nalpha = .05 / 14\r\nn_simulations = 1:1000\r\nset.seed(0123)\r\nfor (j in seq_along(n_sample)) {\r\n  for (i in seq_along(n_simulations)) {\r\n    treatment = rpois(n_sample[j], lambda = overall$mean + 1)\r\n    control = rpois(n_sample[j], lambda = overall$mean)\r\n    sim_data = data.frame(\r\n      response = c(treatment, control),\r\n      treatment = rep(c(0, 1), each = n_sample[j]),\r\n      species = rep(c(0, 1), each = n_sample[j])\r\n    )\r\n    pval[i] = summary(glm(\r\n      response ~ treatment + species,\r\n      data = sim_data,\r\n      family = poisson(link = 'log')\r\n    ))$coeff[\"treatment\", \"Pr(>|z|)\"]\r\n    \r\n  }\r\n  n = c(n, n_sample[j])\r\n  powr = c(powr, sum(pval < alpha) / length(n_simulations))\r\n  out = data.frame(n, powr)\r\n}\r\n\r\n\r\nknitr::kable(out, caption = \"Power as simulated for different sample sizes\")\r\n\r\n\r\nTable 1: Power as simulated for different sample sizes\r\nn\r\npowr\r\n50\r\n0.067\r\n100\r\n0.156\r\n150\r\n0.272\r\n200\r\n0.457\r\n250\r\n0.551\r\n300\r\n0.671\r\n350\r\n0.774\r\n400\r\n0.823\r\n450\r\n0.903\r\n500\r\n0.924\r\n550\r\n0.942\r\n600\r\n0.974\r\n\r\nA visualization of the simulated sample sizes and power is presented:\r\n\r\n\r\nggplot(out,aes(x = n, y = powr)) +\r\n  geom_line(color = 'red', size = 1.5) +\r\n  geom_hline(aes(yintercept = .85), linetype = 'dashed') +\r\n  theme_minimal() +\r\n  # cowplot::theme_minimal_hgrid(rel_small = 1) +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  labs(x = \"Sample size\", y = 'Power')\r\n\r\n\r\n\r\n\r\nAdditional links on sample size determination:\r\nhttps://nickch-k.github.io/EconometricsSlides/Week_08/Power_Simulations.html https://cran.r-project.org/web/packages/paramtest/vignettes/Simulating-Power.html\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-19-sample-size-calculation-for-glmm-model-with-a-log-link/figures/unnamed-chunk-5-1.png",
    "last_modified": "2021-04-19T21:52:41+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-12-04-data-munging-withdatatable/",
    "title": "data munging with data.table",
    "description": "A quick glimpse into data.table for data manipulation.",
    "author": [
      {
        "name": "Basil Okola",
        "url": "https://github.com/Bokola"
      }
    ],
    "date": "2020-12-04",
    "categories": [
      "Data manipulation"
    ],
    "contents": "\r\n\r\nData table syntax is of the form DT[i, j, by]\r\ni: on which row\r\nj: what to do\r\nby: group by what\r\nData Manipulation\r\nJust looking\r\nRemoving information\r\nAdding information\r\nReducing information\r\nCombining information\r\nJust looking\r\n\r\n\r\nirisDT = as.data.table(iris)\r\n#tables()# show loaded tables\r\n\r\n\r\n\r\nSorting / ordering rows\r\nsetorder(data.table, …)\r\n-: to sort a variable in descending order\r\n\r\n\r\nsetorder(irisDT, Sepal.Length, Sepal.Width)\r\nirisDT[1:5]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n1:          4.3         3.0          1.1         0.1  setosa\r\n2:          4.4         2.9          1.4         0.2  setosa\r\n3:          4.4         3.0          1.3         0.2  setosa\r\n4:          4.4         3.2          1.3         0.2  setosa\r\n5:          4.5         2.3          1.3         0.3  setosa\r\n\r\n\r\n\r\nsetorder(irisDT, -Species, Sepal.Width)\r\nirisDT[1:5]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          6.0         2.2          5.0         1.5 virginica\r\n2:          4.9         2.5          4.5         1.7 virginica\r\n3:          5.7         2.5          5.0         2.0 virginica\r\n4:          6.3         2.5          5.0         1.9 virginica\r\n5:          6.7         2.5          5.8         1.8 virginica\r\n\r\nRemoving information\r\nSelecting rows\r\nDT[i, j, by]\r\ni: on which rows?\r\n\r\n\r\nirisDT[3:4]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          5.7         2.5            5         2.0 virginica\r\n2:          6.3         2.5            5         1.9 virginica\r\n\r\n\r\n\r\n# irisDT[3:4,] #row 3 and 4\r\n# irisDT[-(1:5)] #delete rows 1:5\r\n# irisDT[!(1:5)] # using false to delete as well\r\n# irisDT[.N] # last row\r\n# irisDT[1:(.N-10)] # all but the last 10 rows\r\n# irisDT[Species == \"virginica\"] # based on conditions fulfilled in columns\r\nirisDT[Species %like% \"^v\"][1:5]# using %like% helper function\r\nirisDT[Petal.Width %between% c(0.3, 0.4)][1:5]# values in an interval\r\n\r\n\r\n\r\nSelecting columns\r\nDT[i, j, by]\r\nj: what to do? -> select columns\r\n\r\n\r\nirisDT[, Species][1:5] # returns a vector\r\nirisDT[, \"Species\"][1:5] # returns a dataframe/data.table\r\nirisDT[, -c(\"Species\")]\r\nirisDT[, !c(\"Species\")]\r\nirisDT[, list(Species, LS = Sepal.Length)] # select and rename\r\n# .() is an alias for list\r\n\r\nirisDT[, .(Species, SL = Sepal.Length)]\r\nirisDT[Species == \"virginica\" & Sepal.Length > 7 & Sepal.Width < 3, !c(\"Species\")]\r\nirisDT[Species == \"virginica\" &\r\nSepal.Length > 7 &Sepal.Width < 3,.(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)]\r\n\r\n\r\n\r\nAdding information\r\nMaking new columns - preserving existing ones\r\nDT[i, j, by]\r\nj: what to do? -> compute new columns preserving existing ones\r\ndata.table uses a new operator := to add/update/delete columns (by reference)\r\noption 1\r\n\r\n\r\nirisDT[, maxLength := max(Sepal.Length, Petal.Length)][,\r\n          minWidth := min(Sepal.Length, Petal.Width)]\r\nirisDT[1:5]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          6.0         2.2          5.0         1.5 virginica\r\n2:          4.9         2.5          4.5         1.7 virginica\r\n3:          5.7         2.5          5.0         2.0 virginica\r\n4:          6.3         2.5          5.0         1.9 virginica\r\n5:          6.7         2.5          5.8         1.8 virginica\r\n   maxLength minWidth\r\n1:       7.9      0.1\r\n2:       7.9      0.1\r\n3:       7.9      0.1\r\n4:       7.9      0.1\r\n5:       7.9      0.1\r\n\r\noption 2\r\nLHS := RHS form\r\n\r\n\r\nirisDT[, c(\"maxLength\", \"minWidth\") :=\r\n         list(\r\n           max(Sepal.Length, Petal.Length),\r\n           min(Sepal.Width, Petal.Width)\r\n         )]\r\n\r\nirisDT[1:5]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          6.0         2.2          5.0         1.5 virginica\r\n2:          4.9         2.5          4.5         1.7 virginica\r\n3:          5.7         2.5          5.0         2.0 virginica\r\n4:          6.3         2.5          5.0         1.9 virginica\r\n5:          6.7         2.5          5.8         1.8 virginica\r\n   maxLength minWidth\r\n1:       7.9      0.1\r\n2:       7.9      0.1\r\n3:       7.9      0.1\r\n4:       7.9      0.1\r\n5:       7.9      0.1\r\n\r\nOption 3\r\nFunctional form\r\n\r\n\r\nirisDT[, `:=` (\r\n  maxLength = max(Sepal.Length, Petal.Length),\r\n  minWidth = min(Sepal.Width, Petal.Width)\r\n)]\r\nirisDT[1:5]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          6.0         2.2          5.0         1.5 virginica\r\n2:          4.9         2.5          4.5         1.7 virginica\r\n3:          5.7         2.5          5.0         2.0 virginica\r\n4:          6.3         2.5          5.0         1.9 virginica\r\n5:          6.7         2.5          5.8         1.8 virginica\r\n   maxLength minWidth\r\n1:       7.9      0.1\r\n2:       7.9      0.1\r\n3:       7.9      0.1\r\n4:       7.9      0.1\r\n5:       7.9      0.1\r\n\r\nMaking new columns - dropping existing ones\r\nDT[i, j, by]\r\nj: what to do? -> compute new columns dropping existing ones\r\n\r\n\r\nirisDT[, .(maxLength = pmax(Sepal.Length, Petal.Length),\r\nminWidth = pmin(Sepal.Width, Petal.Width))][1:5]\r\n\r\n\r\n   maxLength minWidth\r\n1:       6.0      1.5\r\n2:       4.9      1.7\r\n3:       5.7      2.0\r\n4:       6.3      1.9\r\n5:       6.7      1.8\r\n\r\n\r\n\r\nnewIris <- irisDT[, .(Sepal.Area = Sepal.Width * Sepal.Length,\r\nPetal.Area = Petal.Width * Petal.Length)]\r\nnewIris[, Area.Ratio := Petal.Area / Sepal.Area]\r\nnewIris[1:5]\r\n\r\n\r\n   Sepal.Area Petal.Area Area.Ratio\r\n1:      13.20       7.50  0.5681818\r\n2:      12.25       7.65  0.6244898\r\n3:      14.25      10.00  0.7017544\r\n4:      15.75       9.50  0.6031746\r\n5:      16.75      10.44  0.6232836\r\n\r\nReducing information\r\nSummarizing rows\r\n\r\n\r\nirisDT[, .(meanSepalLength = mean(Sepal.Length),\r\n           meanSepalWidth = mean(Sepal.Width),\r\n           meanPetalLength = mean(Petal.Length),\r\n           meanPetalWidth = mean(Petal.Width))]\r\n\r\n\r\n   meanSepalLength meanSepalWidth meanPetalLength meanPetalWidth\r\n1:        5.843333       3.057333           3.758       1.199333\r\n\r\n\r\n\r\nirisDT[, .(nSamples = .N, nSpecies = uniqueN(Species))\r\n       ]\r\n\r\n\r\n   nSamples nSpecies\r\n1:      150        3\r\n\r\nSummarizing rows with filtering\r\n\r\n\r\nirisDT[Species == \"versicolor\",.(\r\n  meanSepalLength = mean(Sepal.Length),\r\n           meanSepalWidth = mean(Sepal.Width),\r\n           meanPetalLength = mean(Petal.Length),\r\n           meanPetalWidth = mean(Petal.Width)\r\n)]\r\n\r\n\r\n   meanSepalLength meanSepalWidth meanPetalLength meanPetalWidth\r\n1:           5.936           2.77            4.26          1.326\r\n\r\nGrouping by one or more variables\r\n\r\n\r\nirisDT[, .(meanSepalLength = mean(Sepal.Length),\r\nmeanSepalWidth = mean(Sepal.Width),\r\nmeanPetalLength = mean(Petal.Length),\r\nmeanPetalWidth = mean(Petal.Width)), by = \"Species\"]\r\n\r\n\r\n      Species meanSepalLength meanSepalWidth meanPetalLength\r\n1:  virginica           6.588          2.974           5.552\r\n2: versicolor           5.936          2.770           4.260\r\n3:     setosa           5.006          3.428           1.462\r\n   meanPetalWidth\r\n1:          2.026\r\n2:          1.326\r\n3:          0.246\r\n\r\n\r\n\r\nirisDT[, .(nSamples = .N, nSpecies = uniqueN(Species)), by = .(Species)]\r\n\r\n\r\n      Species nSamples nSpecies\r\n1:  virginica       50        1\r\n2: versicolor       50        1\r\n3:     setosa       50        1\r\n\r\nCombining information\r\njoining 2 data.tables\r\nInner join: return all rows from x where there are matching values in y and all columns from x and y. In case of multiple matches between x and y, all combinations of the matches are returned.\r\nFull join: return all rows and columns from both x and y. \r\nLeft join: return all rows from x and all columns from x and y. In case of multiple matches between x and y, all combinations of the matches are returned.\r\nRight join: return all rows from y and all columns from x and y. In case of multiple matches between x and y, all combinations of the matches are returned.\r\nDT[i, on]\r\ni: join to which data.table?\r\non: join key columns?\r\n\r\n\r\n(x <- data.table(id = c(1, 2, 4, 5, 6),\r\nx = c(9, 12, 14, 21, 8)))\r\n\r\n\r\n   id  x\r\n1:  1  9\r\n2:  2 12\r\n3:  4 14\r\n4:  5 21\r\n5:  6  8\r\n\r\n(y <- data.table(id = c(1, 3, 4, 6, 6),\r\ny = c(8, 14, 19, 2, 4)))\r\n\r\n\r\n   id  y\r\n1:  1  8\r\n2:  3 14\r\n3:  4 19\r\n4:  6  2\r\n5:  6  4\r\n\r\ninner join - data.table\r\n\r\n\r\ny[x, on = .(id), nomatch = 0]\r\n\r\n\r\n   id  y  x\r\n1:  1  8  9\r\n2:  4 19 14\r\n3:  6  2  8\r\n4:  6  4  8\r\n\r\nfull join - merge\r\n\r\n\r\nmerge.data.table(x = x, y = y, by = \"id\", all = TRUE)\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  2 12 NA\r\n3:  3 NA 14\r\n4:  4 14 19\r\n5:  5 21 NA\r\n6:  6  8  2\r\n7:  6  8  4\r\n\r\nleft join - merge\r\n\r\n\r\nmerge.data.table(x = x, y = y, by = \"id\", all.x = TRUE)\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  2 12 NA\r\n3:  4 14 19\r\n4:  5 21 NA\r\n5:  6  8  2\r\n6:  6  8  4\r\n\r\nleft join - data.table\r\n\r\n\r\ny[x, on = .(id)]\r\n\r\n\r\n   id  y  x\r\n1:  1  8  9\r\n2:  2 NA 12\r\n3:  4 19 14\r\n4:  5 NA 21\r\n5:  6  2  8\r\n6:  6  4  8\r\n\r\nright join - merge\r\n\r\n\r\ndata.table::merge.data.table(x = x, y = y, by = \"id\", all.y = T)\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  3 NA 14\r\n3:  4 14 19\r\n4:  6  8  2\r\n5:  6  8  4\r\n\r\nright jpin - data.table\r\n\r\n\r\nx[y, on = .(id)]\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  3 NA 14\r\n3:  4 14 19\r\n4:  6  8  2\r\n5:  6  8  4\r\n\r\nantijoin - data.table\r\n\r\n\r\nx[!y, on = .(id)]\r\n\r\n\r\n   id  x\r\n1:  2 12\r\n2:  5 21\r\n\r\nKeys\r\nNo need of the on argument when performing a join\r\nSorts the data.table in memory by the key column(s)\r\nMultiple columns can be set and used as keys\r\nUseful functions:\r\nsetkey(DT, …) or setkeyv(DT, keys)\r\nhaskey(DT)\r\nkey(DT\r\n\r\n\r\nsetkey(x, id)\r\nsetkey(y, \"id\")\r\nhaskey(x); haskey(y)\r\n\r\n\r\n[1] TRUE\r\n[1] TRUE\r\n\r\nkey(x); key(y)\r\n\r\n\r\n[1] \"id\"\r\n[1] \"id\"\r\n\r\njoins with keys already specified\r\nInner join x and y\r\n\r\n\r\nx[y, nomatch = 0]\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  4 14 19\r\n3:  6  8  2\r\n4:  6  8  4\r\n\r\nRight join x and y\r\n\r\n\r\nx[y]\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  3 NA 14\r\n3:  4 14 19\r\n4:  6  8  2\r\n5:  6  8  4\r\n\r\nLeft join x and y\r\n\r\n\r\ny[x]\r\n\r\n\r\n   id  y  x\r\n1:  1  8  9\r\n2:  2 NA 12\r\n3:  4 19 14\r\n4:  5 NA 21\r\n5:  6  2  8\r\n6:  6  4  8\r\n\r\nExercise 3\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(data.table)\r\ndata(starwars)\r\nstarwarsDT <- as.data.table(starwars)\r\nstarwarsDT[1]\r\n\r\n\r\n             name height mass hair_color skin_color eye_color\r\n1: Luke Skywalker    172   77      blond       fair      blue\r\n   birth_year  sex    gender homeworld species\r\n1:         19 male masculine  Tatooine   Human\r\n                                                                                         films\r\n1: The Empire Strikes Back,Revenge of the Sith,Return of the Jedi,A New Hope,The Force Awakens\r\n                            vehicles               starships\r\n1: Snowspeeder,Imperial Speeder Bike X-wing,Imperial shuttle\r\n\r\nstarwarsDT[, .(min = min(mass, na.rm = T),\r\n               max = max(mass, na.rm = T),\r\n               nobs = .N), by = homeworld][nobs >1] \r\n\r\n\r\n    homeworld   min   max nobs\r\n 1:  Tatooine  32.0 136.0   10\r\n 2:     Naboo  32.0  85.0   11\r\n 3:  Alderaan  49.0  79.0    3\r\n 4:  Kashyyyk 112.0 136.0    2\r\n 5:  Corellia  77.0  80.0    2\r\n 6:      <NA>  17.0 140.0   10\r\n 7:    Kamino  78.2  88.0    3\r\n 8: Coruscant  50.0  50.0    3\r\n 9:    Ryloth  55.0  55.0    2\r\n10:    Mirial  50.0  56.2    2\r\n\r\nReshaping data with data.table\r\nTidy data\r\nEach variable is a column -Each observation is a row\r\nEach value is a cell\r\n\r\nTwo fundamental verbs\r\nmelt(): ‘Wide’ -> ‘long’ data\r\ndcast(): ‘long’ -> ‘wide’ data\r\n\r\nWide format\r\n\r\n\r\nknitr::kable(head(iris[1:4,], n =3))\r\n\r\n\r\n\r\nSepal.Length\r\n\r\n\r\nSepal.Width\r\n\r\n\r\nPetal.Length\r\n\r\n\r\nPetal.Width\r\n\r\n\r\nSpecies\r\n\r\n\r\n5.1\r\n\r\n\r\n3.5\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.9\r\n\r\n\r\n3.0\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.7\r\n\r\n\r\n3.2\r\n\r\n\r\n1.3\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\nmelt(wide_df, id.vars = 'grouping variable', variable.name = 'measures_across_grouping_var',value.name = 'value_name_as_desired')\r\nLong to wide\r\ndcast(long_df, group ~ atttribute, value.var = \"value\")\r\nSeparating and uniting information\r\n\r\n\r\nirisUnited <- irisDT[ , Sepal.Length.Width := paste0(Sepal.Length, \"/\", Sepal.Width)]\r\nirisUnited[1:5]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          6.0         2.2          5.0         1.5 virginica\r\n2:          4.9         2.5          4.5         1.7 virginica\r\n3:          5.7         2.5          5.0         2.0 virginica\r\n4:          6.3         2.5          5.0         1.9 virginica\r\n5:          6.7         2.5          5.8         1.8 virginica\r\n   maxLength minWidth Sepal.Length.Width\r\n1:       7.9      0.1              6/2.2\r\n2:       7.9      0.1            4.9/2.5\r\n3:       7.9      0.1            5.7/2.5\r\n4:       7.9      0.1            6.3/2.5\r\n5:       7.9      0.1            6.7/2.5\r\n\r\n\r\n\r\nirisSeparated = irisUnited[, c('Sepal.Length', 'Sepal.Width') := tstrsplit(Sepal.Length.Width, \"/\")]\r\nirisSeparated[1:5]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:            6         2.2          5.0         1.5 virginica\r\n2:          4.9         2.5          4.5         1.7 virginica\r\n3:          5.7         2.5          5.0         2.0 virginica\r\n4:          6.3         2.5          5.0         1.9 virginica\r\n5:          6.7         2.5          5.8         1.8 virginica\r\n   maxLength minWidth Sepal.Length.Width\r\n1:       7.9      0.1              6/2.2\r\n2:       7.9      0.1            4.9/2.5\r\n3:       7.9      0.1            5.7/2.5\r\n4:       7.9      0.1            6.3/2.5\r\n5:       7.9      0.1            6.7/2.5\r\n\r\nHands-on session\r\nExercise 1\r\n\r\n\r\ndata(\"relig_income\")\r\na = as.data.table(relig_income)\r\na[1:5]\r\n\r\n\r\n             religion <$10k $10-20k $20-30k $30-40k $40-50k $50-75k\r\n1:           Agnostic    27      34      60      81      76     137\r\n2:            Atheist    12      27      37      52      35      70\r\n3:           Buddhist    27      21      30      34      33      58\r\n4:           Catholic   418     617     732     670     638    1116\r\n5: Don’t know/refused    15      14      15      11      10      35\r\n   $75-100k $100-150k >150k Don't know/refused\r\n1:      122       109    84                 96\r\n2:       73        59    74                 76\r\n3:       62        39    53                 54\r\n4:      949       792   633               1489\r\n5:       21        17    18                116\r\n\r\nb = melt(a, id.vars = \"religion\", variable.name = \"income_category\", value.name = \"n\")\r\n\r\n\r\n\r\nExercise 2\r\n\r\n\r\ndata(\"storms\")\r\naa = as.data.table(storms)\r\naa[, date:= paste0(year, \"-\", month,\"-\", day)]\r\n\r\n\r\n\r\nUseful symbols/ functions\r\n.SD: Subset of Data for each group\r\n.SD along with lapply can be used to apply any function to multiple columns by group\r\n\r\n.SDcols: Columns of the data.table that are included in .SD\r\nna.omit(): returns the object with incomplete cases removed\r\nSummarizing for specific condition\r\n\r\n\r\ncols = sapply(irisDT, is.numeric)\r\ncols = names(cols)[cols]\r\nirisDT[, lapply(.SD, mean), by = Species, .SDcols = cols]\r\n\r\n\r\n      Species Petal.Length Petal.Width maxLength minWidth\r\n1:  virginica        5.552       2.026       7.9      0.1\r\n2: versicolor        4.260       1.326       7.9      0.1\r\n3:     setosa        1.462       0.246       7.9      0.1\r\n\r\nSummarizing for specific variable of interest\r\n\r\n\r\nirisDT[, lapply(.SD, mean), by = Species, .SDcols = c(\"Petal.Length\", \"Petal.Width\")]\r\n\r\n\r\n      Species Petal.Length Petal.Width\r\n1:  virginica        5.552       2.026\r\n2: versicolor        4.260       1.326\r\n3:     setosa        1.462       0.246\r\n\r\nSummarizing all columns\r\n\r\n\r\nirisDT[ , unlist(recursive = FALSE, lapply(.(mean = mean, sd = sd, min = min, max = max), \r\n                                           function(f) lapply(.SD, f))), by = Species][1:5]\r\n\r\n\r\n      Species mean.Sepal.Length mean.Sepal.Width mean.Petal.Length\r\n1:  virginica                NA               NA             5.552\r\n2: versicolor                NA               NA             4.260\r\n3:     setosa                NA               NA             1.462\r\n4:       <NA>                NA               NA                NA\r\n5:       <NA>                NA               NA                NA\r\n   mean.Petal.Width mean.maxLength mean.minWidth\r\n1:            2.026            7.9           0.1\r\n2:            1.326            7.9           0.1\r\n3:            0.246            7.9           0.1\r\n4:               NA             NA            NA\r\n5:               NA             NA            NA\r\n   mean.Sepal.Length.Width sd.Sepal.Length sd.Sepal.Width\r\n1:                      NA       0.6358796      0.3224966\r\n2:                      NA       0.5161711      0.3137983\r\n3:                      NA       0.3524897      0.3790644\r\n4:                      NA              NA             NA\r\n5:                      NA              NA             NA\r\n   sd.Petal.Length sd.Petal.Width sd.maxLength sd.minWidth\r\n1:       0.5518947      0.2746501            0           0\r\n2:       0.4699110      0.1977527            0           0\r\n3:       0.1736640      0.1053856            0           0\r\n4:              NA             NA           NA          NA\r\n5:              NA             NA           NA          NA\r\n   sd.Sepal.Length.Width min.Sepal.Length min.Sepal.Width\r\n1:                    NA              4.9             2.2\r\n2:                    NA              4.9               2\r\n3:                    NA              4.3             2.3\r\n4:                    NA             <NA>            <NA>\r\n5:                    NA             <NA>            <NA>\r\n   min.Petal.Length min.Petal.Width min.maxLength min.minWidth\r\n1:              4.5             1.4           7.9          0.1\r\n2:              3.0             1.0           7.9          0.1\r\n3:              1.0             0.1           7.9          0.1\r\n4:               NA              NA            NA           NA\r\n5:               NA              NA            NA           NA\r\n   min.Sepal.Length.Width max.Sepal.Length max.Sepal.Width\r\n1:                4.9/2.5              7.9             3.8\r\n2:                4.9/2.4                7             3.4\r\n3:                  4.3/3              5.8             4.4\r\n4:                   <NA>             <NA>            <NA>\r\n5:                   <NA>             <NA>            <NA>\r\n   max.Petal.Length max.Petal.Width max.maxLength max.minWidth\r\n1:              6.9             2.5           7.9          0.1\r\n2:              5.1             1.8           7.9          0.1\r\n3:              1.9             0.6           7.9          0.1\r\n4:               NA              NA            NA           NA\r\n5:               NA              NA            NA           NA\r\n   max.Sepal.Length.Width\r\n1:                7.9/3.8\r\n2:                  7/3.2\r\n3:                  5/3.6\r\n4:                   <NA>\r\n5:                   <NA>\r\n\r\nHands-on 3\r\nExercise 1 - Compute the minimum, maximum, median and mean for all numeric columns\r\n\r\n\r\ndata(\"starwars\")\r\nstarwarsDT = as.data.table(starwars)\r\ncols = sapply(starwarsDT, is.numeric)\r\ncols = names(cols)[cols]\r\n\r\nstarwarsDT[, height:= as.numeric(height)]\r\n\r\nstarwarsDT[, unlist(recursive = F,\r\n                    lapply(.(min = min, max = max, median = median, mean = mean), function(f) lapply(.SD, f))), by = .(homeworld, species), .SDcols = cols][1:5]\r\n\r\n\r\n   homeworld species min.height min.mass min.birth_year max.height\r\n1:  Tatooine   Human        163       NA             19        202\r\n2:  Tatooine   Droid         97       32             NA        167\r\n3:     Naboo   Droid         96       32             33         96\r\n4:  Alderaan   Human        150       NA             NA        191\r\n5:   Stewjon   Human        182       77             57        182\r\n   max.mass max.birth_year median.height median.mass\r\n1:       NA             82         180.5          NA\r\n2:       75             NA         132.0        53.5\r\n3:       32             33          96.0        32.0\r\n4:       NA             NA         188.0          NA\r\n5:       77             57         182.0        77.0\r\n   median.birth_year mean.height mean.mass mean.birth_year\r\n1:             44.45    179.2500        NA          47.475\r\n2:                NA    132.0000      53.5              NA\r\n3:             33.00     96.0000      32.0          33.000\r\n4:                NA    176.3333        NA              NA\r\n5:             57.00    182.0000      77.0          57.000\r\n\r\nExercise 2 - all colnames to upper case\r\n\r\n\r\nstarwarsDT = as.data.table(starwars)\r\nsetnames(starwarsDT, toupper(names(starwarsDT)))\r\n\r\n\r\n\r\nConvert all character columns to upper case\r\n\r\n\r\ncols = sapply(starwarsDT, is.character)\r\ncols = names(cols)[cols]\r\nstarwarsDT[, lapply(.SD, toupper)][1:5]\r\n\r\n\r\n             NAME HEIGHT MASS HAIR_COLOR  SKIN_COLOR EYE_COLOR\r\n1: LUKE SKYWALKER    172   77      BLOND        FAIR      BLUE\r\n2:          C-3PO    167   75       <NA>        GOLD    YELLOW\r\n3:          R2-D2     96   32       <NA> WHITE, BLUE       RED\r\n4:    DARTH VADER    202  136       NONE       WHITE    YELLOW\r\n5:    LEIA ORGANA    150   49      BROWN       LIGHT     BROWN\r\n   BIRTH_YEAR    SEX    GENDER HOMEWORLD SPECIES\r\n1:         19   MALE MASCULINE  TATOOINE   HUMAN\r\n2:        112   NONE MASCULINE  TATOOINE   DROID\r\n3:         33   NONE MASCULINE     NABOO   DROID\r\n4:       41.9   MALE MASCULINE  TATOOINE   HUMAN\r\n5:         19 FEMALE  FEMININE  ALDERAAN   HUMAN\r\n                                                                                                                                                        FILMS\r\n1:                                               C(\"THE EMPIRE STRIKES BACK\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\", \"THE FORCE AWAKENS\")\r\n2:                      C(\"THE EMPIRE STRIKES BACK\", \"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\")\r\n3: C(\"THE EMPIRE STRIKES BACK\", \"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\", \"THE FORCE AWAKENS\")\r\n4:                                                                    C(\"THE EMPIRE STRIKES BACK\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\")\r\n5:                                               C(\"THE EMPIRE STRIKES BACK\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\", \"THE FORCE AWAKENS\")\r\n                                    VEHICLES\r\n1: C(\"SNOWSPEEDER\", \"IMPERIAL SPEEDER BIKE\")\r\n2:                              CHARACTER(0)\r\n3:                              CHARACTER(0)\r\n4:                              CHARACTER(0)\r\n5:                     IMPERIAL SPEEDER BIKE\r\n                         STARSHIPS\r\n1: C(\"X-WING\", \"IMPERIAL SHUTTLE\")\r\n2:                    CHARACTER(0)\r\n3:                    CHARACTER(0)\r\n4:                 TIE ADVANCED X1\r\n5:                    CHARACTER(0)\r\n\r\nExercise 3\r\nRemove the data which have NA for the numeric variables\r\nGroup the data by all character variables\r\nCompute the mean for ts_diameter and hu_diameter\r\n\r\n\r\nstormsDT <- as.data.table(storms)\r\nnumericCols = sapply(stormsDT, is.numeric)\r\nnumericCols = names(numericCols)[numericCols]\r\nstormsDTNew <- na.omit(stormsDT, cols = numericCols)\r\ncharacterCols <- sapply(stormsDTNew, is.character)\r\ncharacterCols <- names(characterCols)[characterCols]\r\ncolsOfInterest <- c(\"ts_diameter\", \"hu_diameter\")\r\nstormsDTNew[, lapply(.SD, mean), .SDcols = colsOfInterest, by = characterCols][1:5]\r\n\r\n\r\n     name              status ts_diameter hu_diameter\r\n1:   Alex tropical depression     0.00000      0.0000\r\n2:   Alex      tropical storm   150.80013      0.0000\r\n3:   Alex           hurricane   304.40871     63.5669\r\n4: Bonnie tropical depression     0.00000      0.0000\r\n5: Bonnie      tropical storm    75.40637      0.0000\r\n\r\n.SD: Subset of data - choose row by ordinal position\r\n\r\n\r\n# irisDT[, .SD[1L]]\r\n# irisDT[, .SD[1L], by = Species]\r\nirisDT[, .SD[1L], by = Species, .SDcols = c(\"Petal.Length\", \"Petal.Width\")]\r\n\r\n\r\n      Species Petal.Length Petal.Width\r\n1:  virginica          5.0         1.5\r\n2: versicolor          3.5         1.0\r\n3:     setosa          1.3         0.3\r\n\r\n.SD: Subset of Data - select top entries (in each group)\r\n\r\n\r\nirisDT[order(-Sepal.Length), .SD[1:2]]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          7.9         3.8          6.4         2.0 virginica\r\n2:          7.7         2.6          6.9         2.3 virginica\r\n   maxLength minWidth Sepal.Length.Width\r\n1:       7.9      0.1            7.9/3.8\r\n2:       7.9      0.1            7.7/2.6\r\n\r\nirisDT[, .SD[which.max(Sepal.Length)], by = Species, .SDcols = c(\"Sepal.Length\",\"Sepal.Width\")]\r\n\r\n\r\n      Species Sepal.Length Sepal.Width\r\n1:  virginica          7.9         3.8\r\n2: versicolor            7         3.2\r\n3:     setosa          5.8           4\r\n\r\nMultiple if else statements\r\n\r\n\r\nirisDT[, text := {\r\n  if(Sepal.Length > 4 & Sepal.Width > 4) \"sepal length and width is larger than 4\"\r\n  else if(Petal.Length < 2 & Petal.Width < 2) \"petal length and width is smaller than 2\"\r\n  else \"other\"\r\n}, by = .(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)][, .SD[1], by = text][1:5]\r\n\r\n\r\n                                       text Sepal.Length Sepal.Width\r\n1:                                    other            6         2.2\r\n2: petal length and width is smaller than 2          4.5         2.3\r\n3:  sepal length and width is larger than 4          5.2         4.1\r\n4:                                     <NA>         <NA>        <NA>\r\n5:                                     <NA>         <NA>        <NA>\r\n   Petal.Length Petal.Width   Species maxLength minWidth\r\n1:          5.0         1.5 virginica       7.9      0.1\r\n2:          1.3         0.3    setosa       7.9      0.1\r\n3:          1.5         0.1    setosa       7.9      0.1\r\n4:           NA          NA      <NA>        NA       NA\r\n5:           NA          NA      <NA>        NA       NA\r\n   Sepal.Length.Width\r\n1:              6/2.2\r\n2:            4.5/2.3\r\n3:            5.2/4.1\r\n4:               <NA>\r\n5:               <NA>\r\n\r\n\r\n\r\nbin_iris <- function(sl, sw, pl, pw){\r\n  if(sl > 4 & sw > 4) \"sepal length and width is larger than 4\"\r\n  else if(pl < 2 & pw < 2) \"petal length and width is smaller than 2\"\r\n  else \"other\"\r\n}\r\nirisDT[, text := bin_iris(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width), \r\n       by = .(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)][, .SD[1], by = text][1:5]\r\n\r\n\r\n                                       text Sepal.Length Sepal.Width\r\n1:                                    other            6         2.2\r\n2: petal length and width is smaller than 2          4.5         2.3\r\n3:  sepal length and width is larger than 4          5.2         4.1\r\n4:                                     <NA>         <NA>        <NA>\r\n5:                                     <NA>         <NA>        <NA>\r\n   Petal.Length Petal.Width   Species maxLength minWidth\r\n1:          5.0         1.5 virginica       7.9      0.1\r\n2:          1.3         0.3    setosa       7.9      0.1\r\n3:          1.5         0.1    setosa       7.9      0.1\r\n4:           NA          NA      <NA>        NA       NA\r\n5:           NA          NA      <NA>        NA       NA\r\n   Sepal.Length.Width\r\n1:              6/2.2\r\n2:            4.5/2.3\r\n3:            5.2/4.1\r\n4:               <NA>\r\n5:               <NA>\r\n\r\nHands - on 4\r\nExercise 1\r\nPrint the fifth row for each species.\r\n\r\n\r\nstarwarsDT <- as.data.table(starwars)\r\nstarwarsDT[, if(.N >= 5) .SD[5L], by = species]\r\n\r\n\r\n   species               name height mass hair_color  skin_color\r\n1:   Human Beru Whitesun lars    165   75      brown       light\r\n2:   Droid             R4-P17     96   NA       none silver, red\r\n   eye_color birth_year    sex   gender homeworld\r\n1:      blue         47 female feminine  Tatooine\r\n2: red, blue         NA   none feminine      <NA>\r\n                                                 films vehicles\r\n1: Attack of the Clones,Revenge of the Sith,A New Hope         \r\n2:            Attack of the Clones,Revenge of the Sith         \r\n   starships\r\n1:          \r\n2:          \r\n\r\nExercise 2\r\nConsider the columns name, status and pressure.\r\nPrint for each storm, the 5 entries with the lowest pressure.\r\n\r\n\r\nstormsDT <- as.data.table(storms)\r\nstormsDT[, .(name, status, pressure)][order(name, pressure)][, .SD[1:min(5, .N)], by = name][1:5]\r\n\r\n\r\n       name              status pressure\r\n1: AL011993 tropical depression      999\r\n2: AL011993 tropical depression      999\r\n3: AL011993 tropical depression      999\r\n4: AL011993 tropical depression      999\r\n5: AL011993 tropical depression     1000\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-15T20:23:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "A deeper look into data.table functionalities",
    "description": "Welcome to our new blog, More into data.table. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Basil Okola",
        "url": "https://github.com/Bokola"
      }
    ],
    "date": "2020-12-04",
    "categories": [],
    "contents": "\r\n\r\nData table syntax is of the form DT[i, j, by]\r\ni: on which row\r\nj: what to do\r\nby: group by what\r\nData Manipulation\r\nJust looking\r\nRemoving information\r\nAdding information\r\nReducing information\r\nCombining information\r\nJust looking\r\n\r\n\r\nirisDT = as.data.table(iris)\r\ntables() # show loaded tables\r\n\r\n\r\n     NAME NROW NCOL MB\r\n1: irisDT  150    5  0\r\n                                                        COLS KEY\r\n1: Sepal.Length,Sepal.Width,Petal.Length,Petal.Width,Species    \r\nTotal: 0MB\r\n\r\nSorting / ordering rows\r\nsetorder(data.table, …)\r\n-: to sort a variable in descending order\r\n\r\n\r\nsetorder(irisDT, Sepal.Length, Sepal.Width)\r\nirisDT\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n  1:          4.3         3.0          1.1         0.1    setosa\r\n  2:          4.4         2.9          1.4         0.2    setosa\r\n  3:          4.4         3.0          1.3         0.2    setosa\r\n  4:          4.4         3.2          1.3         0.2    setosa\r\n  5:          4.5         2.3          1.3         0.3    setosa\r\n ---                                                            \r\n146:          7.7         2.6          6.9         2.3 virginica\r\n147:          7.7         2.8          6.7         2.0 virginica\r\n148:          7.7         3.0          6.1         2.3 virginica\r\n149:          7.7         3.8          6.7         2.2 virginica\r\n150:          7.9         3.8          6.4         2.0 virginica\r\n\r\n\r\n\r\nsetorder(irisDT, -Species, Sepal.Width)\r\nirisDT\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n  1:          6.0         2.2          5.0         1.5 virginica\r\n  2:          4.9         2.5          4.5         1.7 virginica\r\n  3:          5.7         2.5          5.0         2.0 virginica\r\n  4:          6.3         2.5          5.0         1.9 virginica\r\n  5:          6.7         2.5          5.8         1.8 virginica\r\n ---                                                            \r\n146:          5.4         3.9          1.3         0.4    setosa\r\n147:          5.8         4.0          1.2         0.2    setosa\r\n148:          5.2         4.1          1.5         0.1    setosa\r\n149:          5.5         4.2          1.4         0.2    setosa\r\n150:          5.7         4.4          1.5         0.4    setosa\r\n\r\nRemoving information\r\nSelecting rows\r\nDT[i, j, by]\r\ni: on which rows?\r\n\r\n\r\nirisDT[3:4]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          5.7         2.5            5         2.0 virginica\r\n2:          6.3         2.5            5         1.9 virginica\r\n\r\n\r\n\r\nirisDT[3:4,] #row 3 and 4\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          5.7         2.5            5         2.0 virginica\r\n2:          6.3         2.5            5         1.9 virginica\r\n\r\nirisDT[-(1:5)] #delete rows 1:5\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n  1:          6.1         2.6          5.6         1.4 virginica\r\n  2:          7.7         2.6          6.9         2.3 virginica\r\n  3:          5.8         2.7          5.1         1.9 virginica\r\n  4:          5.8         2.7          5.1         1.9 virginica\r\n  5:          6.3         2.7          4.9         1.8 virginica\r\n ---                                                            \r\n141:          5.4         3.9          1.3         0.4    setosa\r\n142:          5.8         4.0          1.2         0.2    setosa\r\n143:          5.2         4.1          1.5         0.1    setosa\r\n144:          5.5         4.2          1.4         0.2    setosa\r\n145:          5.7         4.4          1.5         0.4    setosa\r\n\r\nirisDT[!(1:5)] # using false to delete as well\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n  1:          6.1         2.6          5.6         1.4 virginica\r\n  2:          7.7         2.6          6.9         2.3 virginica\r\n  3:          5.8         2.7          5.1         1.9 virginica\r\n  4:          5.8         2.7          5.1         1.9 virginica\r\n  5:          6.3         2.7          4.9         1.8 virginica\r\n ---                                                            \r\n141:          5.4         3.9          1.3         0.4    setosa\r\n142:          5.8         4.0          1.2         0.2    setosa\r\n143:          5.2         4.1          1.5         0.1    setosa\r\n144:          5.5         4.2          1.4         0.2    setosa\r\n145:          5.7         4.4          1.5         0.4    setosa\r\n\r\nirisDT[.N] # last row\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n1:          5.7         4.4          1.5         0.4  setosa\r\n\r\nirisDT[1:(.N-10)] # all but the last 10 rows\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n  1:          6.0         2.2          5.0         1.5 virginica\r\n  2:          4.9         2.5          4.5         1.7 virginica\r\n  3:          5.7         2.5          5.0         2.0 virginica\r\n  4:          6.3         2.5          5.0         1.9 virginica\r\n  5:          6.7         2.5          5.8         1.8 virginica\r\n ---                                                            \r\n136:          4.9         3.6          1.4         0.1    setosa\r\n137:          5.0         3.6          1.4         0.2    setosa\r\n138:          5.1         3.7          1.5         0.4    setosa\r\n139:          5.3         3.7          1.5         0.2    setosa\r\n140:          5.4         3.7          1.5         0.2    setosa\r\n\r\nirisDT[Species == \"virginica\"] # based on conditions fulfilled in columns\r\n\r\n\r\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n 1:          6.0         2.2          5.0         1.5 virginica\r\n 2:          4.9         2.5          4.5         1.7 virginica\r\n 3:          5.7         2.5          5.0         2.0 virginica\r\n 4:          6.3         2.5          5.0         1.9 virginica\r\n 5:          6.7         2.5          5.8         1.8 virginica\r\n 6:          6.1         2.6          5.6         1.4 virginica\r\n 7:          7.7         2.6          6.9         2.3 virginica\r\n 8:          5.8         2.7          5.1         1.9 virginica\r\n 9:          5.8         2.7          5.1         1.9 virginica\r\n10:          6.3         2.7          4.9         1.8 virginica\r\n11:          6.4         2.7          5.3         1.9 virginica\r\n12:          5.6         2.8          4.9         2.0 virginica\r\n13:          5.8         2.8          5.1         2.4 virginica\r\n14:          6.2         2.8          4.8         1.8 virginica\r\n15:          6.3         2.8          5.1         1.5 virginica\r\n16:          6.4         2.8          5.6         2.1 virginica\r\n17:          6.4         2.8          5.6         2.2 virginica\r\n18:          7.4         2.8          6.1         1.9 virginica\r\n19:          7.7         2.8          6.7         2.0 virginica\r\n20:          6.3         2.9          5.6         1.8 virginica\r\n21:          7.3         2.9          6.3         1.8 virginica\r\n22:          5.9         3.0          5.1         1.8 virginica\r\n23:          6.0         3.0          4.8         1.8 virginica\r\n24:          6.1         3.0          4.9         1.8 virginica\r\n25:          6.5         3.0          5.8         2.2 virginica\r\n26:          6.5         3.0          5.5         1.8 virginica\r\n27:          6.5         3.0          5.2         2.0 virginica\r\n28:          6.7         3.0          5.2         2.3 virginica\r\n29:          6.8         3.0          5.5         2.1 virginica\r\n30:          7.1         3.0          5.9         2.1 virginica\r\n31:          7.2         3.0          5.8         1.6 virginica\r\n32:          7.6         3.0          6.6         2.1 virginica\r\n33:          7.7         3.0          6.1         2.3 virginica\r\n34:          6.4         3.1          5.5         1.8 virginica\r\n35:          6.7         3.1          5.6         2.4 virginica\r\n36:          6.9         3.1          5.4         2.1 virginica\r\n37:          6.9         3.1          5.1         2.3 virginica\r\n38:          6.4         3.2          5.3         2.3 virginica\r\n39:          6.5         3.2          5.1         2.0 virginica\r\n40:          6.8         3.2          5.9         2.3 virginica\r\n41:          6.9         3.2          5.7         2.3 virginica\r\n42:          7.2         3.2          6.0         1.8 virginica\r\n43:          6.3         3.3          6.0         2.5 virginica\r\n44:          6.7         3.3          5.7         2.1 virginica\r\n45:          6.7         3.3          5.7         2.5 virginica\r\n46:          6.2         3.4          5.4         2.3 virginica\r\n47:          6.3         3.4          5.6         2.4 virginica\r\n48:          7.2         3.6          6.1         2.5 virginica\r\n49:          7.7         3.8          6.7         2.2 virginica\r\n50:          7.9         3.8          6.4         2.0 virginica\r\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n\r\nirisDT[Species %like% \"^v\"] # using %like% helper function\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\r\n  1:          6.0         2.2          5.0         1.5  virginica\r\n  2:          4.9         2.5          4.5         1.7  virginica\r\n  3:          5.7         2.5          5.0         2.0  virginica\r\n  4:          6.3         2.5          5.0         1.9  virginica\r\n  5:          6.7         2.5          5.8         1.8  virginica\r\n  6:          6.1         2.6          5.6         1.4  virginica\r\n  7:          7.7         2.6          6.9         2.3  virginica\r\n  8:          5.8         2.7          5.1         1.9  virginica\r\n  9:          5.8         2.7          5.1         1.9  virginica\r\n 10:          6.3         2.7          4.9         1.8  virginica\r\n 11:          6.4         2.7          5.3         1.9  virginica\r\n 12:          5.6         2.8          4.9         2.0  virginica\r\n 13:          5.8         2.8          5.1         2.4  virginica\r\n 14:          6.2         2.8          4.8         1.8  virginica\r\n 15:          6.3         2.8          5.1         1.5  virginica\r\n 16:          6.4         2.8          5.6         2.1  virginica\r\n 17:          6.4         2.8          5.6         2.2  virginica\r\n 18:          7.4         2.8          6.1         1.9  virginica\r\n 19:          7.7         2.8          6.7         2.0  virginica\r\n 20:          6.3         2.9          5.6         1.8  virginica\r\n 21:          7.3         2.9          6.3         1.8  virginica\r\n 22:          5.9         3.0          5.1         1.8  virginica\r\n 23:          6.0         3.0          4.8         1.8  virginica\r\n 24:          6.1         3.0          4.9         1.8  virginica\r\n 25:          6.5         3.0          5.8         2.2  virginica\r\n 26:          6.5         3.0          5.5         1.8  virginica\r\n 27:          6.5         3.0          5.2         2.0  virginica\r\n 28:          6.7         3.0          5.2         2.3  virginica\r\n 29:          6.8         3.0          5.5         2.1  virginica\r\n 30:          7.1         3.0          5.9         2.1  virginica\r\n 31:          7.2         3.0          5.8         1.6  virginica\r\n 32:          7.6         3.0          6.6         2.1  virginica\r\n 33:          7.7         3.0          6.1         2.3  virginica\r\n 34:          6.4         3.1          5.5         1.8  virginica\r\n 35:          6.7         3.1          5.6         2.4  virginica\r\n 36:          6.9         3.1          5.4         2.1  virginica\r\n 37:          6.9         3.1          5.1         2.3  virginica\r\n 38:          6.4         3.2          5.3         2.3  virginica\r\n 39:          6.5         3.2          5.1         2.0  virginica\r\n 40:          6.8         3.2          5.9         2.3  virginica\r\n 41:          6.9         3.2          5.7         2.3  virginica\r\n 42:          7.2         3.2          6.0         1.8  virginica\r\n 43:          6.3         3.3          6.0         2.5  virginica\r\n 44:          6.7         3.3          5.7         2.1  virginica\r\n 45:          6.7         3.3          5.7         2.5  virginica\r\n 46:          6.2         3.4          5.4         2.3  virginica\r\n 47:          6.3         3.4          5.6         2.4  virginica\r\n 48:          7.2         3.6          6.1         2.5  virginica\r\n 49:          7.7         3.8          6.7         2.2  virginica\r\n 50:          7.9         3.8          6.4         2.0  virginica\r\n 51:          5.0         2.0          3.5         1.0 versicolor\r\n 52:          6.0         2.2          4.0         1.0 versicolor\r\n 53:          6.2         2.2          4.5         1.5 versicolor\r\n 54:          5.0         2.3          3.3         1.0 versicolor\r\n 55:          5.5         2.3          4.0         1.3 versicolor\r\n 56:          6.3         2.3          4.4         1.3 versicolor\r\n 57:          4.9         2.4          3.3         1.0 versicolor\r\n 58:          5.5         2.4          3.8         1.1 versicolor\r\n 59:          5.5         2.4          3.7         1.0 versicolor\r\n 60:          5.1         2.5          3.0         1.1 versicolor\r\n 61:          5.5         2.5          4.0         1.3 versicolor\r\n 62:          5.6         2.5          3.9         1.1 versicolor\r\n 63:          6.3         2.5          4.9         1.5 versicolor\r\n 64:          5.5         2.6          4.4         1.2 versicolor\r\n 65:          5.7         2.6          3.5         1.0 versicolor\r\n 66:          5.8         2.6          4.0         1.2 versicolor\r\n 67:          5.2         2.7          3.9         1.4 versicolor\r\n 68:          5.6         2.7          4.2         1.3 versicolor\r\n 69:          5.8         2.7          4.1         1.0 versicolor\r\n 70:          5.8         2.7          3.9         1.2 versicolor\r\n 71:          6.0         2.7          5.1         1.6 versicolor\r\n 72:          5.7         2.8          4.5         1.3 versicolor\r\n 73:          5.7         2.8          4.1         1.3 versicolor\r\n 74:          6.1         2.8          4.0         1.3 versicolor\r\n 75:          6.1         2.8          4.7         1.2 versicolor\r\n 76:          6.5         2.8          4.6         1.5 versicolor\r\n 77:          6.8         2.8          4.8         1.4 versicolor\r\n 78:          5.6         2.9          3.6         1.3 versicolor\r\n 79:          5.7         2.9          4.2         1.3 versicolor\r\n 80:          6.0         2.9          4.5         1.5 versicolor\r\n 81:          6.1         2.9          4.7         1.4 versicolor\r\n 82:          6.2         2.9          4.3         1.3 versicolor\r\n 83:          6.4         2.9          4.3         1.3 versicolor\r\n 84:          6.6         2.9          4.6         1.3 versicolor\r\n 85:          5.4         3.0          4.5         1.5 versicolor\r\n 86:          5.6         3.0          4.5         1.5 versicolor\r\n 87:          5.6         3.0          4.1         1.3 versicolor\r\n 88:          5.7         3.0          4.2         1.2 versicolor\r\n 89:          5.9         3.0          4.2         1.5 versicolor\r\n 90:          6.1         3.0          4.6         1.4 versicolor\r\n 91:          6.6         3.0          4.4         1.4 versicolor\r\n 92:          6.7         3.0          5.0         1.7 versicolor\r\n 93:          6.7         3.1          4.4         1.4 versicolor\r\n 94:          6.7         3.1          4.7         1.5 versicolor\r\n 95:          6.9         3.1          4.9         1.5 versicolor\r\n 96:          5.9         3.2          4.8         1.8 versicolor\r\n 97:          6.4         3.2          4.5         1.5 versicolor\r\n 98:          7.0         3.2          4.7         1.4 versicolor\r\n 99:          6.3         3.3          4.7         1.6 versicolor\r\n100:          6.0         3.4          4.5         1.6 versicolor\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\r\n\r\nirisDT[Petal.Width %between% c(0.3, 0.4)] # values in an interval\r\n\r\n\r\n    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n 1:          4.5         2.3          1.3         0.3  setosa\r\n 2:          4.8         3.0          1.4         0.3  setosa\r\n 3:          4.6         3.4          1.4         0.3  setosa\r\n 4:          5.0         3.4          1.6         0.4  setosa\r\n 5:          5.4         3.4          1.5         0.4  setosa\r\n 6:          5.0         3.5          1.3         0.3  setosa\r\n 7:          5.1         3.5          1.4         0.3  setosa\r\n 8:          5.1         3.7          1.5         0.4  setosa\r\n 9:          5.1         3.8          1.5         0.3  setosa\r\n10:          5.1         3.8          1.9         0.4  setosa\r\n11:          5.7         3.8          1.7         0.3  setosa\r\n12:          5.4         3.9          1.7         0.4  setosa\r\n13:          5.4         3.9          1.3         0.4  setosa\r\n14:          5.7         4.4          1.5         0.4  setosa\r\n\r\nSelecting columns\r\nDT[i, j, by]\r\nj: what to do? -> select columns\r\n\r\n\r\nirisDT[, Species] # returns a vector\r\n\r\n\r\n  [1] virginica  virginica  virginica  virginica  virginica \r\n  [6] virginica  virginica  virginica  virginica  virginica \r\n [11] virginica  virginica  virginica  virginica  virginica \r\n [16] virginica  virginica  virginica  virginica  virginica \r\n [21] virginica  virginica  virginica  virginica  virginica \r\n [26] virginica  virginica  virginica  virginica  virginica \r\n [31] virginica  virginica  virginica  virginica  virginica \r\n [36] virginica  virginica  virginica  virginica  virginica \r\n [41] virginica  virginica  virginica  virginica  virginica \r\n [46] virginica  virginica  virginica  virginica  virginica \r\n [51] versicolor versicolor versicolor versicolor versicolor\r\n [56] versicolor versicolor versicolor versicolor versicolor\r\n [61] versicolor versicolor versicolor versicolor versicolor\r\n [66] versicolor versicolor versicolor versicolor versicolor\r\n [71] versicolor versicolor versicolor versicolor versicolor\r\n [76] versicolor versicolor versicolor versicolor versicolor\r\n [81] versicolor versicolor versicolor versicolor versicolor\r\n [86] versicolor versicolor versicolor versicolor versicolor\r\n [91] versicolor versicolor versicolor versicolor versicolor\r\n [96] versicolor versicolor versicolor versicolor versicolor\r\n[101] setosa     setosa     setosa     setosa     setosa    \r\n[106] setosa     setosa     setosa     setosa     setosa    \r\n[111] setosa     setosa     setosa     setosa     setosa    \r\n[116] setosa     setosa     setosa     setosa     setosa    \r\n[121] setosa     setosa     setosa     setosa     setosa    \r\n[126] setosa     setosa     setosa     setosa     setosa    \r\n[131] setosa     setosa     setosa     setosa     setosa    \r\n[136] setosa     setosa     setosa     setosa     setosa    \r\n[141] setosa     setosa     setosa     setosa     setosa    \r\n[146] setosa     setosa     setosa     setosa     setosa    \r\nLevels: setosa versicolor virginica\r\n\r\nirisDT[, \"Species\"] # returns a dataframe/data.table\r\n\r\n\r\n       Species\r\n  1: virginica\r\n  2: virginica\r\n  3: virginica\r\n  4: virginica\r\n  5: virginica\r\n ---          \r\n146:    setosa\r\n147:    setosa\r\n148:    setosa\r\n149:    setosa\r\n150:    setosa\r\n\r\nirisDT[, -c(\"Species\")]\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\r\n  1:          6.0         2.2          5.0         1.5\r\n  2:          4.9         2.5          4.5         1.7\r\n  3:          5.7         2.5          5.0         2.0\r\n  4:          6.3         2.5          5.0         1.9\r\n  5:          6.7         2.5          5.8         1.8\r\n ---                                                  \r\n146:          5.4         3.9          1.3         0.4\r\n147:          5.8         4.0          1.2         0.2\r\n148:          5.2         4.1          1.5         0.1\r\n149:          5.5         4.2          1.4         0.2\r\n150:          5.7         4.4          1.5         0.4\r\n\r\nirisDT[, !c(\"Species\")]\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\r\n  1:          6.0         2.2          5.0         1.5\r\n  2:          4.9         2.5          4.5         1.7\r\n  3:          5.7         2.5          5.0         2.0\r\n  4:          6.3         2.5          5.0         1.9\r\n  5:          6.7         2.5          5.8         1.8\r\n ---                                                  \r\n146:          5.4         3.9          1.3         0.4\r\n147:          5.8         4.0          1.2         0.2\r\n148:          5.2         4.1          1.5         0.1\r\n149:          5.5         4.2          1.4         0.2\r\n150:          5.7         4.4          1.5         0.4\r\n\r\nirisDT[, list(Species, LS = Sepal.Length)] # select and rename\r\n\r\n\r\n       Species  LS\r\n  1: virginica 6.0\r\n  2: virginica 4.9\r\n  3: virginica 5.7\r\n  4: virginica 6.3\r\n  5: virginica 6.7\r\n ---              \r\n146:    setosa 5.4\r\n147:    setosa 5.8\r\n148:    setosa 5.2\r\n149:    setosa 5.5\r\n150:    setosa 5.7\r\n\r\n# .() is an alias for list\r\n\r\nirisDT[, .(Species, SL = Sepal.Length)]\r\n\r\n\r\n       Species  SL\r\n  1: virginica 6.0\r\n  2: virginica 4.9\r\n  3: virginica 5.7\r\n  4: virginica 6.3\r\n  5: virginica 6.7\r\n ---              \r\n146:    setosa 5.4\r\n147:    setosa 5.8\r\n148:    setosa 5.2\r\n149:    setosa 5.5\r\n150:    setosa 5.7\r\n\r\nirisDT[Species == \"virginica\" & Sepal.Length > 7 & Sepal.Width < 3, !c(\"Species\")]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width\r\n1:          7.7         2.6          6.9         2.3\r\n2:          7.4         2.8          6.1         1.9\r\n3:          7.7         2.8          6.7         2.0\r\n4:          7.3         2.9          6.3         1.8\r\n\r\nirisDT[Species == \"virginica\" &\r\nSepal.Length > 7 &Sepal.Width < 3,.(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width\r\n1:          7.7         2.6          6.9         2.3\r\n2:          7.4         2.8          6.1         1.9\r\n3:          7.7         2.8          6.7         2.0\r\n4:          7.3         2.9          6.3         1.8\r\n\r\nAdding information\r\nMaking new columns - preserving existing ones\r\nDT[i, j, by]\r\nj: what to do? -> compute new columns preserving existing ones\r\ndata.table uses a new operator := to add/update/delete columns (by reference)\r\noption 1\r\n\r\n\r\nirisDT[, maxLength := max(Sepal.Length, Petal.Length)][,\r\n          minWidth := min(Sepal.Length, Petal.Width)]\r\nirisDT\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n  1:          6.0         2.2          5.0         1.5 virginica\r\n  2:          4.9         2.5          4.5         1.7 virginica\r\n  3:          5.7         2.5          5.0         2.0 virginica\r\n  4:          6.3         2.5          5.0         1.9 virginica\r\n  5:          6.7         2.5          5.8         1.8 virginica\r\n ---                                                            \r\n146:          5.4         3.9          1.3         0.4    setosa\r\n147:          5.8         4.0          1.2         0.2    setosa\r\n148:          5.2         4.1          1.5         0.1    setosa\r\n149:          5.5         4.2          1.4         0.2    setosa\r\n150:          5.7         4.4          1.5         0.4    setosa\r\n     maxLength minWidth\r\n  1:       7.9      0.1\r\n  2:       7.9      0.1\r\n  3:       7.9      0.1\r\n  4:       7.9      0.1\r\n  5:       7.9      0.1\r\n ---                   \r\n146:       7.9      0.1\r\n147:       7.9      0.1\r\n148:       7.9      0.1\r\n149:       7.9      0.1\r\n150:       7.9      0.1\r\n\r\noption 2\r\nLHS := RHS form\r\n\r\n\r\nirisDT[, c(\"maxLength\", \"minWidth\") :=\r\n         list(\r\n           max(Sepal.Length, Petal.Length),\r\n           min(Sepal.Width, Petal.Width)\r\n         )]\r\n\r\nirisDT\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n  1:          6.0         2.2          5.0         1.5 virginica\r\n  2:          4.9         2.5          4.5         1.7 virginica\r\n  3:          5.7         2.5          5.0         2.0 virginica\r\n  4:          6.3         2.5          5.0         1.9 virginica\r\n  5:          6.7         2.5          5.8         1.8 virginica\r\n ---                                                            \r\n146:          5.4         3.9          1.3         0.4    setosa\r\n147:          5.8         4.0          1.2         0.2    setosa\r\n148:          5.2         4.1          1.5         0.1    setosa\r\n149:          5.5         4.2          1.4         0.2    setosa\r\n150:          5.7         4.4          1.5         0.4    setosa\r\n     maxLength minWidth\r\n  1:       7.9      0.1\r\n  2:       7.9      0.1\r\n  3:       7.9      0.1\r\n  4:       7.9      0.1\r\n  5:       7.9      0.1\r\n ---                   \r\n146:       7.9      0.1\r\n147:       7.9      0.1\r\n148:       7.9      0.1\r\n149:       7.9      0.1\r\n150:       7.9      0.1\r\n\r\nOption 3\r\nFunctional form\r\n\r\n\r\nirisDT[, `:=` (\r\n  maxLength = max(Sepal.Length, Petal.Length),\r\n  minWidth = min(Sepal.Width, Petal.Width)\r\n)]\r\nirisDT\r\n\r\n\r\n     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n  1:          6.0         2.2          5.0         1.5 virginica\r\n  2:          4.9         2.5          4.5         1.7 virginica\r\n  3:          5.7         2.5          5.0         2.0 virginica\r\n  4:          6.3         2.5          5.0         1.9 virginica\r\n  5:          6.7         2.5          5.8         1.8 virginica\r\n ---                                                            \r\n146:          5.4         3.9          1.3         0.4    setosa\r\n147:          5.8         4.0          1.2         0.2    setosa\r\n148:          5.2         4.1          1.5         0.1    setosa\r\n149:          5.5         4.2          1.4         0.2    setosa\r\n150:          5.7         4.4          1.5         0.4    setosa\r\n     maxLength minWidth\r\n  1:       7.9      0.1\r\n  2:       7.9      0.1\r\n  3:       7.9      0.1\r\n  4:       7.9      0.1\r\n  5:       7.9      0.1\r\n ---                   \r\n146:       7.9      0.1\r\n147:       7.9      0.1\r\n148:       7.9      0.1\r\n149:       7.9      0.1\r\n150:       7.9      0.1\r\n\r\nMaking new columns - dropping existing ones\r\nDT[i, j, by]\r\nj: what to do? -> compute new columns dropping existing ones\r\n\r\n\r\nirisDT[, .(maxLength = pmax(Sepal.Length, Petal.Length),\r\nminWidth = pmin(Sepal.Width, Petal.Width))]\r\n\r\n\r\n     maxLength minWidth\r\n  1:       6.0      1.5\r\n  2:       4.9      1.7\r\n  3:       5.7      2.0\r\n  4:       6.3      1.9\r\n  5:       6.7      1.8\r\n ---                   \r\n146:       5.4      0.4\r\n147:       5.8      0.2\r\n148:       5.2      0.1\r\n149:       5.5      0.2\r\n150:       5.7      0.4\r\n\r\n\r\n\r\nnewIris <- irisDT[, .(Sepal.Area = Sepal.Width * Sepal.Length,\r\nPetal.Area = Petal.Width * Petal.Length)]\r\nnewIris[, Area.Ratio := Petal.Area / Sepal.Area]\r\nnewIris\r\n\r\n\r\n     Sepal.Area Petal.Area  Area.Ratio\r\n  1:      13.20       7.50 0.568181818\r\n  2:      12.25       7.65 0.624489796\r\n  3:      14.25      10.00 0.701754386\r\n  4:      15.75       9.50 0.603174603\r\n  5:      16.75      10.44 0.623283582\r\n ---                                  \r\n146:      21.06       0.52 0.024691358\r\n147:      23.20       0.24 0.010344828\r\n148:      21.32       0.15 0.007035647\r\n149:      23.10       0.28 0.012121212\r\n150:      25.08       0.60 0.023923445\r\n\r\nReducing information\r\nSummarizing rows\r\n\r\n\r\nirisDT[, .(meanSepalLength = mean(Sepal.Length),\r\n           meanSepalWidth = mean(Sepal.Width),\r\n           meanPetalLength = mean(Petal.Length),\r\n           meanPetalWidth = mean(Petal.Width))]\r\n\r\n\r\n   meanSepalLength meanSepalWidth meanPetalLength meanPetalWidth\r\n1:        5.843333       3.057333           3.758       1.199333\r\n\r\n\r\n\r\nirisDT[, .(nSamples = .N, nSpecies = uniqueN(Species))\r\n       ]\r\n\r\n\r\n   nSamples nSpecies\r\n1:      150        3\r\n\r\nSummarizing rows with filtering\r\n\r\n\r\nirisDT[Species == \"versicolor\",.(\r\n  meanSepalLength = mean(Sepal.Length),\r\n           meanSepalWidth = mean(Sepal.Width),\r\n           meanPetalLength = mean(Petal.Length),\r\n           meanPetalWidth = mean(Petal.Width)\r\n)]\r\n\r\n\r\n   meanSepalLength meanSepalWidth meanPetalLength meanPetalWidth\r\n1:           5.936           2.77            4.26          1.326\r\n\r\nGrouping by one or more variables\r\n\r\n\r\nirisDT[, .(meanSepalLength = mean(Sepal.Length),\r\nmeanSepalWidth = mean(Sepal.Width),\r\nmeanPetalLength = mean(Petal.Length),\r\nmeanPetalWidth = mean(Petal.Width)), by = \"Species\"]\r\n\r\n\r\n      Species meanSepalLength meanSepalWidth meanPetalLength\r\n1:  virginica           6.588          2.974           5.552\r\n2: versicolor           5.936          2.770           4.260\r\n3:     setosa           5.006          3.428           1.462\r\n   meanPetalWidth\r\n1:          2.026\r\n2:          1.326\r\n3:          0.246\r\n\r\n\r\n\r\nirisDT[, .(nSamples = .N, nSpecies = uniqueN(Species)), by = .(Species)]\r\n\r\n\r\n      Species nSamples nSpecies\r\n1:  virginica       50        1\r\n2: versicolor       50        1\r\n3:     setosa       50        1\r\n\r\nCombining information\r\njoining 2 data.tables\r\nInner join: return all rows from x where there are matching values in y and all columns from x and y. In case of multiple matches between x and y, all combinations of the matches are returned.\r\nFull join: return all rows and columns from both x and y. \r\nLeft join: return all rows from x and all columns from x and y. In case of multiple matches between x and y, all combinations of the matches are returned.\r\nRight join: return all rows from y and all columns from x and y. In case of multiple matches between x and y, all combinations of the matches are returned.\r\nDT[i, on]\r\ni: join to which data.table?\r\non: join key columns?\r\n\r\n\r\n(x <- data.table(id = c(1, 2, 4, 5, 6),\r\nx = c(9, 12, 14, 21, 8)))\r\n\r\n\r\n   id  x\r\n1:  1  9\r\n2:  2 12\r\n3:  4 14\r\n4:  5 21\r\n5:  6  8\r\n\r\n(y <- data.table(id = c(1, 3, 4, 6, 6),\r\ny = c(8, 14, 19, 2, 4)))\r\n\r\n\r\n   id  y\r\n1:  1  8\r\n2:  3 14\r\n3:  4 19\r\n4:  6  2\r\n5:  6  4\r\n\r\ninner join - data.table\r\n\r\n\r\ny[x, on = .(id), nomatch = 0]\r\n\r\n\r\n   id  y  x\r\n1:  1  8  9\r\n2:  4 19 14\r\n3:  6  2  8\r\n4:  6  4  8\r\n\r\nfull join - merge\r\n\r\n\r\nmerge.data.table(x = x, y = y, by = \"id\", all = TRUE)\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  2 12 NA\r\n3:  3 NA 14\r\n4:  4 14 19\r\n5:  5 21 NA\r\n6:  6  8  2\r\n7:  6  8  4\r\n\r\nleft join - merge\r\n\r\n\r\nmerge.data.table(x = x, y = y, by = \"id\", all.x = TRUE)\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  2 12 NA\r\n3:  4 14 19\r\n4:  5 21 NA\r\n5:  6  8  2\r\n6:  6  8  4\r\n\r\nleft join - data.table\r\n\r\n\r\ny[x, on = .(id)]\r\n\r\n\r\n   id  y  x\r\n1:  1  8  9\r\n2:  2 NA 12\r\n3:  4 19 14\r\n4:  5 NA 21\r\n5:  6  2  8\r\n6:  6  4  8\r\n\r\nright join - merge\r\n\r\n\r\ndata.table::merge.data.table(x = x, y = y, by = \"id\", all.y = T)\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  3 NA 14\r\n3:  4 14 19\r\n4:  6  8  2\r\n5:  6  8  4\r\n\r\nright jpin - data.table\r\n\r\n\r\nx[y, on = .(id)]\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  3 NA 14\r\n3:  4 14 19\r\n4:  6  8  2\r\n5:  6  8  4\r\n\r\nantijoin - data.table\r\n\r\n\r\nx[!y, on = .(id)]\r\n\r\n\r\n   id  x\r\n1:  2 12\r\n2:  5 21\r\n\r\nKeys\r\nNo need of the on argument when performing a join\r\nSorts the data.table in memory by the key column(s)\r\nMultiple columns can be set and used as keys\r\nUseful functions:\r\nsetkey(DT, …) or setkeyv(DT, keys)\r\nhaskey(DT)\r\nkey(DT\r\n\r\n\r\nsetkey(x, id)\r\nsetkey(y, \"id\")\r\nhaskey(x); haskey(y)\r\n\r\n\r\n[1] TRUE\r\n[1] TRUE\r\n\r\nkey(x); key(y)\r\n\r\n\r\n[1] \"id\"\r\n[1] \"id\"\r\n\r\njoins with keys already specified\r\n\r\n\r\n# Inner join x and y\r\nx[y, nomatch = 0]\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  4 14 19\r\n3:  6  8  2\r\n4:  6  8  4\r\n\r\n# Right join x and y\r\nx[y]\r\n\r\n\r\n   id  x  y\r\n1:  1  9  8\r\n2:  3 NA 14\r\n3:  4 14 19\r\n4:  6  8  2\r\n5:  6  8  4\r\n\r\n# Left join x and y\r\ny[x]\r\n\r\n\r\n   id  y  x\r\n1:  1  8  9\r\n2:  2 NA 12\r\n3:  4 19 14\r\n4:  5 NA 21\r\n5:  6  2  8\r\n6:  6  4  8\r\n\r\nExercise 3\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(data.table)\r\ndata(starwars)\r\nstarwarsDT <- as.data.table(starwars)\r\nstarwarsDT[1]\r\n\r\n\r\n             name height mass hair_color skin_color eye_color\r\n1: Luke Skywalker    172   77      blond       fair      blue\r\n   birth_year  sex    gender homeworld species\r\n1:         19 male masculine  Tatooine   Human\r\n                                                                                         films\r\n1: The Empire Strikes Back,Revenge of the Sith,Return of the Jedi,A New Hope,The Force Awakens\r\n                            vehicles               starships\r\n1: Snowspeeder,Imperial Speeder Bike X-wing,Imperial shuttle\r\n\r\nstarwarsDT[, .(min = min(mass, na.rm = T),\r\n               max = max(mass, na.rm = T),\r\n               nobs = .N), by = homeworld][nobs >1] \r\n\r\n\r\n    homeworld   min   max nobs\r\n 1:  Tatooine  32.0 136.0   10\r\n 2:     Naboo  32.0  85.0   11\r\n 3:  Alderaan  49.0  79.0    3\r\n 4:  Kashyyyk 112.0 136.0    2\r\n 5:  Corellia  77.0  80.0    2\r\n 6:      <NA>  17.0 140.0   10\r\n 7:    Kamino  78.2  88.0    3\r\n 8: Coruscant  50.0  50.0    3\r\n 9:    Ryloth  55.0  55.0    2\r\n10:    Mirial  50.0  56.2    2\r\n\r\nReshaping data with data.table\r\nTidy data\r\nEach variable is a column -Each observation is a row\r\nEach value is a cell\r\n\r\nTwo fundamental verbs\r\nmelt(): ‘Wide’ -> ‘long’ data\r\ndcast(): ‘long’ -> ‘wide’ data\r\n\r\nWide format\r\n\r\n\r\nknitr::kable(head(iris[1:4,], n =3))\r\n\r\n\r\n\r\nSepal.Length\r\n\r\n\r\nSepal.Width\r\n\r\n\r\nPetal.Length\r\n\r\n\r\nPetal.Width\r\n\r\n\r\nSpecies\r\n\r\n\r\n5.1\r\n\r\n\r\n3.5\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.9\r\n\r\n\r\n3.0\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.7\r\n\r\n\r\n3.2\r\n\r\n\r\n1.3\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\nmelt(wide_df, id.vars = 'grouping variable', variable.name = 'measures_across_grouping_var',value.name = 'value_name_as_desired')\r\nLong to wide\r\ndcast(long_df, group ~ atttribute, value.var = \"value\")\r\nSeparating and uniting information\r\n\r\n\r\nirisUnited <- irisDT[ , Sepal.Length.Width := paste0(Sepal.Length, \"/\", Sepal.Width)]\r\nirisUnited[1:5]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          6.0         2.2          5.0         1.5 virginica\r\n2:          4.9         2.5          4.5         1.7 virginica\r\n3:          5.7         2.5          5.0         2.0 virginica\r\n4:          6.3         2.5          5.0         1.9 virginica\r\n5:          6.7         2.5          5.8         1.8 virginica\r\n   maxLength minWidth Sepal.Length.Width\r\n1:       7.9      0.1              6/2.2\r\n2:       7.9      0.1            4.9/2.5\r\n3:       7.9      0.1            5.7/2.5\r\n4:       7.9      0.1            6.3/2.5\r\n5:       7.9      0.1            6.7/2.5\r\n\r\n\r\n\r\nirisSeparated = irisUnited[, c('Sepal.Length', 'Sepal.Width') := tstrsplit(Sepal.Length.Width, \"/\")]\r\nirisSeparated[1:5]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:            6         2.2          5.0         1.5 virginica\r\n2:          4.9         2.5          4.5         1.7 virginica\r\n3:          5.7         2.5          5.0         2.0 virginica\r\n4:          6.3         2.5          5.0         1.9 virginica\r\n5:          6.7         2.5          5.8         1.8 virginica\r\n   maxLength minWidth Sepal.Length.Width\r\n1:       7.9      0.1              6/2.2\r\n2:       7.9      0.1            4.9/2.5\r\n3:       7.9      0.1            5.7/2.5\r\n4:       7.9      0.1            6.3/2.5\r\n5:       7.9      0.1            6.7/2.5\r\n\r\nHands-on session\r\nExercise 1\r\n\r\n\r\ndata(\"relig_income\")\r\na = as.data.table(relig_income)\r\na[1:10]\r\n\r\n\r\n                   religion <$10k $10-20k $20-30k $30-40k $40-50k\r\n 1:                Agnostic    27      34      60      81      76\r\n 2:                 Atheist    12      27      37      52      35\r\n 3:                Buddhist    27      21      30      34      33\r\n 4:                Catholic   418     617     732     670     638\r\n 5:      Don’t know/refused    15      14      15      11      10\r\n 6:        Evangelical Prot   575     869    1064     982     881\r\n 7:                   Hindu     1       9       7       9      11\r\n 8: Historically Black Prot   228     244     236     238     197\r\n 9:       Jehovah's Witness    20      27      24      24      21\r\n10:                  Jewish    19      19      25      25      30\r\n    $50-75k $75-100k $100-150k >150k Don't know/refused\r\n 1:     137      122       109    84                 96\r\n 2:      70       73        59    74                 76\r\n 3:      58       62        39    53                 54\r\n 4:    1116      949       792   633               1489\r\n 5:      35       21        17    18                116\r\n 6:    1486      949       723   414               1529\r\n 7:      34       47        48    54                 37\r\n 8:     223      131        81    78                339\r\n 9:      30       15        11     6                 37\r\n10:      95       69        87   151                162\r\n\r\nb = melt(a, id.vars = \"religion\", variable.name = \"income_category\", value.name = \"n\")\r\n\r\n\r\n\r\nExercise 2\r\n\r\n\r\ndata(\"storms\")\r\naa = as.data.table(storms)\r\naa[, date:= paste0(year, \"-\", month,\"-\", day)]\r\n\r\n\r\n\r\nUseful symbols/ functions\r\n.SD: Subset of Data for each group\r\n.SD along with lapply can be used to apply any function to multiple columns by group\r\n\r\n.SDcols: Columns of the data.table that are included in .SD\r\nna.omit(): returns the object with incomplete cases removed\r\nSummarizing for specific condition\r\n\r\n\r\ncols = sapply(irisDT, is.numeric)\r\ncols = names(cols)[cols]\r\nirisDT[, lapply(.SD, mean), by = Species, .SDcols = cols]\r\n\r\n\r\n      Species Petal.Length Petal.Width maxLength minWidth\r\n1:  virginica        5.552       2.026       7.9      0.1\r\n2: versicolor        4.260       1.326       7.9      0.1\r\n3:     setosa        1.462       0.246       7.9      0.1\r\n\r\nSummarizing for specific variable of interest\r\n\r\n\r\nirisDT[, lapply(.SD, mean), by = Species, .SDcols = c(\"Petal.Length\", \"Petal.Width\")]\r\n\r\n\r\n      Species Petal.Length Petal.Width\r\n1:  virginica        5.552       2.026\r\n2: versicolor        4.260       1.326\r\n3:     setosa        1.462       0.246\r\n\r\nSummarizing all columns\r\n\r\n\r\nirisDT[ , unlist(recursive = FALSE, lapply(.(mean = mean, sd = sd, min = min, max = max), \r\n                                           function(f) lapply(.SD, f))), by = Species]\r\n\r\n\r\n      Species mean.Sepal.Length mean.Sepal.Width mean.Petal.Length\r\n1:  virginica                NA               NA             5.552\r\n2: versicolor                NA               NA             4.260\r\n3:     setosa                NA               NA             1.462\r\n   mean.Petal.Width mean.maxLength mean.minWidth\r\n1:            2.026            7.9           0.1\r\n2:            1.326            7.9           0.1\r\n3:            0.246            7.9           0.1\r\n   mean.Sepal.Length.Width sd.Sepal.Length sd.Sepal.Width\r\n1:                      NA       0.6358796      0.3224966\r\n2:                      NA       0.5161711      0.3137983\r\n3:                      NA       0.3524897      0.3790644\r\n   sd.Petal.Length sd.Petal.Width sd.maxLength sd.minWidth\r\n1:       0.5518947      0.2746501            0           0\r\n2:       0.4699110      0.1977527            0           0\r\n3:       0.1736640      0.1053856            0           0\r\n   sd.Sepal.Length.Width min.Sepal.Length min.Sepal.Width\r\n1:                    NA              4.9             2.2\r\n2:                    NA              4.9               2\r\n3:                    NA              4.3             2.3\r\n   min.Petal.Length min.Petal.Width min.maxLength min.minWidth\r\n1:              4.5             1.4           7.9          0.1\r\n2:              3.0             1.0           7.9          0.1\r\n3:              1.0             0.1           7.9          0.1\r\n   min.Sepal.Length.Width max.Sepal.Length max.Sepal.Width\r\n1:                4.9/2.5              7.9             3.8\r\n2:                4.9/2.4                7             3.4\r\n3:                  4.3/3              5.8             4.4\r\n   max.Petal.Length max.Petal.Width max.maxLength max.minWidth\r\n1:              6.9             2.5           7.9          0.1\r\n2:              5.1             1.8           7.9          0.1\r\n3:              1.9             0.6           7.9          0.1\r\n   max.Sepal.Length.Width\r\n1:                7.9/3.8\r\n2:                  7/3.2\r\n3:                  5/3.6\r\n\r\nHands-on 3\r\nExercise 1 - Compute the minimum, maximum, median and mean for all numeric columns\r\n\r\n\r\ndata(\"starwars\")\r\nstarwarsDT = as.data.table(starwars)\r\ncols = sapply(starwarsDT, is.numeric)\r\ncols = names(cols)[cols]\r\n\r\nstarwarsDT[, height:= as.numeric(height)]\r\n\r\nstarwarsDT[, unlist(recursive = F,\r\n                    lapply(.(min = min, max = max, median = median, mean = mean), function(f) lapply(.SD, f))), by = .(homeworld, species), .SDcols = cols]\r\n\r\n\r\n         homeworld        species min.height min.mass min.birth_year\r\n 1:       Tatooine          Human        163       NA           19.0\r\n 2:       Tatooine          Droid         97     32.0             NA\r\n 3:          Naboo          Droid         96     32.0           33.0\r\n 4:       Alderaan          Human        150       NA             NA\r\n 5:        Stewjon          Human        182     77.0           57.0\r\n 6:         Eriadu          Human        180       NA           64.0\r\n 7:       Kashyyyk        Wookiee        228    112.0             NA\r\n 8:       Corellia          Human        170     77.0           21.0\r\n 9:          Rodia         Rodian        173     74.0           44.0\r\n10:      Nal Hutta           Hutt        175   1358.0          600.0\r\n11:     Bestine IV          Human        180    110.0             NA\r\n12:           <NA> Yoda's species         66     17.0          896.0\r\n13:          Naboo          Human        157       NA             NA\r\n14:         Kamino          Human        183     78.2           31.5\r\n15:           <NA>          Droid         NA       NA             NA\r\n16:      Trandosha     Trandoshan        190    113.0           53.0\r\n17:        Socorro          Human        177     79.0           31.0\r\n18:         Bespin          Human        175     79.0           37.0\r\n19:       Mon Cala   Mon Calamari        180     83.0           41.0\r\n20:      Chandrila          Human        150       NA           48.0\r\n21:           <NA>          Human         NA       NA             NA\r\n22:          Endor           Ewok         88     20.0            8.0\r\n23:        Sullust      Sullustan        160     68.0             NA\r\n24: Cato Neimoidia      Neimodian        191     90.0             NA\r\n25:      Coruscant          Human        167       NA             NA\r\n26:          Naboo         Gungan        196       NA             NA\r\n27:          Naboo           <NA>        183       NA             NA\r\n28:       Toydaria      Toydarian        137       NA             NA\r\n29:      Malastare            Dug        112     40.0             NA\r\n30:       Dathomir         Zabrak        175     80.0           54.0\r\n31:         Ryloth        Twi'lek        178       NA             NA\r\n32:        Vulpter     Vulptereen         94     45.0             NA\r\n33:        Troiken          Xexto        122       NA             NA\r\n34:           Tund          Toong        163     65.0             NA\r\n35:     Haruun Kal          Human        188     84.0           72.0\r\n36:          Cerea         Cerean        198     82.0           92.0\r\n37:    Glee Anselm       Nautolan        196     87.0             NA\r\n38:       Iridonia         Zabrak        171       NA             NA\r\n39:      Coruscant     Tholothian        184     50.0             NA\r\n40:        Iktotch       Iktotchi        188       NA             NA\r\n41:        Quermia       Quermian        264       NA             NA\r\n42:          Dorin        Kel Dor        188     80.0           22.0\r\n43:       Champala       Chagrian        196       NA             NA\r\n44:       Geonosis      Geonosian        183     80.0             NA\r\n45:         Mirial       Mirialan        166     50.0           40.0\r\n46:        Serenno          Human        193     80.0          102.0\r\n47:   Concord Dawn          Human        183     79.0           66.0\r\n48:          Zolan       Clawdite        168     55.0             NA\r\n49:           Ojom       Besalisk        198    102.0             NA\r\n50:         Kamino       Kaminoan        213       NA             NA\r\n51:    Aleen Minor         Aleena         79     15.0             NA\r\n52:          Skako        Skakoan        193     48.0             NA\r\n53:     Muunilinst           Muun        191       NA             NA\r\n54:          Shili        Togruta        178     57.0             NA\r\n55:          Kalee        Kaleesh        216    159.0             NA\r\n56:         Umbara           <NA>        178     48.0             NA\r\n57:         Utapau         Pau'an        206     80.0             NA\r\n58:           <NA>           <NA>         NA       NA             NA\r\n         homeworld        species min.height min.mass min.birth_year\r\n    max.height max.mass max.birth_year median.height median.mass\r\n 1:        202       NA           82.0         180.5          NA\r\n 2:        167     75.0             NA         132.0        53.5\r\n 3:         96     32.0           33.0          96.0        32.0\r\n 4:        191       NA             NA         188.0          NA\r\n 5:        182     77.0           57.0         182.0        77.0\r\n 6:        180       NA           64.0         180.0          NA\r\n 7:        234    136.0             NA         231.0       124.0\r\n 8:        180     80.0           29.0         175.0        78.5\r\n 9:        173     74.0           44.0         173.0        74.0\r\n10:        175   1358.0          600.0         175.0      1358.0\r\n11:        180    110.0             NA         180.0       110.0\r\n12:         66     17.0          896.0          66.0        17.0\r\n13:        185       NA             NA         165.0          NA\r\n14:        183     78.2           31.5         183.0        78.2\r\n15:         NA       NA             NA            NA          NA\r\n16:        190    113.0           53.0         190.0       113.0\r\n17:        177     79.0           31.0         177.0        79.0\r\n18:        175     79.0           37.0         175.0        79.0\r\n19:        180     83.0           41.0         180.0        83.0\r\n20:        150       NA           48.0         150.0          NA\r\n21:         NA       NA             NA            NA          NA\r\n22:         88     20.0            8.0          88.0        20.0\r\n23:        160     68.0             NA         160.0        68.0\r\n24:        191     90.0             NA         191.0        90.0\r\n25:        170       NA             NA         168.5          NA\r\n26:        224       NA             NA         206.0          NA\r\n27:        183       NA             NA         183.0          NA\r\n28:        137       NA             NA         137.0          NA\r\n29:        112     40.0             NA         112.0        40.0\r\n30:        175     80.0           54.0         175.0        80.0\r\n31:        180       NA             NA         179.0          NA\r\n32:         94     45.0             NA          94.0        45.0\r\n33:        122       NA             NA         122.0          NA\r\n34:        163     65.0             NA         163.0        65.0\r\n35:        188     84.0           72.0         188.0        84.0\r\n36:        198     82.0           92.0         198.0        82.0\r\n37:        196     87.0             NA         196.0        87.0\r\n38:        171       NA             NA         171.0          NA\r\n39:        184     50.0             NA         184.0        50.0\r\n40:        188       NA             NA         188.0          NA\r\n41:        264       NA             NA         264.0          NA\r\n42:        188     80.0           22.0         188.0        80.0\r\n43:        196       NA             NA         196.0          NA\r\n44:        183     80.0             NA         183.0        80.0\r\n45:        170     56.2           58.0         168.0        53.1\r\n46:        193     80.0          102.0         193.0        80.0\r\n47:        183     79.0           66.0         183.0        79.0\r\n48:        168     55.0             NA         168.0        55.0\r\n49:        198    102.0             NA         198.0       102.0\r\n50:        229       NA             NA         221.0          NA\r\n51:         79     15.0             NA          79.0        15.0\r\n52:        193     48.0             NA         193.0        48.0\r\n53:        191       NA             NA         191.0          NA\r\n54:        178     57.0             NA         178.0        57.0\r\n55:        216    159.0             NA         216.0       159.0\r\n56:        178     48.0             NA         178.0        48.0\r\n57:        206     80.0             NA         206.0        80.0\r\n58:         NA       NA             NA            NA          NA\r\n    max.height max.mass max.birth_year median.height median.mass\r\n    median.birth_year mean.height mean.mass mean.birth_year\r\n 1:             44.45    179.2500        NA          47.475\r\n 2:                NA    132.0000      53.5              NA\r\n 3:             33.00     96.0000      32.0          33.000\r\n 4:                NA    176.3333        NA              NA\r\n 5:             57.00    182.0000      77.0          57.000\r\n 6:             64.00    180.0000        NA          64.000\r\n 7:                NA    231.0000     124.0              NA\r\n 8:             25.00    175.0000      78.5          25.000\r\n 9:             44.00    173.0000      74.0          44.000\r\n10:            600.00    175.0000    1358.0         600.000\r\n11:                NA    180.0000     110.0              NA\r\n12:            896.00     66.0000      17.0         896.000\r\n13:                NA    168.4000        NA              NA\r\n14:             31.50    183.0000      78.2          31.500\r\n15:                NA          NA        NA              NA\r\n16:             53.00    190.0000     113.0          53.000\r\n17:             31.00    177.0000      79.0          31.000\r\n18:             37.00    175.0000      79.0          37.000\r\n19:             41.00    180.0000      83.0          41.000\r\n20:             48.00    150.0000        NA          48.000\r\n21:                NA          NA        NA              NA\r\n22:              8.00     88.0000      20.0           8.000\r\n23:                NA    160.0000      68.0              NA\r\n24:                NA    191.0000      90.0              NA\r\n25:                NA    168.5000        NA              NA\r\n26:                NA    208.6667        NA              NA\r\n27:                NA    183.0000        NA              NA\r\n28:                NA    137.0000        NA              NA\r\n29:                NA    112.0000      40.0              NA\r\n30:             54.00    175.0000      80.0          54.000\r\n31:                NA    179.0000        NA              NA\r\n32:                NA     94.0000      45.0              NA\r\n33:                NA    122.0000        NA              NA\r\n34:                NA    163.0000      65.0              NA\r\n35:             72.00    188.0000      84.0          72.000\r\n36:             92.00    198.0000      82.0          92.000\r\n37:                NA    196.0000      87.0              NA\r\n38:                NA    171.0000        NA              NA\r\n39:                NA    184.0000      50.0              NA\r\n40:                NA    188.0000        NA              NA\r\n41:                NA    264.0000        NA              NA\r\n42:             22.00    188.0000      80.0          22.000\r\n43:                NA    196.0000        NA              NA\r\n44:                NA    183.0000      80.0              NA\r\n45:             49.00    168.0000      53.1          49.000\r\n46:            102.00    193.0000      80.0         102.000\r\n47:             66.00    183.0000      79.0          66.000\r\n48:                NA    168.0000      55.0              NA\r\n49:                NA    198.0000     102.0              NA\r\n50:                NA    221.0000        NA              NA\r\n51:                NA     79.0000      15.0              NA\r\n52:                NA    193.0000      48.0              NA\r\n53:                NA    191.0000        NA              NA\r\n54:                NA    178.0000      57.0              NA\r\n55:                NA    216.0000     159.0              NA\r\n56:                NA    178.0000      48.0              NA\r\n57:                NA    206.0000      80.0              NA\r\n58:                NA          NA        NA              NA\r\n    median.birth_year mean.height mean.mass mean.birth_year\r\n\r\nExercise 2 - all colnames to upper case\r\n\r\n\r\nstarwarsDT = as.data.table(starwars)\r\nsetnames(starwarsDT, toupper(names(starwarsDT)))\r\n\r\n\r\n\r\nConvert all character columns to upper case\r\n\r\n\r\ncols = sapply(starwarsDT, is.character)\r\ncols = names(cols)[cols]\r\nstarwarsDT[, lapply(.SD, toupper)]\r\n\r\n\r\n                     NAME HEIGHT MASS    HAIR_COLOR\r\n 1:        LUKE SKYWALKER    172   77         BLOND\r\n 2:                 C-3PO    167   75          <NA>\r\n 3:                 R2-D2     96   32          <NA>\r\n 4:           DARTH VADER    202  136          NONE\r\n 5:           LEIA ORGANA    150   49         BROWN\r\n 6:             OWEN LARS    178  120   BROWN, GREY\r\n 7:    BERU WHITESUN LARS    165   75         BROWN\r\n 8:                 R5-D4     97   32          <NA>\r\n 9:     BIGGS DARKLIGHTER    183   84         BLACK\r\n10:        OBI-WAN KENOBI    182   77 AUBURN, WHITE\r\n11:      ANAKIN SKYWALKER    188   84         BLOND\r\n12:        WILHUFF TARKIN    180 <NA>  AUBURN, GREY\r\n13:             CHEWBACCA    228  112         BROWN\r\n14:              HAN SOLO    180   80         BROWN\r\n15:                GREEDO    173   74          <NA>\r\n16: JABBA DESILIJIC TIURE    175 1358          <NA>\r\n17:        WEDGE ANTILLES    170   77         BROWN\r\n18:      JEK TONO PORKINS    180  110         BROWN\r\n19:                  YODA     66   17         WHITE\r\n20:             PALPATINE    170   75          GREY\r\n21:             BOBA FETT    183 78.2         BLACK\r\n22:                 IG-88    200  140          NONE\r\n23:                 BOSSK    190  113          NONE\r\n24:      LANDO CALRISSIAN    177   79         BLACK\r\n25:                 LOBOT    175   79          NONE\r\n26:                ACKBAR    180   83          NONE\r\n27:            MON MOTHMA    150 <NA>        AUBURN\r\n28:          ARVEL CRYNYD   <NA> <NA>         BROWN\r\n29: WICKET SYSTRI WARRICK     88   20         BROWN\r\n30:             NIEN NUNB    160   68          NONE\r\n31:          QUI-GON JINN    193   89         BROWN\r\n32:           NUTE GUNRAY    191   90          NONE\r\n33:         FINIS VALORUM    170 <NA>         BLOND\r\n34:         JAR JAR BINKS    196   66          NONE\r\n35:          ROOS TARPALS    224   82          NONE\r\n36:            RUGOR NASS    206 <NA>          NONE\r\n37:              RIC OLIÉ    183 <NA>         BROWN\r\n38:                 WATTO    137 <NA>         BLACK\r\n39:               SEBULBA    112   40          NONE\r\n40:         QUARSH PANAKA    183 <NA>         BLACK\r\n41:        SHMI SKYWALKER    163 <NA>         BLACK\r\n42:            DARTH MAUL    175   80          NONE\r\n43:           BIB FORTUNA    180 <NA>          NONE\r\n44:           AYLA SECURA    178   55          NONE\r\n45:              DUD BOLT     94   45          NONE\r\n46:               GASGANO    122 <NA>          NONE\r\n47:        BEN QUADINAROS    163   65          NONE\r\n48:            MACE WINDU    188   84          NONE\r\n49:          KI-ADI-MUNDI    198   82         WHITE\r\n50:             KIT FISTO    196   87          NONE\r\n51:             EETH KOTH    171 <NA>         BLACK\r\n52:            ADI GALLIA    184   50          NONE\r\n53:           SAESEE TIIN    188 <NA>          NONE\r\n54:           YARAEL POOF    264 <NA>          NONE\r\n55:              PLO KOON    188   80          NONE\r\n56:            MAS AMEDDA    196 <NA>          NONE\r\n57:          GREGAR TYPHO    185   85         BLACK\r\n58:                 CORDÉ    157 <NA>         BROWN\r\n59:           CLIEGG LARS    183 <NA>         BROWN\r\n60:     POGGLE THE LESSER    183   80          NONE\r\n61:       LUMINARA UNDULI    170 56.2         BLACK\r\n62:         BARRISS OFFEE    166   50         BLACK\r\n63:                 DORMÉ    165 <NA>         BROWN\r\n64:                 DOOKU    193   80         WHITE\r\n65:   BAIL PRESTOR ORGANA    191 <NA>         BLACK\r\n66:            JANGO FETT    183   79         BLACK\r\n67:            ZAM WESELL    168   55        BLONDE\r\n68:       DEXTER JETTSTER    198  102          NONE\r\n69:               LAMA SU    229   88          NONE\r\n70:               TAUN WE    213 <NA>          NONE\r\n71:            JOCASTA NU    167 <NA>         WHITE\r\n72:         RATTS TYERELL     79   15          NONE\r\n73:                R4-P17     96 <NA>          NONE\r\n74:            WAT TAMBOR    193   48          NONE\r\n75:              SAN HILL    191 <NA>          NONE\r\n76:              SHAAK TI    178   57          NONE\r\n77:              GRIEVOUS    216  159          NONE\r\n78:               TARFFUL    234  136         BROWN\r\n79:       RAYMUS ANTILLES    188   79         BROWN\r\n80:             SLY MOORE    178   48          NONE\r\n81:            TION MEDON    206   80          NONE\r\n82:                  FINN   <NA> <NA>         BLACK\r\n83:                   REY   <NA> <NA>         BROWN\r\n84:           POE DAMERON   <NA> <NA>         BROWN\r\n85:                   BB8   <NA> <NA>          NONE\r\n86:        CAPTAIN PHASMA   <NA> <NA>       UNKNOWN\r\n87:         PADMÉ AMIDALA    165   45         BROWN\r\n                     NAME HEIGHT MASS    HAIR_COLOR\r\n             SKIN_COLOR     EYE_COLOR BIRTH_YEAR            SEX\r\n 1:                FAIR          BLUE         19           MALE\r\n 2:                GOLD        YELLOW        112           NONE\r\n 3:         WHITE, BLUE           RED         33           NONE\r\n 4:               WHITE        YELLOW       41.9           MALE\r\n 5:               LIGHT         BROWN         19         FEMALE\r\n 6:               LIGHT          BLUE         52           MALE\r\n 7:               LIGHT          BLUE         47         FEMALE\r\n 8:          WHITE, RED           RED       <NA>           NONE\r\n 9:               LIGHT         BROWN         24           MALE\r\n10:                FAIR     BLUE-GRAY         57           MALE\r\n11:                FAIR          BLUE       41.9           MALE\r\n12:                FAIR          BLUE         64           MALE\r\n13:             UNKNOWN          BLUE        200           MALE\r\n14:                FAIR         BROWN         29           MALE\r\n15:               GREEN         BLACK         44           MALE\r\n16:    GREEN-TAN, BROWN        ORANGE        600 HERMAPHRODITIC\r\n17:                FAIR         HAZEL         21           MALE\r\n18:                FAIR          BLUE       <NA>           MALE\r\n19:               GREEN         BROWN        896           MALE\r\n20:                PALE        YELLOW         82           MALE\r\n21:                FAIR         BROWN       31.5           MALE\r\n22:               METAL           RED         15           NONE\r\n23:               GREEN           RED         53           MALE\r\n24:                DARK         BROWN         31           MALE\r\n25:               LIGHT          BLUE         37           MALE\r\n26:        BROWN MOTTLE        ORANGE         41           MALE\r\n27:                FAIR          BLUE         48         FEMALE\r\n28:                FAIR         BROWN       <NA>           MALE\r\n29:               BROWN         BROWN          8           MALE\r\n30:                GREY         BLACK       <NA>           MALE\r\n31:                FAIR          BLUE         92           MALE\r\n32:       MOTTLED GREEN           RED       <NA>           MALE\r\n33:                FAIR          BLUE         91           MALE\r\n34:              ORANGE        ORANGE         52           MALE\r\n35:                GREY        ORANGE       <NA>           MALE\r\n36:               GREEN        ORANGE       <NA>           MALE\r\n37:                FAIR          BLUE       <NA>           <NA>\r\n38:          BLUE, GREY        YELLOW       <NA>           MALE\r\n39:           GREY, RED        ORANGE       <NA>           MALE\r\n40:                DARK         BROWN         62           <NA>\r\n41:                FAIR         BROWN         72         FEMALE\r\n42:                 RED        YELLOW         54           MALE\r\n43:                PALE          PINK       <NA>           MALE\r\n44:                BLUE         HAZEL         48         FEMALE\r\n45:          BLUE, GREY        YELLOW       <NA>           MALE\r\n46:         WHITE, BLUE         BLACK       <NA>           MALE\r\n47: GREY, GREEN, YELLOW        ORANGE       <NA>           MALE\r\n48:                DARK         BROWN         72           MALE\r\n49:                PALE        YELLOW         92           MALE\r\n50:               GREEN         BLACK       <NA>           MALE\r\n51:               BROWN         BROWN       <NA>           MALE\r\n52:                DARK          BLUE       <NA>         FEMALE\r\n53:                PALE        ORANGE       <NA>           MALE\r\n54:               WHITE        YELLOW       <NA>           MALE\r\n55:              ORANGE         BLACK         22           MALE\r\n56:                BLUE          BLUE       <NA>           MALE\r\n57:                DARK         BROWN       <NA>           MALE\r\n58:               LIGHT         BROWN       <NA>         FEMALE\r\n59:                FAIR          BLUE         82           MALE\r\n60:               GREEN        YELLOW       <NA>           MALE\r\n61:              YELLOW          BLUE         58         FEMALE\r\n62:              YELLOW          BLUE         40         FEMALE\r\n63:               LIGHT         BROWN       <NA>         FEMALE\r\n64:                FAIR         BROWN        102           MALE\r\n65:                 TAN         BROWN         67           MALE\r\n66:                 TAN         BROWN         66           MALE\r\n67: FAIR, GREEN, YELLOW        YELLOW       <NA>         FEMALE\r\n68:               BROWN        YELLOW       <NA>           MALE\r\n69:                GREY         BLACK       <NA>           MALE\r\n70:                GREY         BLACK       <NA>         FEMALE\r\n71:                FAIR          BLUE       <NA>         FEMALE\r\n72:          GREY, BLUE       UNKNOWN       <NA>           MALE\r\n73:         SILVER, RED     RED, BLUE       <NA>           NONE\r\n74:         GREEN, GREY       UNKNOWN       <NA>           MALE\r\n75:                GREY          GOLD       <NA>           MALE\r\n76:    RED, BLUE, WHITE         BLACK       <NA>         FEMALE\r\n77:        BROWN, WHITE GREEN, YELLOW       <NA>           MALE\r\n78:               BROWN          BLUE       <NA>           MALE\r\n79:               LIGHT         BROWN       <NA>           MALE\r\n80:                PALE         WHITE       <NA>           <NA>\r\n81:                GREY         BLACK       <NA>           MALE\r\n82:                DARK          DARK       <NA>           MALE\r\n83:               LIGHT         HAZEL       <NA>         FEMALE\r\n84:               LIGHT         BROWN       <NA>           MALE\r\n85:                NONE         BLACK       <NA>           NONE\r\n86:             UNKNOWN       UNKNOWN       <NA>           <NA>\r\n87:               LIGHT         BROWN         46         FEMALE\r\n             SKIN_COLOR     EYE_COLOR BIRTH_YEAR            SEX\r\n       GENDER      HOMEWORLD        SPECIES\r\n 1: MASCULINE       TATOOINE          HUMAN\r\n 2: MASCULINE       TATOOINE          DROID\r\n 3: MASCULINE          NABOO          DROID\r\n 4: MASCULINE       TATOOINE          HUMAN\r\n 5:  FEMININE       ALDERAAN          HUMAN\r\n 6: MASCULINE       TATOOINE          HUMAN\r\n 7:  FEMININE       TATOOINE          HUMAN\r\n 8: MASCULINE       TATOOINE          DROID\r\n 9: MASCULINE       TATOOINE          HUMAN\r\n10: MASCULINE        STEWJON          HUMAN\r\n11: MASCULINE       TATOOINE          HUMAN\r\n12: MASCULINE         ERIADU          HUMAN\r\n13: MASCULINE       KASHYYYK        WOOKIEE\r\n14: MASCULINE       CORELLIA          HUMAN\r\n15: MASCULINE          RODIA         RODIAN\r\n16: MASCULINE      NAL HUTTA           HUTT\r\n17: MASCULINE       CORELLIA          HUMAN\r\n18: MASCULINE     BESTINE IV          HUMAN\r\n19: MASCULINE           <NA> YODA'S SPECIES\r\n20: MASCULINE          NABOO          HUMAN\r\n21: MASCULINE         KAMINO          HUMAN\r\n22: MASCULINE           <NA>          DROID\r\n23: MASCULINE      TRANDOSHA     TRANDOSHAN\r\n24: MASCULINE        SOCORRO          HUMAN\r\n25: MASCULINE         BESPIN          HUMAN\r\n26: MASCULINE       MON CALA   MON CALAMARI\r\n27:  FEMININE      CHANDRILA          HUMAN\r\n28: MASCULINE           <NA>          HUMAN\r\n29: MASCULINE          ENDOR           EWOK\r\n30: MASCULINE        SULLUST      SULLUSTAN\r\n31: MASCULINE           <NA>          HUMAN\r\n32: MASCULINE CATO NEIMOIDIA      NEIMODIAN\r\n33: MASCULINE      CORUSCANT          HUMAN\r\n34: MASCULINE          NABOO         GUNGAN\r\n35: MASCULINE          NABOO         GUNGAN\r\n36: MASCULINE          NABOO         GUNGAN\r\n37:      <NA>          NABOO           <NA>\r\n38: MASCULINE       TOYDARIA      TOYDARIAN\r\n39: MASCULINE      MALASTARE            DUG\r\n40:      <NA>          NABOO           <NA>\r\n41:  FEMININE       TATOOINE          HUMAN\r\n42: MASCULINE       DATHOMIR         ZABRAK\r\n43: MASCULINE         RYLOTH        TWI'LEK\r\n44:  FEMININE         RYLOTH        TWI'LEK\r\n45: MASCULINE        VULPTER     VULPTEREEN\r\n46: MASCULINE        TROIKEN          XEXTO\r\n47: MASCULINE           TUND          TOONG\r\n48: MASCULINE     HARUUN KAL          HUMAN\r\n49: MASCULINE          CEREA         CEREAN\r\n50: MASCULINE    GLEE ANSELM       NAUTOLAN\r\n51: MASCULINE       IRIDONIA         ZABRAK\r\n52:  FEMININE      CORUSCANT     THOLOTHIAN\r\n53: MASCULINE        IKTOTCH       IKTOTCHI\r\n54: MASCULINE        QUERMIA       QUERMIAN\r\n55: MASCULINE          DORIN        KEL DOR\r\n56: MASCULINE       CHAMPALA       CHAGRIAN\r\n57: MASCULINE          NABOO          HUMAN\r\n58:  FEMININE          NABOO          HUMAN\r\n59: MASCULINE       TATOOINE          HUMAN\r\n60: MASCULINE       GEONOSIS      GEONOSIAN\r\n61:  FEMININE         MIRIAL       MIRIALAN\r\n62:  FEMININE         MIRIAL       MIRIALAN\r\n63:  FEMININE          NABOO          HUMAN\r\n64: MASCULINE        SERENNO          HUMAN\r\n65: MASCULINE       ALDERAAN          HUMAN\r\n66: MASCULINE   CONCORD DAWN          HUMAN\r\n67:  FEMININE          ZOLAN       CLAWDITE\r\n68: MASCULINE           OJOM       BESALISK\r\n69: MASCULINE         KAMINO       KAMINOAN\r\n70:  FEMININE         KAMINO       KAMINOAN\r\n71:  FEMININE      CORUSCANT          HUMAN\r\n72: MASCULINE    ALEEN MINOR         ALEENA\r\n73:  FEMININE           <NA>          DROID\r\n74: MASCULINE          SKAKO        SKAKOAN\r\n75: MASCULINE     MUUNILINST           MUUN\r\n76:  FEMININE          SHILI        TOGRUTA\r\n77: MASCULINE          KALEE        KALEESH\r\n78: MASCULINE       KASHYYYK        WOOKIEE\r\n79: MASCULINE       ALDERAAN          HUMAN\r\n80:      <NA>         UMBARA           <NA>\r\n81: MASCULINE         UTAPAU         PAU'AN\r\n82: MASCULINE           <NA>          HUMAN\r\n83:  FEMININE           <NA>          HUMAN\r\n84: MASCULINE           <NA>          HUMAN\r\n85: MASCULINE           <NA>          DROID\r\n86:      <NA>           <NA>           <NA>\r\n87:  FEMININE          NABOO          HUMAN\r\n       GENDER      HOMEWORLD        SPECIES\r\n                                                                                                                                                         FILMS\r\n 1:                                               C(\"THE EMPIRE STRIKES BACK\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\", \"THE FORCE AWAKENS\")\r\n 2:                      C(\"THE EMPIRE STRIKES BACK\", \"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\")\r\n 3: C(\"THE EMPIRE STRIKES BACK\", \"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\", \"THE FORCE AWAKENS\")\r\n 4:                                                                    C(\"THE EMPIRE STRIKES BACK\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\")\r\n 5:                                               C(\"THE EMPIRE STRIKES BACK\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\", \"THE FORCE AWAKENS\")\r\n 6:                                                                                             C(\"ATTACK OF THE CLONES\", \"REVENGE OF THE SITH\", \"A NEW HOPE\")\r\n 7:                                                                                             C(\"ATTACK OF THE CLONES\", \"REVENGE OF THE SITH\", \"A NEW HOPE\")\r\n 8:                                                                                                                                                 A NEW HOPE\r\n 9:                                                                                                                                                 A NEW HOPE\r\n10:                      C(\"THE EMPIRE STRIKES BACK\", \"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\")\r\n11:                                                                                     C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n12:                                                                                                                     C(\"REVENGE OF THE SITH\", \"A NEW HOPE\")\r\n13:                                               C(\"THE EMPIRE STRIKES BACK\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\", \"A NEW HOPE\", \"THE FORCE AWAKENS\")\r\n14:                                                                      C(\"THE EMPIRE STRIKES BACK\", \"RETURN OF THE JEDI\", \"A NEW HOPE\", \"THE FORCE AWAKENS\")\r\n15:                                                                                                                                                 A NEW HOPE\r\n16:                                                                                                C(\"THE PHANTOM MENACE\", \"RETURN OF THE JEDI\", \"A NEW HOPE\")\r\n17:                                                                                           C(\"THE EMPIRE STRIKES BACK\", \"RETURN OF THE JEDI\", \"A NEW HOPE\")\r\n18:                                                                                                                                                 A NEW HOPE\r\n19:                                    C(\"THE EMPIRE STRIKES BACK\", \"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\")\r\n20:                                    C(\"THE EMPIRE STRIKES BACK\", \"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\", \"RETURN OF THE JEDI\")\r\n21:                                                                                 C(\"THE EMPIRE STRIKES BACK\", \"ATTACK OF THE CLONES\", \"RETURN OF THE JEDI\")\r\n22:                                                                                                                                    THE EMPIRE STRIKES BACK\r\n23:                                                                                                                                    THE EMPIRE STRIKES BACK\r\n24:                                                                                                         C(\"THE EMPIRE STRIKES BACK\", \"RETURN OF THE JEDI\")\r\n25:                                                                                                                                    THE EMPIRE STRIKES BACK\r\n26:                                                                                                               C(\"RETURN OF THE JEDI\", \"THE FORCE AWAKENS\")\r\n27:                                                                                                                                         RETURN OF THE JEDI\r\n28:                                                                                                                                         RETURN OF THE JEDI\r\n29:                                                                                                                                         RETURN OF THE JEDI\r\n30:                                                                                                                                         RETURN OF THE JEDI\r\n31:                                                                                                                                         THE PHANTOM MENACE\r\n32:                                                                                     C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n33:                                                                                                                                         THE PHANTOM MENACE\r\n34:                                                                                                            C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\")\r\n35:                                                                                                                                         THE PHANTOM MENACE\r\n36:                                                                                                                                         THE PHANTOM MENACE\r\n37:                                                                                                                                         THE PHANTOM MENACE\r\n38:                                                                                                            C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\")\r\n39:                                                                                                                                         THE PHANTOM MENACE\r\n40:                                                                                                                                         THE PHANTOM MENACE\r\n41:                                                                                                            C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\")\r\n42:                                                                                                                                         THE PHANTOM MENACE\r\n43:                                                                                                                                         RETURN OF THE JEDI\r\n44:                                                                                     C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n45:                                                                                                                                         THE PHANTOM MENACE\r\n46:                                                                                                                                         THE PHANTOM MENACE\r\n47:                                                                                                                                         THE PHANTOM MENACE\r\n48:                                                                                     C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n49:                                                                                     C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n50:                                                                                     C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n51:                                                                                                             C(\"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n52:                                                                                                             C(\"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n53:                                                                                                             C(\"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n54:                                                                                                                                         THE PHANTOM MENACE\r\n55:                                                                                     C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n56:                                                                                                            C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\")\r\n57:                                                                                                                                       ATTACK OF THE CLONES\r\n58:                                                                                                                                       ATTACK OF THE CLONES\r\n59:                                                                                                                                       ATTACK OF THE CLONES\r\n60:                                                                                                           C(\"ATTACK OF THE CLONES\", \"REVENGE OF THE SITH\")\r\n61:                                                                                                           C(\"ATTACK OF THE CLONES\", \"REVENGE OF THE SITH\")\r\n62:                                                                                                                                       ATTACK OF THE CLONES\r\n63:                                                                                                                                       ATTACK OF THE CLONES\r\n64:                                                                                                           C(\"ATTACK OF THE CLONES\", \"REVENGE OF THE SITH\")\r\n65:                                                                                                           C(\"ATTACK OF THE CLONES\", \"REVENGE OF THE SITH\")\r\n66:                                                                                                                                       ATTACK OF THE CLONES\r\n67:                                                                                                                                       ATTACK OF THE CLONES\r\n68:                                                                                                                                       ATTACK OF THE CLONES\r\n69:                                                                                                                                       ATTACK OF THE CLONES\r\n70:                                                                                                                                       ATTACK OF THE CLONES\r\n71:                                                                                                                                       ATTACK OF THE CLONES\r\n72:                                                                                                                                         THE PHANTOM MENACE\r\n73:                                                                                                           C(\"ATTACK OF THE CLONES\", \"REVENGE OF THE SITH\")\r\n74:                                                                                                                                       ATTACK OF THE CLONES\r\n75:                                                                                                                                       ATTACK OF THE CLONES\r\n76:                                                                                                           C(\"ATTACK OF THE CLONES\", \"REVENGE OF THE SITH\")\r\n77:                                                                                                                                        REVENGE OF THE SITH\r\n78:                                                                                                                                        REVENGE OF THE SITH\r\n79:                                                                                                                     C(\"REVENGE OF THE SITH\", \"A NEW HOPE\")\r\n80:                                                                                                           C(\"ATTACK OF THE CLONES\", \"REVENGE OF THE SITH\")\r\n81:                                                                                                                                        REVENGE OF THE SITH\r\n82:                                                                                                                                          THE FORCE AWAKENS\r\n83:                                                                                                                                          THE FORCE AWAKENS\r\n84:                                                                                                                                          THE FORCE AWAKENS\r\n85:                                                                                                                                          THE FORCE AWAKENS\r\n86:                                                                                                                                          THE FORCE AWAKENS\r\n87:                                                                                     C(\"ATTACK OF THE CLONES\", \"THE PHANTOM MENACE\", \"REVENGE OF THE SITH\")\r\n                                                                                                                                                         FILMS\r\n                                       VEHICLES\r\n 1:   C(\"SNOWSPEEDER\", \"IMPERIAL SPEEDER BIKE\")\r\n 2:                                CHARACTER(0)\r\n 3:                                CHARACTER(0)\r\n 4:                                CHARACTER(0)\r\n 5:                       IMPERIAL SPEEDER BIKE\r\n 6:                                CHARACTER(0)\r\n 7:                                CHARACTER(0)\r\n 8:                                CHARACTER(0)\r\n 9:                                CHARACTER(0)\r\n10:                             TRIBUBBLE BONGO\r\n11: C(\"ZEPHYR-G SWOOP BIKE\", \"XJ-6 AIRSPEEDER\")\r\n12:                                CHARACTER(0)\r\n13:                                       AT-ST\r\n14:                                CHARACTER(0)\r\n15:                                CHARACTER(0)\r\n16:                                CHARACTER(0)\r\n17:                                 SNOWSPEEDER\r\n18:                                CHARACTER(0)\r\n19:                                CHARACTER(0)\r\n20:                                CHARACTER(0)\r\n21:                                CHARACTER(0)\r\n22:                                CHARACTER(0)\r\n23:                                CHARACTER(0)\r\n24:                                CHARACTER(0)\r\n25:                                CHARACTER(0)\r\n26:                                CHARACTER(0)\r\n27:                                CHARACTER(0)\r\n28:                                CHARACTER(0)\r\n29:                                CHARACTER(0)\r\n30:                                CHARACTER(0)\r\n31:                             TRIBUBBLE BONGO\r\n32:                                CHARACTER(0)\r\n33:                                CHARACTER(0)\r\n34:                                CHARACTER(0)\r\n35:                                CHARACTER(0)\r\n36:                                CHARACTER(0)\r\n37:                                CHARACTER(0)\r\n38:                                CHARACTER(0)\r\n39:                                CHARACTER(0)\r\n40:                                CHARACTER(0)\r\n41:                                CHARACTER(0)\r\n42:                                SITH SPEEDER\r\n43:                                CHARACTER(0)\r\n44:                                CHARACTER(0)\r\n45:                                CHARACTER(0)\r\n46:                                CHARACTER(0)\r\n47:                                CHARACTER(0)\r\n48:                                CHARACTER(0)\r\n49:                                CHARACTER(0)\r\n50:                                CHARACTER(0)\r\n51:                                CHARACTER(0)\r\n52:                                CHARACTER(0)\r\n53:                                CHARACTER(0)\r\n54:                                CHARACTER(0)\r\n55:                                CHARACTER(0)\r\n56:                                CHARACTER(0)\r\n57:                                CHARACTER(0)\r\n58:                                CHARACTER(0)\r\n59:                                CHARACTER(0)\r\n60:                                CHARACTER(0)\r\n61:                                CHARACTER(0)\r\n62:                                CHARACTER(0)\r\n63:                                CHARACTER(0)\r\n64:                            FLITKNOT SPEEDER\r\n65:                                CHARACTER(0)\r\n66:                                CHARACTER(0)\r\n67:                  KORO-2 EXODRIVE AIRSPEEDER\r\n68:                                CHARACTER(0)\r\n69:                                CHARACTER(0)\r\n70:                                CHARACTER(0)\r\n71:                                CHARACTER(0)\r\n72:                                CHARACTER(0)\r\n73:                                CHARACTER(0)\r\n74:                                CHARACTER(0)\r\n75:                                CHARACTER(0)\r\n76:                                CHARACTER(0)\r\n77:                 TSMEU-6 PERSONAL WHEEL BIKE\r\n78:                                CHARACTER(0)\r\n79:                                CHARACTER(0)\r\n80:                                CHARACTER(0)\r\n81:                                CHARACTER(0)\r\n82:                                CHARACTER(0)\r\n83:                                CHARACTER(0)\r\n84:                                CHARACTER(0)\r\n85:                                CHARACTER(0)\r\n86:                                CHARACTER(0)\r\n87:                                CHARACTER(0)\r\n                                       VEHICLES\r\n                                                                                                                STARSHIPS\r\n 1:                                                                                       C(\"X-WING\", \"IMPERIAL SHUTTLE\")\r\n 2:                                                                                                          CHARACTER(0)\r\n 3:                                                                                                          CHARACTER(0)\r\n 4:                                                                                                       TIE ADVANCED X1\r\n 5:                                                                                                          CHARACTER(0)\r\n 6:                                                                                                          CHARACTER(0)\r\n 7:                                                                                                          CHARACTER(0)\r\n 8:                                                                                                          CHARACTER(0)\r\n 9:                                                                                                                X-WING\r\n10: C(\"JEDI STARFIGHTER\", \"TRADE FEDERATION CRUISER\", \"NABOO STAR SKIFF\", \"JEDI INTERCEPTOR\", \"BELBULLAB-22 STARFIGHTER\")\r\n11:                                                    C(\"TRADE FEDERATION CRUISER\", \"JEDI INTERCEPTOR\", \"NABOO FIGHTER\")\r\n12:                                                                                                          CHARACTER(0)\r\n13:                                                                            C(\"MILLENNIUM FALCON\", \"IMPERIAL SHUTTLE\")\r\n14:                                                                            C(\"MILLENNIUM FALCON\", \"IMPERIAL SHUTTLE\")\r\n15:                                                                                                          CHARACTER(0)\r\n16:                                                                                                          CHARACTER(0)\r\n17:                                                                                                                X-WING\r\n18:                                                                                                                X-WING\r\n19:                                                                                                          CHARACTER(0)\r\n20:                                                                                                          CHARACTER(0)\r\n21:                                                                                                               SLAVE 1\r\n22:                                                                                                          CHARACTER(0)\r\n23:                                                                                                          CHARACTER(0)\r\n24:                                                                                                     MILLENNIUM FALCON\r\n25:                                                                                                          CHARACTER(0)\r\n26:                                                                                                          CHARACTER(0)\r\n27:                                                                                                          CHARACTER(0)\r\n28:                                                                                                                A-WING\r\n29:                                                                                                          CHARACTER(0)\r\n30:                                                                                                     MILLENNIUM FALCON\r\n31:                                                                                                          CHARACTER(0)\r\n32:                                                                                                          CHARACTER(0)\r\n33:                                                                                                          CHARACTER(0)\r\n34:                                                                                                          CHARACTER(0)\r\n35:                                                                                                          CHARACTER(0)\r\n36:                                                                                                          CHARACTER(0)\r\n37:                                                                                                  NABOO ROYAL STARSHIP\r\n38:                                                                                                          CHARACTER(0)\r\n39:                                                                                                          CHARACTER(0)\r\n40:                                                                                                          CHARACTER(0)\r\n41:                                                                                                          CHARACTER(0)\r\n42:                                                                                                              SCIMITAR\r\n43:                                                                                                          CHARACTER(0)\r\n44:                                                                                                          CHARACTER(0)\r\n45:                                                                                                          CHARACTER(0)\r\n46:                                                                                                          CHARACTER(0)\r\n47:                                                                                                          CHARACTER(0)\r\n48:                                                                                                          CHARACTER(0)\r\n49:                                                                                                          CHARACTER(0)\r\n50:                                                                                                          CHARACTER(0)\r\n51:                                                                                                          CHARACTER(0)\r\n52:                                                                                                          CHARACTER(0)\r\n53:                                                                                                          CHARACTER(0)\r\n54:                                                                                                          CHARACTER(0)\r\n55:                                                                                                      JEDI STARFIGHTER\r\n56:                                                                                                          CHARACTER(0)\r\n57:                                                                                                         NABOO FIGHTER\r\n58:                                                                                                          CHARACTER(0)\r\n59:                                                                                                          CHARACTER(0)\r\n60:                                                                                                          CHARACTER(0)\r\n61:                                                                                                          CHARACTER(0)\r\n62:                                                                                                          CHARACTER(0)\r\n63:                                                                                                          CHARACTER(0)\r\n64:                                                                                                          CHARACTER(0)\r\n65:                                                                                                          CHARACTER(0)\r\n66:                                                                                                          CHARACTER(0)\r\n67:                                                                                                          CHARACTER(0)\r\n68:                                                                                                          CHARACTER(0)\r\n69:                                                                                                          CHARACTER(0)\r\n70:                                                                                                          CHARACTER(0)\r\n71:                                                                                                          CHARACTER(0)\r\n72:                                                                                                          CHARACTER(0)\r\n73:                                                                                                          CHARACTER(0)\r\n74:                                                                                                          CHARACTER(0)\r\n75:                                                                                                          CHARACTER(0)\r\n76:                                                                                                          CHARACTER(0)\r\n77:                                                                                              BELBULLAB-22 STARFIGHTER\r\n78:                                                                                                          CHARACTER(0)\r\n79:                                                                                                          CHARACTER(0)\r\n80:                                                                                                          CHARACTER(0)\r\n81:                                                                                                          CHARACTER(0)\r\n82:                                                                                                          CHARACTER(0)\r\n83:                                                                                                          CHARACTER(0)\r\n84:                                                                                                   T-70 X-WING FIGHTER\r\n85:                                                                                                          CHARACTER(0)\r\n86:                                                                                                          CHARACTER(0)\r\n87:                                                         C(\"H-TYPE NUBIAN YACHT\", \"NABOO STAR SKIFF\", \"NABOO FIGHTER\")\r\n                                                                                                                STARSHIPS\r\n\r\nExercise 3\r\nRemove the data which have NA for the numeric variables\r\nGroup the data by all character variables\r\nCompute the mean for ts_diameter and hu_diameter\r\n\r\n\r\nstormsDT <- as.data.table(storms)\r\nnumericCols = sapply(stormsDT, is.numeric)\r\nnumericCols = names(numericCols)[numericCols]\r\nstormsDTNew <- na.omit(stormsDT, cols = numericCols)\r\ncharacterCols <- sapply(stormsDTNew, is.character)\r\ncharacterCols <- names(characterCols)[characterCols]\r\ncolsOfInterest <- c(\"ts_diameter\", \"hu_diameter\")\r\nstormsDTNew[, lapply(.SD, mean), .SDcols = colsOfInterest, by = characterCols]\r\n\r\n\r\n        name              status ts_diameter hu_diameter\r\n  1:    Alex tropical depression     0.00000     0.00000\r\n  2:    Alex      tropical storm   150.80013     0.00000\r\n  3:    Alex           hurricane   304.40871    63.56690\r\n  4:  Bonnie tropical depression     0.00000     0.00000\r\n  5:  Bonnie      tropical storm    75.40637     0.00000\r\n ---                                                    \r\n273: Joaquin      tropical storm   198.92054     0.00000\r\n274: Joaquin           hurricane   332.98376    85.00923\r\n275:    Kate tropical depression     0.00000     0.00000\r\n276:    Kate      tropical storm    85.48651     0.00000\r\n277:    Kate           hurricane   273.31025    31.64645\r\n\r\n.SD: Subset of data - choose row by ordinal position\r\n\r\n\r\nirisDT[, .SD[1L]]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:            6         2.2            5         1.5 virginica\r\n   maxLength minWidth Sepal.Length.Width\r\n1:       7.9      0.1              6/2.2\r\n\r\nirisDT[, .SD[1L], by = Species]\r\n\r\n\r\n      Species Sepal.Length Sepal.Width Petal.Length Petal.Width\r\n1:  virginica            6         2.2          5.0         1.5\r\n2: versicolor            5           2          3.5         1.0\r\n3:     setosa          4.5         2.3          1.3         0.3\r\n   maxLength minWidth Sepal.Length.Width\r\n1:       7.9      0.1              6/2.2\r\n2:       7.9      0.1                5/2\r\n3:       7.9      0.1            4.5/2.3\r\n\r\nirisDT[, .SD[1L], by = Species, .SDcols = c(\"Petal.Length\", \"Petal.Width\")]\r\n\r\n\r\n      Species Petal.Length Petal.Width\r\n1:  virginica          5.0         1.5\r\n2: versicolor          3.5         1.0\r\n3:     setosa          1.3         0.3\r\n\r\n.SD: Subset of Data - select top entries (in each group)\r\n\r\n\r\nirisDT[order(-Sepal.Length), .SD[1:2]]\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n1:          7.9         3.8          6.4         2.0 virginica\r\n2:          7.7         2.6          6.9         2.3 virginica\r\n   maxLength minWidth Sepal.Length.Width\r\n1:       7.9      0.1            7.9/3.8\r\n2:       7.9      0.1            7.7/2.6\r\n\r\nirisDT[, .SD[which.max(Sepal.Length)], by = Species, .SDcols = c(\"Sepal.Length\",\"Sepal.Width\")]\r\n\r\n\r\n      Species Sepal.Length Sepal.Width\r\n1:  virginica          7.9         3.8\r\n2: versicolor            7         3.2\r\n3:     setosa          5.8           4\r\n\r\nMultiple if else statements\r\n\r\n\r\nirisDT[, text := {\r\n  if(Sepal.Length > 4 & Sepal.Width > 4) \"sepal length and width is larger than 4\"\r\n  else if(Petal.Length < 2 & Petal.Width < 2) \"petal length and width is smaller than 2\"\r\n  else \"other\"\r\n}, by = .(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)][, .SD[1], by = text]\r\n\r\n\r\n                                       text Sepal.Length Sepal.Width\r\n1:                                    other            6         2.2\r\n2: petal length and width is smaller than 2          4.5         2.3\r\n3:  sepal length and width is larger than 4          5.2         4.1\r\n   Petal.Length Petal.Width   Species maxLength minWidth\r\n1:          5.0         1.5 virginica       7.9      0.1\r\n2:          1.3         0.3    setosa       7.9      0.1\r\n3:          1.5         0.1    setosa       7.9      0.1\r\n   Sepal.Length.Width\r\n1:              6/2.2\r\n2:            4.5/2.3\r\n3:            5.2/4.1\r\n\r\n\r\n\r\nbin_iris <- function(sl, sw, pl, pw){\r\n  if(sl > 4 & sw > 4) \"sepal length and width is larger than 4\"\r\n  else if(pl < 2 & pw < 2) \"petal length and width is smaller than 2\"\r\n  else \"other\"\r\n}\r\nirisDT[, text := bin_iris(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width), \r\n       by = .(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)][, .SD[1], by = text]\r\n\r\n\r\n                                       text Sepal.Length Sepal.Width\r\n1:                                    other            6         2.2\r\n2: petal length and width is smaller than 2          4.5         2.3\r\n3:  sepal length and width is larger than 4          5.2         4.1\r\n   Petal.Length Petal.Width   Species maxLength minWidth\r\n1:          5.0         1.5 virginica       7.9      0.1\r\n2:          1.3         0.3    setosa       7.9      0.1\r\n3:          1.5         0.1    setosa       7.9      0.1\r\n   Sepal.Length.Width\r\n1:              6/2.2\r\n2:            4.5/2.3\r\n3:            5.2/4.1\r\n\r\nHands - on 4\r\nExercise 1\r\nPrint the fifth row for each species.\r\n\r\n\r\nstarwarsDT <- as.data.table(starwars)\r\nstarwarsDT[, if(.N >= 5) .SD[5L], by = species]\r\n\r\n\r\n   species               name height mass hair_color  skin_color\r\n1:   Human Beru Whitesun lars    165   75      brown       light\r\n2:   Droid             R4-P17     96   NA       none silver, red\r\n   eye_color birth_year    sex   gender homeworld\r\n1:      blue         47 female feminine  Tatooine\r\n2: red, blue         NA   none feminine      <NA>\r\n                                                 films vehicles\r\n1: Attack of the Clones,Revenge of the Sith,A New Hope         \r\n2:            Attack of the Clones,Revenge of the Sith         \r\n   starships\r\n1:          \r\n2:          \r\n\r\nExercise 2\r\nConsider the columns name, status and pressure.\r\nPrint for each storm, the 5 entries with the lowest pressure.\r\n\r\n\r\nstormsDT <- as.data.table(storms)\r\nstormsDT[, .(name, status, pressure)][order(name, pressure)][, .SD[1:min(5, .N)], by = name]\r\n\r\n\r\n         name              status pressure\r\n  1: AL011993 tropical depression      999\r\n  2: AL011993 tropical depression      999\r\n  3: AL011993 tropical depression      999\r\n  4: AL011993 tropical depression      999\r\n  5: AL011993 tropical depression     1000\r\n ---                                      \r\n978:     Zeta      tropical storm      994\r\n979:     Zeta      tropical storm      994\r\n980:     Zeta      tropical storm      994\r\n981:     Zeta      tropical storm      994\r\n982:     Zeta      tropical storm      994\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-12-04T14:49:40+01:00",
    "input_file": {}
  }
]
